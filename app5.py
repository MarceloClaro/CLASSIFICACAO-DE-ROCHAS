import os
import zipfile
import shutil
import tempfile
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image, ImageEnhance
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms, models, datasets
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.metrics import (adjusted_rand_score, normalized_mutual_info_score,
                             confusion_matrix, classification_report,
                             roc_auc_score, roc_curve)
from sklearn.preprocessing import label_binarize
from sklearn.decomposition import PCA
import streamlit as st
import gc
import logging
import base64
# Importa√ß√µes adicionais para Grad-CAM
from torchcam.methods import SmoothGradCAMpp, GradCAM, GradCAMpp, LayerCAM
from torchvision.transforms.functional import normalize, resize, to_pil_image
import cv2
# Importar otimizadores avan√ßados
try:
    import torch_optimizer as optim_advanced
    ADVANCED_OPTIMIZERS_AVAILABLE = True
except ImportError:
    ADVANCED_OPTIMIZERS_AVAILABLE = False

# Import new modules
from visualization_3d import visualize_pca_3d, visualize_activation_heatmap_3d, create_interactive_3d_visualization
from ai_chat_module import AIAnalyzer, describe_gradcam_regions
from academic_references import AcademicReferenceFetcher, format_references_for_display
from genetic_interpreter import GeneticDiagnosticInterpreter
from multi_agent_system import ManagerAgent

# Definir o dispositivo (CPU ou GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Configura√ß√µes para tornar os gr√°ficos mais bonitos
sns.set_style('whitegrid')

def set_seed(seed):
    """
    Define a seed para garantir a reprodutibilidade.
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)  # Definir a seed para reprodutibilidade

# ImageNet normalization values
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

def denormalize_image(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):
    """
    Denormaliza um tensor de imagem normalizado com valores ImageNet.
    
    Args:
        tensor: Tensor de imagem (C, H, W) ou array numpy (H, W, C)
        mean: M√©dia usada na normaliza√ß√£o
        std: Desvio padr√£o usado na normaliza√ß√£o
    
    Returns:
        Array numpy (H, W, C) com valores no intervalo [0, 1]
    """
    if isinstance(tensor, torch.Tensor):
        # Convert tensor to numpy
        image = tensor.permute(1, 2, 0).cpu().numpy()
    else:
        image = tensor
    
    # Denormalize
    mean = np.array(mean)
    std = np.array(std)
    image = std * image + mean
    
    # Clip to valid range
    image = np.clip(image, 0, 1)
    
    return image

# Enhanced image preprocessing class
class EnhancedImagePreprocessor:
    """Classe para melhorar o tratamento de imagens antes do treinamento"""
    
    @staticmethod
    def enhance_image_quality(image):
        """Aplica melhorias de qualidade na imagem"""
        # Ajustar contraste
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.2)
        
        # Ajustar nitidez
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.1)
        
        # Ajustar brilho levemente
        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(1.05)
        
        return image

def get_augmentation_transforms(augmentation_type='standard'):
    """
    Retorna transforma√ß√µes de acordo com o tipo de aumento de dados
    
    Args:
        augmentation_type: 'none', 'standard', 'mixup', 'cutmix'
    """
    if augmentation_type == 'none':
        # Sem aumento de dados - apenas transforma√ß√µes b√°sicas
        train_transform = transforms.Compose([
            transforms.Lambda(EnhancedImagePreprocessor.enhance_image_quality),
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ])
    else:
        # Standard ou base para mixup/cutmix
        train_transform = transforms.Compose([
            transforms.Lambda(EnhancedImagePreprocessor.enhance_image_quality),
            transforms.RandomApply([
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(degrees=90),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
                transforms.RandomAffine(degrees=0, shear=10),
            ], p=0.5),
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ])
    
    return train_transform

# Transforma√ß√µes para valida√ß√£o e teste com normaliza√ß√£o ImageNet
test_transforms = transforms.Compose([
    transforms.Lambda(EnhancedImagePreprocessor.enhance_image_quality),
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])

# Implementa√ß√£o de Mixup
def mixup_data(x, y, alpha=1.0):
    """Aplica Mixup ao batch de dados"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Calcula a loss para Mixup"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# Implementa√ß√£o de CutMix
def cutmix_data(x, y, alpha=1.0):
    """Aplica CutMix ao batch de dados"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)

    # Gerar bbox
    W = x.size()[2]
    H = x.size()[3]
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)

    # Centro do box
    cx = np.random.randint(0, W)
    cy = np.random.randint(0, H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    x_cutmix = x.clone()
    x_cutmix[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]

    # Ajustar lambda com a √°rea real
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
    y_a, y_b = y, y[index]
    return x_cutmix, y_a, y_b, lam

# Definir as transforma√ß√µes padr√£o para compatibilidade com c√≥digo existente
train_transforms = get_augmentation_transforms('standard')

# Dataset personalizado
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

def seed_worker(worker_id):
    """
    Fun√ß√£o para definir a seed em cada worker do DataLoader.
    """
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

def visualize_data(dataset, classes):
    """
    Exibe algumas imagens do conjunto de dados com suas classes.
    """
    st.write("### üìä Visualiza√ß√£o de algumas imagens do conjunto de dados original:")
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    for i in range(5):
        idx = np.random.randint(len(dataset))
        image, label = dataset[idx]
        image = np.array(image)  # Converter a imagem PIL em array NumPy
        axes[i].imshow(image)
        axes[i].set_title(classes[label])
        axes[i].axis('off')
    st.pyplot(fig)
    plt.close(fig)

def plot_class_distribution(dataset, classes, title="Distribui√ß√£o das Classes"):
    """
    Exibe a distribui√ß√£o das classes no conjunto de dados e mostra os valores quantitativos.
    """
    # Extrair os r√≥tulos das classes para todas as imagens no dataset
    labels = [label for _, label in dataset]
    
    # Contagem de cada classe
    class_counts = np.bincount(labels)
    
    # Plotar o gr√°fico com as contagens
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.countplot(x=labels, hue=labels, ax=ax, palette="Set2", legend=False)
    
    # Adicionar os nomes das classes no eixo X
    ax.set_xticks(range(len(classes)))
    ax.set_xticklabels(classes, rotation=45, ha='right')
    
    # Adicionar as contagens acima das barras
    for i, count in enumerate(class_counts):
        ax.text(i, count, str(count), ha='center', va='bottom', fontweight='bold')
    
    ax.set_title(title)
    ax.set_xlabel("Classes")
    ax.set_ylabel("N√∫mero de Imagens")
    
    st.pyplot(fig)
    plt.close(fig)
    
    return class_counts

def show_augmented_images(dataset, transform, classes, num_augmentations=5):
    """
    Mostra imagens originais e suas vers√µes aumentadas.
    """
    st.write("### üîÑ Exemplos de Imagens Aumentadas (Data Augmentation)")
    st.write("Cada linha mostra uma imagem original seguida de suas vers√µes aumentadas:")
    
    # Selecionar 3 imagens aleat√≥rias
    num_samples = 3
    for sample_idx in range(num_samples):
        idx = np.random.randint(len(dataset))
        original_image, label = dataset[idx]
        
        # Criar figura com 1 original + num_augmentations aumentadas
        fig, axes = plt.subplots(1, num_augmentations + 1, figsize=(15, 3))
        
        # Mostrar imagem original
        axes[0].imshow(np.array(original_image))
        axes[0].set_title(f'Original\n{classes[label]}')
        axes[0].axis('off')
        axes[0].set_facecolor('#e6f2ff')
        
        # Mostrar imagens aumentadas
        for i in range(1, num_augmentations + 1):
            augmented_image = transform(original_image)
            # Desnormalizar para visualiza√ß√£o usando a fun√ß√£o helper
            augmented_np = denormalize_image(augmented_image)
            
            axes[i].imshow(augmented_np)
            axes[i].set_title(f'Aumentada {i}')
            axes[i].axis('off')
        
        plt.tight_layout()
        st.pyplot(fig)
        plt.close(fig)

def calculate_dataset_statistics(dataset, classes):
    """
    Calcula estat√≠sticas do dataset incluindo m√©dia, desvio padr√£o, etc.
    """
    st.write("### üìà Estat√≠sticas do Dataset")
    
    # Contagem por classe
    labels = [label for _, label in dataset]
    class_counts = np.bincount(labels)
    
    # Criar dataframe com estat√≠sticas
    stats_data = {
        'Classe': classes,
        'Quantidade': class_counts,
        'Percentual (%)': [f"{(count/len(dataset)*100):.2f}" for count in class_counts]
    }
    
    df_stats = pd.DataFrame(stats_data)
    
    st.write("#### Distribui√ß√£o de Classes:")
    st.dataframe(df_stats, use_container_width=True)
    
    # Estat√≠sticas gerais
    st.write("#### Estat√≠sticas Gerais:")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total de Imagens", len(dataset))
    
    with col2:
        st.metric("N√∫mero de Classes", len(classes))
    
    with col3:
        st.metric("Imagens por Classe (M√©dia)", f"{np.mean(class_counts):.1f}")
    
    with col4:
        st.metric("Desvio Padr√£o", f"{np.std(class_counts):.1f}")
    
    # Verificar balanceamento
    min_count = np.min(class_counts)
    max_count = np.max(class_counts)
    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
    
    if imbalance_ratio > 1.5:
        st.warning(f"‚ö†Ô∏è Dataset desbalanceado detectado! Raz√£o: {imbalance_ratio:.2f}x (Classe mais frequente / Classe menos frequente)")
        st.info("üí° Recomenda√ß√£o: Considere usar 'Perda Ponderada para Classes Desbalanceadas' nas configura√ß√µes.")
    else:
        st.success(f"‚úÖ Dataset relativamente balanceado. Raz√£o: {imbalance_ratio:.2f}x")
    
    return df_stats

def visualize_pca_features(features, labels, classes, n_components=2):
    """
    Visualiza features usando PCA.
    """
    st.write(f"### üî¨ An√°lise PCA ({n_components} Componentes)")
    
    # Aplicar PCA
    pca = PCA(n_components=n_components)
    features_pca = pca.fit_transform(features)
    
    # Mostrar vari√¢ncia explicada
    explained_var = pca.explained_variance_ratio_
    st.write(f"**Vari√¢ncia Explicada:** {explained_var[0]*100:.2f}% (PC1), {explained_var[1]*100:.2f}% (PC2)")
    st.write(f"**Vari√¢ncia Total Explicada:** {sum(explained_var)*100:.2f}%")
    
    # Criar visualiza√ß√£o
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Mapear labels para nomes de classes
    labels_named = [classes[label] for label in labels]
    
    # Criar scatter plot
    scatter = sns.scatterplot(
        x=features_pca[:, 0], 
        y=features_pca[:, 1], 
        hue=labels_named,
        palette="tab10",
        ax=ax,
        s=100,
        alpha=0.7,
        edgecolor='black',
        linewidth=0.5
    )
    
    ax.set_xlabel(f'Componente Principal 1 ({explained_var[0]*100:.1f}%)')
    ax.set_ylabel(f'Componente Principal 2 ({explained_var[1]*100:.1f}%)')
    ax.set_title('Visualiza√ß√£o PCA das Features Extra√≠das')
    ax.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close(fig)
    
    return features_pca, explained_var

def get_model(model_name, num_classes, dropout_p=0.5, fine_tune=False):
    """
    Retorna o modelo pr√©-treinado selecionado.
    """
    if model_name == 'ResNet18':
        model = models.resnet18(weights='DEFAULT')
    elif model_name == 'ResNet50':
        model = models.resnet50(weights='DEFAULT')
    elif model_name == 'DenseNet121':
        model = models.densenet121(weights='DEFAULT')
    else:
        st.error("Modelo n√£o suportado.")
        return None

    if not fine_tune:
        for param in model.parameters():
            param.requires_grad = False

    if model_name.startswith('ResNet'):
        num_ftrs = model.fc.in_features
        model.fc = nn.Sequential(
            nn.Dropout(p=dropout_p),
            nn.Linear(num_ftrs, num_classes)
        )
        # Ensure final layer requires grad
        for param in model.fc.parameters():
            param.requires_grad = True
    elif model_name.startswith('DenseNet'):
        num_ftrs = model.classifier.in_features
        model.classifier = nn.Sequential(
            nn.Dropout(p=dropout_p),
            nn.Linear(num_ftrs, num_classes)
        )
        # Ensure final layer requires grad
        for param in model.classifier.parameters():
            param.requires_grad = True
    else:
        st.error("Modelo n√£o suportado.")
        return None

    model = model.to(device)
    return model

def train_model(data_dir, num_classes, model_name, fine_tune, epochs, learning_rate, batch_size, train_split, valid_split, use_weighted_loss, l2_lambda, l1_lambda, patience, optimizer_name='Adam', scheduler_name='None', augmentation_type='standard'):
    """
    Fun√ß√£o principal para treinamento do modelo.
    
    Args:
        data_dir: Diret√≥rio com os dados
        num_classes: N√∫mero de classes
        model_name: Nome do modelo
        fine_tune: Se deve fazer fine-tuning completo
        epochs: N√∫mero de √©pocas
        learning_rate: Taxa de aprendizagem
        batch_size: Tamanho do lote
        train_split: Propor√ß√£o de treino
        valid_split: Propor√ß√£o de valida√ß√£o
        use_weighted_loss: Se deve usar perda ponderada
        l2_lambda: Regulariza√ß√£o L2 (weight decay)
        l1_lambda: Regulariza√ß√£o L1
        patience: Paci√™ncia para early stopping
        optimizer_name: Nome do otimizador (Adam, AdamW, SGD, Ranger, Lion)
        scheduler_name: Nome do scheduler (None, CosineAnnealingLR, OneCycleLR)
        augmentation_type: Tipo de aumento de dados (none, standard, mixup, cutmix)
    
    Returns:
        tuple: (model, classes) ou None em caso de erro
    """
    set_seed(42)

    # Carregar o dataset original sem transforma√ß√µes
    full_dataset = datasets.ImageFolder(root=data_dir)
    
    # ========== CONTAGEM INICIAL DOS DADOS ==========
    st.write("## üìä AN√ÅLISE INICIAL DO DATASET")
    st.write(f"### üî¢ **Contagem Inicial: {len(full_dataset)} imagens**")
    
    # Exibir estat√≠sticas detalhadas
    stats_df = calculate_dataset_statistics(full_dataset, full_dataset.classes)
    
    # Exibir algumas imagens do dataset original
    visualize_data(full_dataset, full_dataset.classes)
    
    # Plotar distribui√ß√£o inicial
    st.write("### üìä Distribui√ß√£o Inicial das Classes")
    initial_class_counts = plot_class_distribution(full_dataset, full_dataset.classes, 
                                                    title="Distribui√ß√£o INICIAL das Classes (Sem Aumento de Dados)")

    # ========== T√âCNICA DE AUMENTO DE DADOS ==========
    st.write("---")
    st.write("## üîÑ APLICA√á√ÉO DA T√âCNICA DE AUMENTO DE DADOS")
    st.write(f"**T√©cnica Selecionada:** `{augmentation_type}`")
    
    if augmentation_type == 'none':
        st.info("‚ÑπÔ∏è Nenhuma t√©cnica de aumento de dados foi selecionada. As imagens ser√£o usadas como est√£o.")
    elif augmentation_type == 'standard':
        st.info("‚ÑπÔ∏è T√©cnica Standard: Aplica√ß√£o de transforma√ß√µes aleat√≥rias (rota√ß√£o, flip, crop, jitter, etc.)")
    elif augmentation_type == 'mixup':
        st.info("‚ÑπÔ∏è T√©cnica Mixup: Mistura linear de pares de imagens e seus r√≥tulos")
    elif augmentation_type == 'cutmix':
        st.info("‚ÑπÔ∏è T√©cnica CutMix: Recorte e colagem de regi√µes entre imagens diferentes")
    
    # Obter transforma√ß√µes baseadas no tipo de augmenta√ß√£o
    train_transform = get_augmentation_transforms(augmentation_type)
    
    # Mostrar exemplos de imagens aumentadas
    if augmentation_type != 'none':
        show_augmented_images(full_dataset, train_transform, full_dataset.classes, num_augmentations=4)
    
    # ========== ESTIMATIVA AP√ìS AUMENTO ==========
    st.write("---")
    st.write("## üìà ESTIMATIVA AP√ìS AUMENTO DE DADOS")
    
    # Calcular estimativa de imagens ap√≥s aumento
    # Durante o treinamento, cada √©poca gera vers√µes aumentadas
    if augmentation_type == 'none':
        augmentation_multiplier = 1
        st.write(f"### üî¢ **Total Estimado: {len(full_dataset)} imagens** (sem aumento)")
    else:
        # Com augmentation, cada √©poca gera vers√µes diferentes
        # Estimativa conservadora: cada imagem pode gerar de 3-5 varia√ß√µes por √©poca
        augmentation_multiplier = 4  # M√©dia estimada
        total_estimated = len(full_dataset) * augmentation_multiplier * epochs
        st.write(f"### üî¢ **Total de Imagens Original: {len(full_dataset)}**")
        st.write(f"### üî¢ **Multiplicador Estimado por √âpoca: ~{augmentation_multiplier}x**")
        st.write(f"### üî¢ **Total Estimado Durante {epochs} √âpocas: ~{total_estimated:,} imagens aumentadas**")
        st.info(f"üí° **Explica√ß√£o:** Durante o treinamento, cada uma das {len(full_dataset)} imagens originais ser√° " +
                f"transformada aleatoriamente a cada √©poca, gerando aproximadamente {augmentation_multiplier}x varia√ß√µes √∫nicas " +
                f"ao longo de {epochs} √©pocas, totalizando cerca de {total_estimated:,} imagens processadas.")
    
    st.write("---")
    
    # Criar o dataset personalizado com aumento de dados
    train_dataset = CustomDataset(full_dataset, transform=train_transform)
    valid_dataset = CustomDataset(full_dataset, transform=test_transforms)
    test_dataset = CustomDataset(full_dataset, transform=test_transforms)

    # Dividir os √≠ndices para treino, valida√ß√£o e teste
    dataset_size = len(full_dataset)
    indices = list(range(dataset_size))
    np.random.shuffle(indices)

    train_end = int(train_split * dataset_size)
    valid_end = int((train_split + valid_split) * dataset_size)

    train_indices = indices[:train_end]
    valid_indices = indices[train_end:valid_end]
    test_indices = indices[valid_end:]

    train_dataset = torch.utils.data.Subset(train_dataset, train_indices)
    valid_dataset = torch.utils.data.Subset(valid_dataset, valid_indices)
    test_dataset = torch.utils.data.Subset(test_dataset, test_indices)

    # Dataloaders
    g = torch.Generator()
    g.manual_seed(42)

    if use_weighted_loss:
        targets = [full_dataset.targets[i] for i in train_indices]
        class_counts = np.bincount(targets)
        class_counts = class_counts + 1e-6  # Para evitar divis√£o por zero
        class_weights = 1.0 / class_counts
        class_weights = torch.FloatTensor(class_weights).to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
    else:
        criterion = nn.CrossEntropyLoss()

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)

    # Carregar o modelo
    model = get_model(model_name, num_classes, dropout_p=0.5, fine_tune=fine_tune)
    if model is None:
        return None

    # Definir o otimizador com L2 regularization (weight_decay)
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'AdamW':
        optimizer = optim.AdamW(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(trainable_params, lr=learning_rate, weight_decay=l2_lambda, momentum=0.9, nesterov=True)
    elif optimizer_name == 'Ranger' and ADVANCED_OPTIMIZERS_AVAILABLE:
        optimizer = optim_advanced.Ranger(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'Lion' and ADVANCED_OPTIMIZERS_AVAILABLE:
        optimizer = optim_advanced.Lion(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    else:
        st.warning(f"Otimizador {optimizer_name} n√£o dispon√≠vel. Usando Adam.")
        optimizer = optim.Adam(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    
    # Configurar Learning Rate Scheduler
    scheduler = None
    if scheduler_name == 'CosineAnnealingLR':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=learning_rate/100)
    elif scheduler_name == 'OneCycleLR':
        steps_per_epoch = len(train_loader)
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer, 
            max_lr=learning_rate*10, 
            epochs=epochs,
            steps_per_epoch=steps_per_epoch,
            pct_start=0.3
        )

    # Listas para armazenar as perdas e acur√°cias
    train_losses = []
    valid_losses = []
    train_accuracies = []
    valid_accuracies = []

    # Early Stopping
    best_valid_loss = float('inf')
    epochs_no_improve = 0
    
    # Par√¢metros para Mixup e CutMix
    use_mixup = (augmentation_type == 'mixup')
    use_cutmix = (augmentation_type == 'cutmix')
    mixup_alpha = 1.0
    cutmix_alpha = 1.0
    
    # Cache de par√¢metros para regulariza√ß√£o L1 (otimiza√ß√£o)
    trainable_params_list = list(filter(lambda p: p.requires_grad, model.parameters())) if l1_lambda > 0 else []

    # Treinamento
    for epoch in range(epochs):
        set_seed(42 + epoch)
        running_loss = 0.0
        running_corrects = 0
        model.train()

        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            
            # Aplicar Mixup ou CutMix se selecionado
            if use_mixup:
                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha)
            elif use_cutmix:
                inputs, labels_a, labels_b, lam = cutmix_data(inputs, labels, cutmix_alpha)
            
            try:
                outputs = model(inputs)
            except Exception as e:
                st.error(f"Erro durante o treinamento: {e}")
                return None

            # Calcular loss
            if use_mixup or use_cutmix:
                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
                _, preds = torch.max(outputs, 1)
            else:
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)
            
            # Adicionar regulariza√ß√£o L1 se configurado
            if l1_lambda > 0:
                l1_reg = torch.tensor(0., device=device)
                for param in trainable_params_list:
                    l1_reg += torch.norm(param, 1)
                loss = loss + l1_lambda * l1_reg
            
            loss.backward()
            optimizer.step()
            
            # Atualizar scheduler OneCycleLR a cada batch
            if scheduler_name == 'OneCycleLR' and scheduler is not None:
                scheduler.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data if not (use_mixup or use_cutmix) else preds == labels_a.data)

        epoch_loss = running_loss / len(train_dataset)
        epoch_acc = running_corrects.double() / len(train_dataset)
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc.item())

        # Valida√ß√£o
        model.eval()
        valid_running_loss = 0.0
        valid_running_corrects = 0

        with torch.no_grad():
            for inputs, labels in valid_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                valid_running_loss += loss.item() * inputs.size(0)
                valid_running_corrects += torch.sum(preds == labels.data)

        valid_epoch_loss = valid_running_loss / len(valid_dataset)
        valid_epoch_acc = valid_running_corrects.double() / len(valid_dataset)
        valid_losses.append(valid_epoch_loss)
        valid_accuracies.append(valid_epoch_acc.item())

        st.write(f'**√âpoca {epoch+1}/{epochs}**')
        st.write(f'Perda de Treino: {epoch_loss:.4f} | Acur√°cia de Treino: {epoch_acc:.4f}')
        st.write(f'Perda de Valida√ß√£o: {valid_epoch_loss:.4f} | Acur√°cia de Valida√ß√£o: {valid_epoch_acc:.4f}')

        # Atualizar scheduler CosineAnnealingLR ap√≥s cada √©poca
        if scheduler_name == 'CosineAnnealingLR' and scheduler is not None:
            scheduler.step()

        # Early Stopping
        if valid_epoch_loss < best_valid_loss:
            best_valid_loss = valid_epoch_loss
            epochs_no_improve = 0
            best_model_wts = model.state_dict()
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                st.write('Early stopping!')
                model.load_state_dict(best_model_wts)
                break

    # Carregar os melhores pesos do modelo
    model.load_state_dict(best_model_wts)

    # Gr√°ficos de Perda e Acur√°cia
    plot_metrics(epochs, train_losses, valid_losses, train_accuracies, valid_accuracies)

    # Avalia√ß√£o Final no Conjunto de Teste
    st.write("**Avalia√ß√£o no Conjunto de Teste**")
    compute_metrics(model, test_loader, full_dataset.classes)

    # An√°lise de Erros
    st.write("**An√°lise de Erros**")
    error_analysis(model, test_loader, full_dataset.classes)

    # Liberar mem√≥ria
    del train_loader, valid_loader
    gc.collect()

    return model, full_dataset.classes

def plot_metrics(epochs, train_losses, valid_losses, train_accuracies, valid_accuracies):
    """
    Plota os gr√°ficos de perda e acur√°cia.
    """
    epochs_range = range(1, len(train_losses)+1)
    fig, ax = plt.subplots(1, 2, figsize=(14, 5))

    # Gr√°fico de Perda
    ax[0].plot(epochs_range, train_losses, label='Treino')
    ax[0].plot(epochs_range, valid_losses, label='Valida√ß√£o')
    ax[0].set_title('Perda por √âpoca')
    ax[0].set_xlabel('√âpocas')
    ax[0].set_ylabel('Perda')
    ax[0].legend()

    # Gr√°fico de Acur√°cia
    ax[1].plot(epochs_range, train_accuracies, label='Treino')
    ax[1].plot(epochs_range, valid_accuracies, label='Valida√ß√£o')
    ax[1].set_title('Acur√°cia por √âpoca')
    ax[1].set_xlabel('√âpocas')
    ax[1].set_ylabel('Acur√°cia')
    ax[1].legend()

    st.pyplot(fig)
    plt.close(fig)

def compute_metrics(model, dataloader, classes):
    """
    Calcula m√©tricas detalhadas e exibe matriz de confus√£o e relat√≥rio de classifica√ß√£o.
    """
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probabilities.cpu().numpy())

    # Relat√≥rio de Classifica√ß√£o
    report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True, zero_division=0)
    st.text("Relat√≥rio de Classifica√ß√£o:")
    st.write(pd.DataFrame(report).transpose())

    # Matriz de Confus√£o Normalizada
    cm = confusion_matrix(all_labels, all_preds, normalize='true')
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=ax)
    ax.set_xlabel('Predito')
    ax.set_ylabel('Verdadeiro')
    ax.set_title('Matriz de Confus√£o Normalizada')
    st.pyplot(fig)
    plt.close(fig)

    # Curva ROC
    if len(classes) == 2:
        fpr, tpr, thresholds = roc_curve(all_labels, [p[1] for p in all_probs])
        roc_auc = roc_auc_score(all_labels, [p[1] for p in all_probs])
        fig, ax = plt.subplots()
        ax.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)
        ax.plot([0, 1], [0, 1], 'k--')
        ax.set_xlabel('Taxa de Falsos Positivos')
        ax.set_ylabel('Taxa de Verdadeiros Positivos')
        ax.set_title('Curva ROC')
        ax.legend(loc='lower right')
        st.pyplot(fig)
        plt.close(fig)
    else:
        # Multiclasse
        binarized_labels = label_binarize(all_labels, classes=range(len(classes)))
        roc_auc = roc_auc_score(binarized_labels, np.array(all_probs), average='weighted', multi_class='ovr')
        st.write(f"AUC-ROC M√©dia Ponderada: {roc_auc:.4f}")

def error_analysis(model, dataloader, classes):
    """
    Realiza an√°lise de erros mostrando algumas imagens mal classificadas.
    """
    model.eval()
    misclassified_images = []
    misclassified_labels = []
    misclassified_preds = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            incorrect = preds != labels
            if incorrect.any():
                misclassified_images.extend(inputs[incorrect].cpu())
                misclassified_labels.extend(labels[incorrect].cpu())
                misclassified_preds.extend(preds[incorrect].cpu())
                if len(misclassified_images) >= 5:
                    break

    if misclassified_images:
        st.write("Algumas imagens mal classificadas:")
        num_images = min(5, len(misclassified_images))
        fig, axes = plt.subplots(1, num_images, figsize=(15, 3))
        
        # Handle case when only one image (axes is not an array)
        if num_images == 1:
            axes = [axes]
            
        for i in range(num_images):
            image = misclassified_images[i]
            # Denormalize the image for proper display
            image = denormalize_image(image)
            axes[i].imshow(image)
            axes[i].set_title(f"V: {classes[misclassified_labels[i]]}\nP: {classes[misclassified_preds[i]]}")
            axes[i].axis('off')
        st.pyplot(fig)
        plt.close(fig)
    else:
        st.write("Nenhuma imagem mal classificada encontrada.")

def extract_features(dataset, model, batch_size):
    """
    Extrai caracter√≠sticas de um conjunto de dados usando um modelo pr√©-treinado.
    """
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker)

    features = []
    labels = []

    model.eval()
    with torch.no_grad():
        for inputs, lbls in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            outputs = outputs.view(outputs.size(0), -1)  # Flatten
            features.append(outputs.cpu().numpy())
            labels.extend(lbls.numpy())

    features = np.concatenate(features, axis=0)
    labels = np.array(labels)
    return features, labels

def perform_clustering(features, num_clusters):
    """
    Aplica algoritmos de clustering √†s caracter√≠sticas.
    """
    # Clustering Hier√°rquico
    hierarchical = AgglomerativeClustering(n_clusters=num_clusters)
    hierarchical_labels = hierarchical.fit_predict(features)

    # K-Means
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    kmeans_labels = kmeans.fit_predict(features)

    return hierarchical_labels, kmeans_labels

def evaluate_clustering(true_labels, cluster_labels, method_name):
    """
    Avalia os resultados do clustering comparando com as classes reais.
    """
    ari = adjusted_rand_score(true_labels, cluster_labels)
    nmi = normalized_mutual_info_score(true_labels, cluster_labels)
    st.write(f"**M√©tricas para {method_name}:**")
    st.write(f"Adjusted Rand Index: {ari:.4f}")
    st.write(f"Normalized Mutual Information Score: {nmi:.4f}")

def visualize_clusters(features, true_labels, hierarchical_labels, kmeans_labels, classes):
    """
    Visualiza os clusters usando redu√ß√£o de dimensionalidade e inclui as classes verdadeiras com nomes de r√≥tulos.
    """
    # Redu√ß√£o de dimensionalidade com PCA para visualizar os clusters em 2D
    pca = PCA(n_components=2)
    reduced_features = pca.fit_transform(features)

    # Mapear os r√≥tulos verdadeiros para os nomes das classes
    true_labels_named = [classes[label] for label in true_labels]
    
    # Usar as cores distintas e vis√≠veis para garantir que os clusters sejam claramente separados
    color_palette = sns.color_palette("tab10", len(set(true_labels)))

    fig, axes = plt.subplots(1, 3, figsize=(21, 6))  # Agora temos 3 gr√°ficos: Hierarchical, K-Means e classes verdadeiras

    # Clustering Hier√°rquico
    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=hierarchical_labels, palette="deep", ax=axes[0], legend='full')
    axes[0].set_title('Clustering Hier√°rquico')
    ari_hierarchical = adjusted_rand_score(true_labels, hierarchical_labels)
    nmi_hierarchical = normalized_mutual_info_score(true_labels, hierarchical_labels)
    axes[0].text(0.1, 0.9, f"ARI: {ari_hierarchical:.2f}\nNMI: {nmi_hierarchical:.2f}", horizontalalignment='center', verticalalignment='center', transform=axes[0].transAxes, bbox=dict(facecolor='white', alpha=0.5))

    # K-Means Clustering
    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=kmeans_labels, palette="deep", ax=axes[1], legend='full')
    axes[1].set_title('K-Means Clustering')
    ari_kmeans = adjusted_rand_score(true_labels, kmeans_labels)
    nmi_kmeans = normalized_mutual_info_score(true_labels, kmeans_labels)
    axes[1].text(0.1, 0.9, f"ARI: {ari_kmeans:.2f}\nNMI: {nmi_kmeans:.2f}", horizontalalignment='center', verticalalignment='center', transform=axes[1].transAxes, bbox=dict(facecolor='white', alpha=0.5))

    # Classes verdadeiras
    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=true_labels_named, palette=color_palette, ax=axes[2], legend='full')
    axes[2].set_title('Classes Verdadeiras')

    # Exibir os gr√°ficos
    st.pyplot(fig)
    plt.close(fig)

def evaluate_image(model, image, classes):
    """
    Avalia uma √∫nica imagem e retorna a classe predita e a confian√ßa.
    """
    model.eval()
    image_tensor = test_transforms(image).unsqueeze(0).to(device)
    with torch.no_grad():
        output = model(image_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        confidence, predicted = torch.max(probabilities, 1)
        class_idx = predicted.item()
        class_name = classes[class_idx]
        return class_name, confidence.item()

#________________________________________________

#________________________________________________

def visualize_activations(model, image, class_names, gradcam_type='SmoothGradCAMpp'):
    """
    Visualiza as ativa√ß√µes na imagem usando diferentes variantes de Grad-CAM.
    
    Args:
        model: Modelo treinado
        image: Imagem PIL
        class_names: Lista de nomes das classes
        gradcam_type: Tipo de Grad-CAM ('GradCAM', 'GradCAMpp', 'SmoothGradCAMpp', 'LayerCAM')
    
    Returns:
        activation_map_resized: Mapa de ativa√ß√£o normalizado
    """
    try:
        # Ensure model is in eval mode and enable gradients for Grad-CAM
        model.eval()
        
        # Enable requires_grad for necessary layers
        for param in model.parameters():
            param.requires_grad = True
        
        input_tensor = test_transforms(image).unsqueeze(0).to(device)
        input_tensor.requires_grad = True
        
        # Verificar se o modelo √© suportado
        model_type = type(model).__name__
        if 'ResNet' in model_type:
            target_layer = model.layer4[-1]
        elif 'DenseNet' in model_type:
            target_layer = model.features.denseblock4.denselayer16
        else:
            st.error("Modelo n√£o suportado para Grad-CAM.")
            return None
        
        # Criar o objeto CAM usando torchcam
        if gradcam_type == 'GradCAM':
            cam_extractor = GradCAM(model, target_layer=target_layer)
        elif gradcam_type == 'GradCAMpp':
            cam_extractor = GradCAMpp(model, target_layer=target_layer)
        elif gradcam_type == 'SmoothGradCAMpp':
            cam_extractor = SmoothGradCAMpp(model, target_layer=target_layer)
        elif gradcam_type == 'LayerCAM':
            cam_extractor = LayerCAM(model, target_layer=target_layer)
        else:
            st.error(f"Tipo de Grad-CAM n√£o suportado: {gradcam_type}")
            return None
        
        # Habilitar gradientes explicitamente
        with torch.set_grad_enabled(True):
            out = model(input_tensor)  # Faz a previs√£o
            _, pred = torch.max(out, 1)  # Obt√©m a classe predita
            pred_class = pred.item()
        
        # Gerar o mapa de ativa√ß√£o
        activation_map = cam_extractor(pred_class, out)
        
        # Obter o mapa de ativa√ß√£o da primeira imagem no lote
        activation_map = activation_map[0].cpu().detach().numpy()
        
        # Redimensionar o mapa de ativa√ß√£o para coincidir com o tamanho da imagem original
        activation_map_resized = cv2.resize(activation_map, (image.size[0], image.size[1]))
        
        # Normalizar o mapa de ativa√ß√£o para o intervalo [0, 1]
        activation_map_resized = (activation_map_resized - activation_map_resized.min()) / (activation_map_resized.max() - activation_map_resized.min() + 1e-8)
        
        # Converter a imagem para array NumPy
        image_np = np.array(image)
        
        # Converter o mapa de ativa√ß√£o em uma imagem RGB
        heatmap = cv2.applyColorMap(np.uint8(255 * activation_map_resized), cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        # Sobrepor o mapa de ativa√ß√£o na imagem original
        superimposed_img = heatmap * 0.4 + image_np * 0.6
        superimposed_img = np.uint8(superimposed_img)
        
        # Exibir a imagem original e o mapa de ativa√ß√£o sobreposto
        fig, ax = plt.subplots(1, 2, figsize=(10, 5))
        
        # Imagem original
        ax[0].imshow(image_np)
        ax[0].set_title('Imagem Original')
        ax[0].axis('off')
        
        # Imagem com Grad-CAM
        ax[1].imshow(superimposed_img)
        ax[1].set_title(f'{gradcam_type}')
        ax[1].axis('off')
        
        # Exibir as imagens com o Streamlit
        st.pyplot(fig)
        plt.close(fig)
        
        return activation_map_resized
        
    except Exception as e:
        st.error(f"Erro ao gerar Grad-CAM: {str(e)}")
        st.info("Visualiza√ß√£o Grad-CAM n√£o dispon√≠vel para este modelo/configura√ß√£o.")
        return None




def main():

    # Definir o caminho do √≠cone
    icon_path = "logo.png"  # Verifique se o arquivo logo.png est√° no diret√≥rio correto
    
    # Verificar se o arquivo de √≠cone existe antes de configur√°-lo
    if os.path.exists(icon_path):
        st.set_page_config(page_title="Geomaker", page_icon=icon_path, layout="wide")
        logging.info(f"√çcone {icon_path} carregado com sucesso.")
    else:
        # Se o √≠cone n√£o for encontrado, carrega sem favicon
        st.set_page_config(page_title="Geomaker", layout="wide")
        logging.warning(f"√çcone {icon_path} n√£o encontrado, carregando sem favicon.")
    
    # Layout da p√°gina
    if os.path.exists('capa.png'):
        st.image('capa.png', caption='Laborat√≥rio de Educa√ß√£o e Intelig√™ncia Artificial - Geomaker. "A melhor forma de prever o futuro √© invent√°-lo." - Alan Kay', width='stretch')
    else:
        st.warning("Imagem 'capa.png' n√£o encontrada.")
    
    if os.path.exists("logo.png"):
        st.sidebar.image("logo.png", width=200)
    else:
        st.sidebar.text("Imagem do logotipo n√£o encontrada.")
    
    
  #___________________________________________________________
    st.title("Classifica√ß√£o por Imagens com Aprendizado Profundo")
    st.write("Este aplicativo permite treinar um modelo de classifica√ß√£o de imagens e aplicar algoritmos de clustering para an√°lise comparativa.")
    with st.expander("Transforma√ß√µes de Dados e Aumento de Dados no Treinamento de Redes Neurais"):
        st.write("""
        As **transforma√ß√µes de dados** e o **aumento de dados** s√£o t√©cnicas essenciais no treinamento de redes neurais profundas, principalmente em tarefas de vis√£o computacional. 
        Essas abordagens buscam melhorar a capacidade de generaliza√ß√£o dos modelos, gerando **imagens sint√©ticas** a partir dos dados de treinamento. Tais t√©cnicas s√£o particularmente 
        valiosas quando o conjunto de dados dispon√≠vel √© pequeno ou apresenta pouca diversidade. A normaliza√ß√£o, por sua vez, assegura que os valores dos pixels estejam em uma escala adequada, 
        resultando em um treinamento mais est√°vel e eficiente. Diversos estudos apontam que essas pr√°ticas s√£o eficazes para evitar **overfitting** e aumentar a robustez do modelo 
        (Shorten & Khoshgoftaar, 2019).
        """)
    
        st.write("### Aumento de Dados no Treinamento")
    
        st.write("""
        O **aumento de dados** ou *data augmentation* consiste na aplica√ß√£o de transforma√ß√µes aleat√≥rias √†s imagens do conjunto de treinamento para gerar novas amostras sint√©ticas. 
        No c√≥digo implementado, essa t√©cnica √© realizada com a classe `transforms.Compose` da biblioteca **torchvision**, que aplica uma sequ√™ncia de transforma√ß√µes.
        """)
    
        st.write("#### Transforma√ß√µes Aplicadas no Treinamento")
        
        st.write("""
        1. **RandomApply**: Aplica aleatoriamente um conjunto de transforma√ß√µes com 50% de probabilidade. Esse procedimento aumenta a variabilidade dos dados, gerando imagens diferentes a partir de uma √∫nica imagem de entrada.
       
        2. **RandomHorizontalFlip**: Realiza a invers√£o horizontal da imagem com 50% de probabilidade. Isso √© √∫til em cen√°rios onde a orienta√ß√£o horizontal da imagem n√£o altera seu significado, como em imagens de rochas ou melanomas.
    
        3. **RandomRotation(degrees=90)**: Rotaciona a imagem em at√© 90 graus, criando varia√ß√µes angulares, o que ajuda o modelo a reconhecer objetos independentemente da orienta√ß√£o.
    
        4. **ColorJitter**: Introduz varia√ß√µes de brilho, contraste, satura√ß√£o e matiz, simulando diferentes condi√ß√µes de ilumina√ß√£o e tornando o modelo mais robusto a mudan√ßas de ilumina√ß√£o.
    
        5. **RandomResizedCrop(224, scale=(0.8, 1.0))**: Realiza cortes aleat√≥rios na imagem e os redimensiona para 224x224 pixels, permitindo que diferentes partes da imagem sejam enfatizadas.
    
        6. **RandomAffine(degrees=0, shear=10)**: Aplica transforma√ß√µes afins, como cisalhamento, simulando distor√ß√µes que podem ocorrer no mundo real, como mudan√ßas de perspectiva.
    
        7. **Resize(256)**: Redimensiona a imagem para 256x256 pixels, assegurando que todas as imagens possuam a mesma dimens√£o.
    
        8. **CenterCrop(224)**: Recorta o centro da imagem, garantindo que o tamanho final seja 224x224 pixels.
    
        9. **ToTensor**: Converte a imagem para um tensor PyTorch, normalizando os valores dos pixels para o intervalo de [0,1], facilitando o processamento pelo modelo.
        """)
    
        st.write("### Gera√ß√£o de Imagens Sint√©ticas")
    
        st.write("""
        Essas transforma√ß√µes permitem que cada imagem original gere at√© **5 a 10 imagens sint√©ticas**. Por exemplo, em um conjunto de dados de 1000 imagens, 
        o processo pode expandir o conjunto para **5000 a 10000 imagens** ao longo do treinamento. Essa amplia√ß√£o artificial do conjunto de dados reduz o risco de **overfitting**, 
        permitindo que o modelo treine em um conjunto "maior" e mais diverso, o que √© crucial para melhorar a generaliza√ß√£o do modelo em dados novos.
        """)
    
        st.write("### Normaliza√ß√£o nas Imagens de Teste e Valida√ß√£o")
    
        st.write("""
        Nas imagens de **teste** e **valida√ß√£o**, o aumento de dados n√£o √© aplicado. O objetivo nesses conjuntos √© avaliar o modelo de maneira consistente, 
        utilizando imagens que representem o mais fielmente poss√≠vel os dados reais. No entanto, a normaliza√ß√£o dessas imagens √© fundamental para assegurar que seus valores de pixel 
        estejam adequados para as opera√ß√µes de aprendizado. Isso tamb√©m garante um desempenho est√°vel durante o treinamento.
        """)
    
        st.write("#### Transforma√ß√µes Aplicadas no Teste e Valida√ß√£o")
        
        st.write("""
        1. **Resize(256)**: Redimensiona a imagem para 256x256 pixels, garantindo que todas as imagens tenham o mesmo tamanho inicial.
    
        2. **CenterCrop(224)**: Realiza o corte central para que as dimens√µes da imagem sejam 224x224 pixels, correspondendo ao tamanho esperado pelo modelo.
    
        3. **ToTensor**: Converte a imagem para tensor e normaliza os valores dos pixels para o intervalo de [0,1], o que melhora a estabilidade num√©rica e a taxa de converg√™ncia do treinamento.
        """)
    
        st.write("### Import√¢ncia da Normaliza√ß√£o")
    
        st.write("""
        A **normaliza√ß√£o** garante que os valores dos pixels estejam em uma escala apropriada para as opera√ß√µes aritm√©ticas realizadas no modelo, melhorando a estabilidade e o desempenho do processo de treinamento. 
        Ela tamb√©m contribui para a estabilidade num√©rica durante o c√°lculo do gradiente e para uma converg√™ncia mais eficiente do modelo (Nguy·ªÖn et al., 2021).
        """)
    
        st.write("### Conclus√£o")
    
        st.write("""
        O c√≥digo exemplifica a implementa√ß√£o eficaz de transforma√ß√µes de dados e aumento de dados como parte da pipeline de treinamento de redes neurais profundas. 
        As transforma√ß√µes aplicadas aumentam a diversidade do conjunto de treinamento, ajudando a mitigar o **overfitting** e melhorar a generaliza√ß√£o do modelo. 
        Al√©m disso, a normaliza√ß√£o aplicada aos dados de teste e valida√ß√£o garante que o desempenho do modelo seja avaliado de forma precisa e consistente, 
        alinhada √†s melhores pr√°ticas de aprendizado profundo.
        """)
    
        st.write("### Refer√™ncias")
        
        st.write("""
        - Huang, G., Liu, Z., Maaten, L., & Weinberger, K. (2017). Densely connected convolutional networks. https://doi.org/10.1109/cvpr.2017.243
        - Li, S. (2023). Clouddensenet: lightweight ground-based cloud classification method for large-scale datasets based on reconstructed densenet. *Sensors*, 23(18), 7957. https://doi.org/10.3390/s23187957
        - Nguy·ªÖn, H., Yu, G., Shin, N., Kwon, G., Kwak, W., & Kim, J. (2021). Defective product classification system for smart factory based on deep learning. *Electronics*, 10(7), 826. https://doi.org/10.3390/electronics10070826
        - Shorten, C. & Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. *Journal of Big Data*, 6(1). https://doi.org/10.1186/s40537-019-0197-0
        """)

    # Barra Lateral de Configura√ß√µes
    st.sidebar.title("Configura√ß√µes do Treinamento")
      # Imagem e Contatos___________________________
    #_______________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmulas LaTeX
    with st.sidebar:
        with st.expander("Discuss√£o sobre o N√∫mero de Classes em Modelos de Aprendizado Profundo"):
            st.write("""
            ### Introdu√ß√£o
    
            A discuss√£o sobre o n√∫mero de classes em modelos de aprendizado profundo √© fundamental para a compreens√£o da arquitetura e do desempenho de redes neurais em tarefas de classifica√ß√£o. O n√∫mero de classes refere-se ao total de categorias ou r√≥tulos que um modelo deve prever, e a configura√ß√£o correta desse par√¢metro impacta diretamente o desempenho do modelo, pois afeta a dimens√£o da sa√≠da da rede neural e a complexidade da tarefa. O n√∫mero de classes pode variar de tarefas bin√°rias, que envolvem apenas duas classes, at√© problemas com centenas ou milhares de classes, como nas classifica√ß√µes de imagens do **ImageNet** (Cheng, 2023).
            """)
    
            st.write("### Impacto do N√∫mero de Classes")
            st.write("""
            O n√∫mero de classes define a estrutura da √∫ltima camada da rede neural, que √© respons√°vel por realizar as predi√ß√µes. Para um problema de **classifica√ß√£o bin√°ria**, o modelo ter√° uma √∫nica sa√≠da que prev√™ a probabilidade de uma classe ou outra. Em contrapartida, em um problema de **classifica√ß√£o multiclasse**, o n√∫mero de sa√≠das ser√° igual ao n√∫mero de categorias poss√≠veis (Cheng, 2023). A fun√ß√£o de ativa√ß√£o utilizada na √∫ltima camada √© crucial para a interpreta√ß√£o dos resultados. A equa√ß√£o que representa essa rela√ß√£o pode ser expressa como:
            """)
            st.latex(r'''
            \mathbf{y} = \text{Softmax}(Wx + b)
            ''')
    
            st.write("""
            onde **W** e **b** s√£o os pesos e o bias, respectivamente, que conectam a camada anterior √†s classes de sa√≠da. O resultado √© passado pela fun√ß√£o **softmax**, que converte os valores em probabilidades associadas a cada classe (Petrovska et al., 2020).
            """)
    
                       
            st.write("""
            Em tarefas de classifica√ß√£o bin√°ria, o modelo tem apenas duas classes poss√≠veis, como **detec√ß√£o de fraude** ou **diagn√≥stico de doen√ßas** (positivo ou negativo). Nesse caso, a fun√ß√£o de ativa√ß√£o final √© geralmente a **sigmoide**, que retorna uma probabilidade entre 0 e 1 para cada entrada. Um limiar √© ent√£o aplicado para decidir a classe final predita pelo modelo (Cheng, 2023).
            """)
    
            st.write("### Classifica√ß√£o Multiclasse")
            st.write("""
            Em problemas de classifica√ß√£o multiclasse, o n√∫mero de classes pode variar consideravelmente. Por exemplo, em tarefas de **classifica√ß√£o de imagens geol√≥gicas**, o n√∫mero de classes pode ser pequeno, mas em aplica√ß√µes como a **classifica√ß√£o de imagens m√©dicas** ou **reconhecimento facial**, o n√∫mero de classes pode ser muito maior. A arquitetura da rede deve ser ajustada para garantir que a √∫ltima camada tenha o n√∫mero correto de sa√≠das correspondente ao n√∫mero de categorias (Cheng, 2023; Sardeshmukh, 2023).
            """)
    
            st.write("### Classifica√ß√£o Multirr√≥tulo")
            st.write("""
            Em problemas de **classifica√ß√£o multirr√≥tulo**, uma entrada pode pertencer a mais de uma classe ao mesmo tempo. Nesse cen√°rio, o n√∫mero de sa√≠das da rede neural √© igual ao n√∫mero de classes poss√≠veis, mas cada sa√≠da √© independente das demais. A fun√ß√£o de ativa√ß√£o usada √© a **sigmoide**, pois ela calcula a probabilidade de cada classe independentemente das outras (Petrovska et al., 2020).
            """)
    
            st.write("### Efeitos do N√∫mero de Classes no Desempenho")
            st.write("""
            O n√∫mero de classes influencia diretamente a complexidade do modelo e o tempo de treinamento. Conforme o n√∫mero de classes aumenta, a tarefa de classifica√ß√£o se torna mais dif√≠cil, exigindo mais par√¢metros e tempo de computa√ß√£o. Al√©m disso, um maior n√∫mero de classes aumenta o risco de **sobreajuste** (overfitting), especialmente em conjuntos de dados pequenos (Cheng, 2023; Suhana, 2022).
            """)
    
            st.write("### Conclus√£o")
            st.write("""
            O n√∫mero de classes √© um fator determinante na defini√ß√£o da arquitetura de redes neurais para tarefas de classifica√ß√£o. Seja em problemas bin√°rios, multiclasse ou multirr√≥tulo, a escolha adequada desse par√¢metro garante que a rede neural seja capaz de aprender as caracter√≠sticas relevantes de cada categoria. Em problemas com muitas classes, estrat√©gias como a **regulariza√ß√£o** e o **data augmentation** podem ser utilizadas para melhorar o desempenho do modelo, evitando o sobreajuste (Cheng, 2023; Sardeshmukh, 2023).
            """)
    
            st.write("### Refer√™ncias")
          
            st.write("""
            1. Cheng, R. (2023). Expansion of the CT-scans image set based on the pretrained DCGAN for improving the performance of the CNN. *Journal of Physics Conference Series*, 2646(1), 012015. https://doi.org/10.1088/1742-6596/2646/1/012015
            2. Petrovska, B., Atanasova-Pacemska, T., Corizzo, R., Mignone, P., Lameski, P., & Zdravevski, E. (2020). Aerial Scene Classification through Fine-Tuning with Adaptive Learning Rates and Label Smoothing. *Applied Sciences*, 10(17), 5792. https://doi.org/10.3390/app10175792
            3. Sardeshmukh, M. (2023). Crop image classification using convolutional neural network. *Multidisciplinary Science Journal*, 5(4), 2023039. https://doi.org/10.31893/multiscience.2023039
            4. Suhana, R. (2022). Fish Image Classification Using Adaptive Learning Rate In Transfer Learning Method. *Knowledge Engineering and Data Science*, 5(1), 67-77. https://doi.org/10.17977/um018v5i12022p67-77
            """)

  
    # Nota: O n√∫mero de classes ser√° detectado automaticamente do dataset
    num_classes = st.sidebar.number_input("N√∫mero de Classes (ser√° detectado automaticamente):", min_value=1, step=1, value=2, disabled=True, help="Este valor ser√° automaticamente detectado do dataset ap√≥s o upload")
    #_______________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmula LaTeX
    with st.sidebar:
        with st.expander("Modelos Pr√©-Treinados: ResNet18, ResNet50 e DenseNet121:"):
            st.write("""
            ### Introdu√ß√£o
        
            As redes neurais convolucionais (CNNs) t√™m se tornado uma ferramenta essencial no campo do aprendizado profundo, especialmente em tarefas de vis√£o computacional, como a classifica√ß√£o de imagens. 
            Modelos como **ResNet18**, **ResNet50** e **DenseNet121** s√£o amplamente reconhecidos por seu desempenho superior em competi√ß√µes de classifica√ß√£o de imagens, como o **ImageNet**. Esses modelos s√£o considerados 
            **pr√©-treinados**, pois foram inicialmente treinados em grandes conjuntos de dados, permitindo que sejam reutilizados e ajustados para novas tarefas espec√≠ficas, uma pr√°tica conhecida como **transfer√™ncia de aprendizado** 
            (Cheng, 2023; Petrovska et al., 2020; Alaoui, 2023).
            """)
        
            st.write("### ResNet18 e ResNet50")
            st.write("""
            A arquitetura **ResNet** (Rede Residual) foi desenvolvida para mitigar o problema de **degrada√ß√£o** que ocorre em redes neurais muito profundas, onde o aumento do n√∫mero de camadas pode levar a uma diminui√ß√£o no desempenho.
            A inova√ß√£o dos **blocos residuais** permite que algumas camadas "saltem" conex√µes, aprendendo uma **fun√ß√£o de identidade** em vez de novas representa√ß√µes para cada camada. Essa abordagem facilita o treinamento de redes mais profundas, pois a fun√ß√£o residual pode ser aprendida de forma mais eficiente (Zhang et al., 2018; Sandotra et al., 2023; Petrovska et al., 2020).
            """)
            
            st.latex(r'''
            \mathbf{y} = \mathcal{F}(x, \{W_i\}) + x
            ''')
            
            st.write("""
            onde 
            """)
            st.latex(r'''
            \mathcal{F}(x, \{W_i\}) + x
            ''')
          
            st.write("""
            representa a fun√ß√£o aprendida e x √© a entrada. O termo x √© adicionado √† sa√≠da, o que simplifica o processo de treinamento e permite que redes mais profundas sejam treinadas com maior efic√°cia 
            ("A Framework for Flood Extent Mapping using CNN Transfer Learning", 2022; Petrovska et al., 2020).
            """)
        
            st.write("""
            O modelo **ResNet18** possui 18 camadas trein√°veis e √© uma vers√£o mais leve, adequada para aplica√ß√µes com restri√ß√µes de recursos computacionais, enquanto o **ResNet50**, com 50 camadas, √© capaz de capturar padr√µes mais complexos em imagens, sendo ideal para tarefas que exigem maior profundidade de an√°lise (Sandotra et al., 2023; Qin et al., 2019; Petrovska et al., 2020).
            """)
        
            st.write("""
            Ambos os modelos foram pr√©-treinados no conjunto de dados **ImageNet**, o que facilita a **transfer√™ncia de aprendizado** em novos dom√≠nios. As camadas iniciais desses modelos j√° s√£o capazes de identificar caracter√≠sticas gerais, acelerando o processo de treinamento em conjuntos de dados menores e espec√≠ficos, como em aplica√ß√µes m√©dicas ou de classifica√ß√£o de imagens geol√≥gicas (Cheng, 2023; Petrovska et al., 2020; Alaoui, 2023).
            """)
        
            st.write("### DenseNet121")
            st.write("""
            A arquitetura **DenseNet** (Rede Convolucional Densamente Conectada) oferece uma abordagem alternativa, onde todas as camadas est√£o interconectadas, promovendo a preserva√ß√£o do fluxo de gradiente e da informa√ß√£o original. Isso facilita a reutiliza√ß√£o das representa√ß√µes intermedi√°rias e otimiza a efici√™ncia do modelo. A equa√ß√£o que expressa essa estrutura √©:
            """)
        
            st.latex(r'''
            \mathbf{x}_l = H_l(\mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_{l-1})
            ''')
        
            st.write("""
            onde
            """)
          
            st.latex(r'''
            \mathbf{x}_l 
            ''')
          
            st.write("""
            √© a sa√≠da da l-√©sima camada e 
            """)
          
            st.latex(r'''
             \mathbf{H}_l
            ''')
          
            st.write("""
            √© a fun√ß√£o aplicada. Essa configura√ß√£o otimiza o uso de gradientes e representa√ß√µes, resultando em um desempenho superior em tarefas de classifica√ß√£o 
            (Benegui & Ionescu, 2020; Varshni et al., 2019; Hamdaoui et al., 2021).
            """)
        
            st.write("""
            O modelo **DenseNet121**, que possui 121 camadas trein√°veis, √© particularmente eficaz em contextos onde a efici√™ncia √© crucial, maximizando o uso de recursos computacionais e facilitando a extra√ß√£o de caracter√≠sticas relevantes de imagens (Sardeshmukh, 2023; Hamdaoui et al., 2021).
            """)
        
            st.write("### Transfer√™ncia de Aprendizado e Ajuste Fino")
            st.write("""
            A utiliza√ß√£o de modelos pr√©-treinados, como ResNet18, ResNet50 e DenseNet121, √© uma t√©cnica de **transfer√™ncia de aprendizado** que permite que o conhecimento adquirido em tarefas anteriores seja aplicado a novos problemas. 
            Em vez de treinar um modelo do zero, o ajuste fino √© realizado nas camadas do modelo para se adaptar a um novo conjunto de dados, permitindo que caracter√≠sticas espec√≠ficas sejam aprendidas de forma mais eficiente. Por exemplo, em aplica√ß√µes de **classifica√ß√£o de melanomas** ou **an√°lise de rochas vulc√¢nicas**, as camadas mais profundas dos modelos s√£o ajustadas para entender caracter√≠sticas espec√≠ficas de imagens m√©dicas ou geol√≥gicas (Suhana, 2022; Petrovska et al., 2020).
            """)
        
            st.write("""
            Estudos demonstram que a transfer√™ncia de aprendizado √© especialmente eficaz ao se trabalhar com conjuntos de dados pequenos. O uso de modelos pr√©-treinados pode proporcionar resultados semelhantes ou at√© superiores aos de modelos treinados a partir do zero, reduzindo o tempo de treinamento e melhorando a precis√£o (Raghava et al., 2019; Alaoui, 2023; Ahmed, 2021).
            """)
        
            st.write("### Conclus√£o")
            st.write("""
            As arquiteturas **ResNet18**, **ResNet50** e **DenseNet121** s√£o ferramentas poderosas no campo do aprendizado profundo, especialmente em tarefas de classifica√ß√£o de imagens. Seu pr√©-treinamento em grandes conjuntos de dados, como o **ImageNet**, e a capacidade de serem ajustados para novas tarefas atrav√©s da transfer√™ncia de aprendizado, tornam esses modelos ideais para uma ampla gama de aplica√ß√µes, incluindo a classifica√ß√£o de imagens m√©dicas e geol√≥gicas. O uso dessas arquiteturas n√£o apenas reduz o tempo de treinamento, mas tamb√©m melhora a precis√£o e a efic√°cia em diversas √°reas de pesquisa e aplica√ß√£o pr√°tica (Zeimarani et al., 2020; "Dog Breed Identification with Fine Tuning of Pre-trained Models", 2019; Awais et al., 2020).
            """)
        
            st.write("### Refer√™ncias")
        
            st.write("""
            - (2019). Dog breed identification with fine tuning of pre-trained models. *International Journal of Recent Technology and Engineering*, 8(2S11), 3677-3680. https://doi.org/10.35940/ijrte.b1464.0982s1119
            - (2022). A framework for flood extent mapping using cnn transfer learning. https://doi.org/10.17762/ijisae.v10i3s.2426
            - Ahmed, A. (2021). Pre-trained cnns models for content based image retrieval. *International Journal of Advanced Computer Science and Applications*, 12(7). https://doi.org/10.14569/ijacsa.2021.0120723
            - Alaoui, A. (2023). Pre-trained cnns: evaluating emergency vehicle image classification. *Data & Metadata*, 2, 153. https://doi.org/10.56294/dm2023153
            - Benegui, C. and Ionescu, R. (2020). Convolutional neural networks for user identification based on motion sensors represented as images. *IEEE Access*, 8, 61255-61266. https://doi.org/10.1109/access.2020.2984214
            - Cheng, R. (2023). Expansion of the ct-scans image set based on the pretrained dcgan for improving the performance of the cnn. *Journal of Physics Conference Series*, 2646(1), 012015. https://doi.org/10.1088/1742-6596/2646/1/012015
            - Hamdaoui, H., Ben-fares, A., Boujraf, S., Chaoui, N., Alami, B., Ma√¢roufi, M., ‚Ä¶ & Qjidaa, H. (2021). High precision brain tumor classification model based on deep transfer learning and stacking concepts. *Indonesian Journal of Electrical Engineering and Computer Science*, 24(1), 167. https://doi.org/10.11591/ijeecs.v24.i1.pp167-177
            - Petrovska, B., Atanasova-Pacemska, T., Corizzo, R., Mignone, P., Lameski, P., & Zdravevski, E. (2020). Aerial scene classification through fine-tuning with adaptive learning rates and label smoothing. *Applied Sciences*, 10(17), 5792. https://doi.org/10.3390/app10175792
            - Raghava, Y., Kuthadi, V., & Rajalakshmi, S. (2019). Enhanced deep learning with featured transfer learning in identifying disguised faces. *International Journal of Innovative Technology and Exploring Engineering*, 8(10), 1257-1260. https://doi.org/10.35940/ijitee.h7286.0881019
            - Sandotra, N., Mahajan, P., Abrol, P., & Lehana, P. (2023). Analyzing performance of deep learning models under the presence of distortions in identifying plant leaf disease. *International Journal of Informatics and Communication Technology (IJ-ICT)*, 12(2), 115. https://doi.org/10.11591/ijict.v12i2.pp115-126
            - Sardeshmukh, M. (2023). Crop image classification using convolutional neural network. *Multidisciplinary Science Journal*, 5(4), 2023039. https://doi.org/10.31893/multiscience.2023039
            - Suhana, R. (2022). Fish image classification using adaptive learning rate in transfer learning method. *Knowledge Engineering and Data Science*, 5(1), 67. https://doi.org/10.17977/um018v5i12022p67-77
            - Varshni, D., Thakral, K., Agarwal, L., Nijhawan, R., & Mittal, A. (2019). Pneumonia detection using cnn based feature extraction. https://doi.org/10.1109/icecct.2019.8869364
            - Zeimarani, B., Costa, M., Nurani, N., Bianco, S., Pereira, W., & Filho, C. (2020). Breast lesion classification in ultrasound images using deep convolutional neural network. *IEEE Access*, 8, 133349-133359. https://doi.org/10.1109/access.2020.3010863
            - Zhang, B., Wang, C., Shen, Y., & Liu, Y. (2018). Fully connected conditional random fields for high-resolution remote sensing land use/land cover classification with convolutional neural networks. *Remote Sensing*, 10(12), 1889. https://doi.org/10.3390/rs10121889
            """)

    model_name = st.sidebar.selectbox("Modelo Pr√©-treinado:", options=['ResNet18', 'ResNet50', 'DenseNet121'])

    #________________________________________________________________________________________
    # Fine-Tuning Completo em Redes Neurais Profundas
    with st.sidebar:
        with st.expander("Fine-Tuning Completo em Redes Neurais Profundas:"):
            st.write("""
            ### Introdu√ß√£o
        
            O **fine-tuning** (ajuste fino) √© uma t√©cnica poderosa utilizada para ajustar redes neurais pr√©-treinadas em novos conjuntos de dados. No contexto de redes como a **ResNet18**, **ResNet50** ou **DenseNet121**, que foram inicialmente treinadas em grandes bases de dados (como o **ImageNet**), o fine-tuning permite que essas redes sejam adaptadas a novos problemas, como a **classifica√ß√£o de melanomas** ou de **rochas vulc√¢nicas e plut√¥nicas**. Ao realizar o fine-tuning, todas as camadas do modelo s√£o atualizadas para refletir as caracter√≠sticas do novo conjunto de dados, ao inv√©s de congelar as camadas iniciais, o que permite uma adapta√ß√£o mais profunda e precisa ao novo problema (Piotrowski & Napiorkowski, 2013; Friedrich et al., 2022).
            """)
        
            st.write("""
            ### Fundamenta√ß√£o Te√≥rica
        
            O conceito de fine-tuning √© baseado no princ√≠pio de **transfer√™ncia de aprendizado**, no qual um modelo pr√©-treinado em um grande conjunto de dados gen√©ricos √© reaproveitado para um novo problema espec√≠fico. Essa abordagem √© particularmente √∫til quando o novo conjunto de dados √© relativamente pequeno, pois o modelo j√° foi treinado para capturar padr√µes gerais em dados visuais (como bordas, texturas e formas), o que pode acelerar o treinamento e melhorar a precis√£o final (Al‚Äêrimy et al., 2023; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            Ao utilizar o fine-tuning completo, todas as camadas do modelo s√£o ajustadas com base nos novos dados. Isso significa que os pesos das camadas profundas do modelo, que foram aprendidos durante o treinamento inicial, s√£o atualizados para se adequar √†s caracter√≠sticas espec√≠ficas do novo conjunto de dados. Matematicamente, essa abordagem pode ser descrita como a otimiza√ß√£o da seguinte fun√ß√£o de perda:
            """)
        
            st.latex(r'''
            L_{\text{fine-tuning}} = L_{\text{original}} + \lambda \sum_{i} w_i^2
            ''')
        
            st.write("""
            Onde:
            """)
          
            st.latex(r'''
            L_{\text{fine-tuning}}
            ''')
          
            st.write("""
            √© a fun√ß√£o de perda durante o fine-tuning;
            """)
          
            st.latex(r'''
            L_{\text{original}}
            ''')
          
            st.write("""
            representa a fun√ß√£o de perda original do modelo pr√©-treinado;
            """)
          
            st.latex(r'''
            \lambda
            ''')
          
            st.write("""
            √© o coeficiente de regulariza√ß√£o (no caso de utilizar a regulariza√ß√£o L2);
            """)
          
            st.latex(r'''
            w_i
            ''')
            st.write("""
            s√£o os pesos individuais que ser√£o atualizados durante o processo de fine-tuning (Friedrich et al., 2022; Al‚Äêrimy et al., 2023).
            """)
        
            st.write("""
            ### Benef√≠cios do Fine-Tuning Completo
        
            O fine-tuning completo oferece v√°rios benef√≠cios, especialmente quando o novo conjunto de dados difere substancialmente do conjunto no qual o modelo foi originalmente treinado. No caso da **classifica√ß√£o de melanomas** ou **rochas**, por exemplo, as caracter√≠sticas visuais dos dados podem ser muito diferentes das imagens do **ImageNet**, que incluem uma ampla variedade de objetos, animais e cen√°rios (Piotrowski & Napiorkowski, 2013; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            Os principais benef√≠cios incluem:
            1. **Adapta√ß√£o Profunda**: Ao ajustar todas as camadas, o modelo consegue adaptar n√£o apenas as caracter√≠sticas gen√©ricas (como bordas e texturas), mas tamb√©m padr√µes mais complexos e espec√≠ficos do novo problema.
            2. **Melhoria da Precis√£o**: O fine-tuning completo geralmente resulta em melhorias significativas na precis√£o, especialmente quando os dados de treinamento s√£o limitados ou possuem caracter√≠sticas visuais √∫nicas (Friedrich et al., 2022; Al‚Äêrimy et al., 2023).
            3. **Generaliza√ß√£o Melhorada**: O processo de fine-tuning permite que o modelo generalize melhor para novos dados, uma vez que ele √© treinado para capturar padr√µes mais espec√≠ficos do novo dom√≠nio (Piotrowski & Napiorkowski, 2013; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            ### Compara√ß√£o com o Fine-Tuning Parcial
        
            Em contraste com o fine-tuning completo, no qual todas as camadas s√£o atualizadas, o **fine-tuning parcial** mant√©m algumas das camadas iniciais congeladas, atualizando apenas as camadas finais. Essa abordagem pode ser √∫til quando o novo conjunto de dados √© semelhante ao conjunto de dados original no qual o modelo foi treinado. No entanto, quando os dados diferem substancialmente, o fine-tuning completo tende a ser mais eficaz, pois permite uma adapta√ß√£o mais profunda e personalizada (Al‚Äêrimy et al., 2023; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            ### Efeitos do Fine-Tuning em Problemas Espec√≠ficos
        
            #### Classifica√ß√£o de Melanomas
        
            No caso da **classifica√ß√£o de melanomas**, o fine-tuning completo permite que o modelo identifique padr√µes visuais sutis na pele que podem ser indicativos de c√¢ncer. Essas caracter√≠sticas visuais podem incluir varia√ß√µes de textura, cor e bordas, que s√£o espec√≠ficas de imagens m√©dicas e diferem dos objetos comuns presentes em bases de dados gen√©ricas, como o **ImageNet** (Piotrowski & Napiorkowski, 2013; Friedrich et al., 2022).
            """)
        
            st.write("""
            #### Classifica√ß√£o de Rochas
        
            Para a **classifica√ß√£o de rochas vulc√¢nicas e plut√¥nicas**, o fine-tuning completo permite que o modelo capture padr√µes geol√≥gicos e estruturais espec√≠ficos, como varia√ß√µes de granula√ß√£o e texturas minerais. Novamente, esses padr√µes s√£o significativamente diferentes dos dados de objetos comuns, tornando o fine-tuning completo uma abordagem valiosa para melhorar a precis√£o da classifica√ß√£o (Friedrich et al., 2022; Al‚Äêrimy et al., 2023).
            """)
        
            st.write("""
            ### Considera√ß√µes Pr√°ticas
        
            Durante o processo de fine-tuning, √© importante monitorar o desempenho do modelo em um conjunto de valida√ß√£o para evitar o **overfitting**. Uma t√©cnica comum √© utilizar a **regulariza√ß√£o L2** ou o **dropout** para garantir que o modelo n√£o se ajuste excessivamente aos dados de treinamento (Piotrowski & Napiorkowski, 2013; Sakizadeh et al., 2015). Al√©m disso, a taxa de aprendizado deve ser cuidadosamente ajustada. Em muitos casos, utiliza-se uma taxa de aprendizado menor durante o fine-tuning para garantir que as atualiza√ß√µes dos pesos n√£o sejam muito dr√°sticas, preservando parte das informa√ß√µes aprendidas anteriormente.
            """)
        
            st.write("""
            ### Conclus√£o
        
            O fine-tuning completo √© uma t√©cnica eficaz para ajustar modelos pr√©-treinados, como a **ResNet18**, **ResNet50** ou **DenseNet121**, a novos conjuntos de dados. Ao permitir que todas as camadas do modelo sejam atualizadas, o fine-tuning completo oferece maior flexibilidade e precis√£o em problemas que diferem substancialmente dos dados originais. Quando combinado com outras t√©cnicas de regulariza√ß√£o, como a L2, o fine-tuning pode levar a modelos robustos e capazes de generalizar para novos dados, sendo uma ferramenta essencial no arsenal de t√©cnicas de aprendizado profundo.
            """)
        
            st.write("""
            ### Refer√™ncias
        
            - Al‚ÄêRIMY, B.; SAEED, F.; AL-SAREM, M.; ALBARRAK, A.; QASEM, S. An adaptive early stopping technique for densenet169-based knee osteoarthritis detection model. *Diagnostics*, 13(11), 1903, 2023. https://doi.org/10.3390/diagnostics13111903
            - FRIEDRICH, S. et al. Regularization approaches in clinical biostatistics: a review of methods and their applications. *Statistical Methods in Medical Research*, 32(2), 425-440, 2022. https://doi.org/10.1177/09622802221133557
            - PIOTROWSKI, A.; NAPIORKOWSKI, J. A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling. *Journal of Hydrology*, 476, 97-111, 2013. https://doi.org/10.1016/j.jhydrol.2012.10.019
            - REZAEEZADE, A.; BATINA, L. Regularizers to the rescue: fighting overfitting in deeplearning-based side-channel analysis. 2022. https://doi.org/10.21203/rs.3.rs-2386625/v1
            - SAKIZADEH, M.; MALIAN, A.; AHMADPOUR, E. Groundwater quality modeling with a small data set. *Ground Water*, 54(1), 115-120, 2015. https://doi.org/10.1111/gwat.12317
            """)

    fine_tune = st.sidebar.checkbox("Fine-Tuning Completo", value=False)
    epochs = st.sidebar.slider("N√∫mero de √âpocas:", min_value=1, max_value=500, value=200, step=1)
    learning_rate = st.sidebar.select_slider("Taxa de Aprendizagem:", options=[0.1, 0.01, 0.001, 0.0001], value=0.0001)
    batch_size = st.sidebar.selectbox("Tamanho de Lote:", options=[4, 8, 16, 32, 64], index=2)
    train_split = st.sidebar.slider("Percentual de Treinamento:", min_value=0.5, max_value=0.9, value=0.7, step=0.05)
    valid_split = st.sidebar.slider("Percentual de Valida√ß√£o:", min_value=0.05, max_value=0.4, value=0.15, step=0.05)
    #________________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmula LaTeX
    with st.sidebar:
        with st.expander("Implementa√ß√£o da T√©cnica de Regulariza√ß√£o L2 (Weight Decay):"):
            st.write("""
            ### Introdu√ß√£o
            A regulariza√ß√£o L2, frequentemente referida como *weight decay*, √© uma t√©cnica amplamente utilizada para mitigar o **overfitting** 
            em modelos de aprendizado de m√°quina, especialmente em redes neurais profundas. O *overfitting* ocorre quando o modelo se ajusta n√£o apenas 
            aos padr√µes dos dados de treinamento, mas tamb√©m ao ru√≠do presente, o que compromete sua capacidade de generaliza√ß√£o para novos dados 
            (Piotrowski & Napiorkowski, 2013). A regulariza√ß√£o L2 adiciona um termo de penaliza√ß√£o √† fun√ß√£o de perda do modelo, o que resulta em uma 
            redu√ß√£o dos valores absolutos dos pesos, promovendo, assim, modelos mais simples e generaliz√°veis (Friedrich et al., 2022).
            Esta revis√£o visa fornecer uma vis√£o clara e t√©cnica da aplica√ß√£o da regulariza√ß√£o L2, discutindo seus efeitos, a interpreta√ß√£o do coeficiente de regulariza√ß√£o 
            """)
          
            st.latex(r'''
            \lambda
            ''')
          
            st.write("""
            e as implica√ß√µes da escolha desse par√¢metro.
            """)
          
            st.latex(r'''
            L_{\text{total}} = L_{\text{original}} + \lambda \sum_{i} w_i^2
            ''')
          
            st.write("""
            Onde:
            """) 
            
            st.latex(r'''
            L_{\text{total}}
            ''')
          
            st.write("""
            √© a perda total que o modelo busca minimizar;
            """)
            
            st.latex(r'''
            L_{\text{original}}
            ''')
          
            st.write("""
            √© a fun√ß√£o de perda original (como a perda de entropia cruzada); Œª √© o coeficiente de regulariza√ß√£o, que controla a penalidade aplicada aos pesos;
            """)
          
            st.latex(r'''
            w_i
            ''')
          
            st.write(""" 
            s√£o os pesos individuais do modelo (Al‚ÄêRimy et al., 2023).
            """)
          
            st.write("""
            Este termo adicional penaliza pesos grandes, for√ßando o modelo a priorizar solu√ß√µes que utilizam pesos menores, o que √© crucial para evitar 
            que o modelo memorize os dados de treinamento, promovendo maior capacidade de generaliza√ß√£o (Sakizadeh et al., 2015).
            """)
          
            st.write("""
            ### Fundamenta√ß√£o Te√≥rica
            A regulariza√ß√£o L2 tem uma base te√≥rica s√≥lida, sendo amplamente aplicada para controlar a complexidade do modelo. Ao adicionar o termo de penaliza√ß√£o, 
            a regulariza√ß√£o L2 ajuda a evitar o overfitting e melhora a estabilidade num√©rica do modelo (Friedrich et al., 2022). Isso √© particularmente importante 
            em redes neurais profundas, onde o n√∫mero de par√¢metros pode ser grande e a complexidade do modelo alta.
            """)
          
            st.write("""
            ### Efeitos da Regulariza√ß√£o L2
            A regulariza√ß√£o L2 controla a complexidade do modelo ao penalizar pesos grandes, o que √© particularmente √∫til em cen√°rios com muitos par√¢metros 
            ou dados ruidosos (Piotrowski & Napiorkowski, 2013). Al√©m de reduzir o overfitting, a L2 promove a estabilidade no treinamento, melhorando a consist√™ncia do desempenho 
            em dados de teste (Friedrich et al., 2022).
            """)
    
            st.write("""
            ### Interpreta√ß√£o e Efeitos Pr√°ticos de Œª
            """)
          
            st.write("""        
            A escolha do valor de Œª
            """)
      
            st.write("""
            influencia diretamente o comportamento do modelo:
            """)
    
            st.write("""
            #### Œª = 0
            """)
            st.write("""
            Quando Œª = 0, a regulariza√ß√£o L2 est√° desativada. Isso permite que o modelo ajuste-se livremente aos dados de treinamento, 
            aumentando o risco de overfitting, especialmente em conjuntos de dados pequenos ou ruidosos (Friedrich et al., 2022).
            """)
    
            st.write("""
            #### Œª = 0,01
            """)
            st.write("""
            Este √© um valor moderado, que penaliza de forma equilibrada os pesos do modelo. Essa configura√ß√£o ajuda a evitar o overfitting sem comprometer a capacidade do modelo de 
            aprender padr√µes relevantes (Al‚ÄêRimy et al., 2023).
            """)
    
            st.write("""
            #### Œª = 0,02 ou Œª = 0,03
            Esses valores aumentam a intensidade da penaliza√ß√£o, sendo √∫teis em cen√°rios com dados ruidosos ou em que o n√∫mero de par√¢metros √© alto em rela√ß√£o √† quantidade de dados 
            dispon√≠veis (Piotrowski & Napiorkowski, 2013). Contudo, deve-se monitorar o desempenho do modelo, pois valores elevados de Œª podem resultar em **underfitting**, 
            comprometendo a capacidade do modelo de capturar padr√µes complexos (Friedrich et al., 2022).
            """)
    
            st.write("""
            ### Conclus√£o
            A regulariza√ß√£o L2 √© uma t√©cnica poderosa no treinamento de redes neurais profundas, ajudando a mitigar o overfitting e a melhorar a capacidade de generaliza√ß√£o do modelo. 
            Ao penalizar pesos grandes, a L2 incentiva solu√ß√µes mais simples e robustas. No entanto, a escolha do valor de Œª √© crucial para garantir que o modelo consiga capturar 
            padr√µes complexos sem se ajustar excessivamente aos dados de treinamento.
            """)
    
            st.write("""
            ### Refer√™ncias
            - AL‚ÄêRIMY, B.; SAEED, F.; AL-SAREM, M.; ALBARRAK, A.; QASEM, S. An adaptive early stopping technique for densenet169-based knee osteoarthritis detection model. *Diagnostics*, 13(11), 1903, 2023. https://doi.org/10.3390/diagnostics13111903
            - FRIEDRICH, S. et al. Regularization approaches in clinical biostatistics: a review of methods and their applications. *Statistical Methods in Medical Research*, 32(2), 425-440, 2022. https://doi.org/10.1177/09622802221133557
            - PIOTROWSKI, A.; NAPIORKOWSKI, J. A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling. *Journal of Hydrology*, 476, 97-111, 2013. https://doi.org/10.1016/j.jhydrol.2012.10.019
            - SAKIZADEH, M.; MALIAN, A.; AHMADPOUR, E. Groundwater quality modeling with a small data set. *Ground Water*, 54(1), 115-120, 2015. https://doi.org/10.1111/gwat.12317
            """)
    

  
    l2_lambda = st.sidebar.number_input("L2 Regularization (Weight Decay):", min_value=0.0, max_value=0.1, value=0.01, step=0.01)
    l1_lambda = st.sidebar.number_input("L1 Regularization:", min_value=0.0, max_value=0.01, value=0.0, step=0.001, 
                                        help="Adiciona regulariza√ß√£o L1 (Lasso) ao treinamento. Promove esparsidade nos pesos.")
    
    #________________________________________________________________________________________
    # Novos par√¢metros de treinamento
    st.sidebar.write("---")
    st.sidebar.subheader("‚öôÔ∏è Configura√ß√µes Avan√ßadas")
    
    # Tipo de Aumento de Dados
    augmentation_type = st.sidebar.selectbox(
        "T√©cnica de Aumento de Dados:",
        options=['none', 'standard', 'mixup', 'cutmix'],
        index=1,
        help="None: Sem aumento | Standard: Transforma√ß√µes b√°sicas | Mixup: Mistura imagens | Cutmix: Recorta e cola regi√µes"
    )
    
    # Otimizador
    optimizer_options = ['Adam', 'AdamW', 'SGD']
    if ADVANCED_OPTIMIZERS_AVAILABLE:
        optimizer_options.extend(['Ranger', 'Lion'])
    
    optimizer_name = st.sidebar.selectbox(
        "Otimizador:",
        options=optimizer_options,
        index=0,
        help="Adam: Adaptativo padr√£o | AdamW: Adam com weight decay melhorado | SGD: Gradiente descendente com momento | Ranger: Lookahead + RAdam | Lion: Otimizador eficiente recente"
    )
    
    # Learning Rate Scheduler
    scheduler_name = st.sidebar.selectbox(
        "Agendador de Learning Rate:",
        options=['None', 'CosineAnnealingLR', 'OneCycleLR'],
        index=0,
        help="None: LR constante | CosineAnnealingLR: Reduz LR com coseno | OneCycleLR: Aumenta e depois reduz LR"
    )
    
    # Tipo de Grad-CAM
    gradcam_type = st.sidebar.selectbox(
        "Tipo de Grad-CAM:",
        options=['GradCAM', 'GradCAMpp', 'SmoothGradCAMpp', 'LayerCAM'],
        index=2,
        help="GradCAM: B√°sico | GradCAMpp: Melhorado | SmoothGradCAMpp: Suavizado | LayerCAM: Por camada"
    )
    
    st.sidebar.write("---")
    
    #________________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmula LaTeX
    with st.sidebar:
        with st.expander("Implementa√ß√£o da T√©cnica de Parada Precoce - Early Stopping:"):
            st.write("""
            #### Introdu√ß√£o
            A t√©cnica de **parada precoce** (ou *early stopping*) √© amplamente utilizada para mitigar o **overfitting** no treinamento de redes neurais profundas. 
            O overfitting ocorre quando o modelo se ajusta t√£o bem aos dados de treinamento que sua capacidade de generaliza√ß√£o para novos dados √© prejudicada. 
            O princ√≠pio da parada precoce √© interromper o treinamento quando o desempenho do modelo em um conjunto de valida√ß√£o n√£o apresenta melhorias significativas 
            ap√≥s um n√∫mero predefinido de √©pocas. Essa abordagem baseia-se na observa√ß√£o de que, ap√≥s certo ponto, melhorias no desempenho do modelo em dados de treinamento 
            n√£o resultam em melhorias em dados que o modelo ainda n√£o viu (Piotrowski & Napiorkowski, 2013; Al‚ÄêRimy et al., 2023).
            """)
      
            st.write("Matematicamente, a parada precoce pode ser descrita pela seguinte condi√ß√£o de interrup√ß√£o:")
            # F√≥rmulas matem√°ticas
            st.latex(r'''
            \text{Se } L_{\text{val}}(t) \geq L_{\text{val}}(t-1)
            ''')
            st.write("""
            por (p) √©pocas consecutivas, ent√£o interrompa o treinamento. Aqui,
            """)
            st.latex(r'''
            L_{\text{val}}(t)
            ''')
    
            st.write("""
            representa o valor da **fun√ß√£o de perda** no conjunto de valida√ß√£o na √©poca (t), e (p) √© o **par√¢metro de paci√™ncia**. 
            A paci√™ncia (p) define quanto tempo o treinamento deve continuar mesmo que n√£o haja melhorias imediatas. Se a perda n√£o melhorar por (p) √©pocas consecutivas, 
            o treinamento √© interrompido.
            """)
      
            st.write("""
            #### A Import√¢ncia da Paci√™ncia
            O par√¢metro de **paci√™ncia** define o n√∫mero de √©pocas consecutivas sem melhoria na m√©trica de valida√ß√£o que o modelo pode suportar antes de o treinamento ser interrompido. 
            A escolha do valor de paci√™ncia tem impacto direto no equil√≠brio entre **evitar o overfitting** e **permitir que o modelo continue aprendendo**. 
            """)
      
            st.write("##### Paci√™ncia = 0")
            st.write("""
            Um valor de paci√™ncia igual a zero implica que o treinamento ser√° interrompido imediatamente ap√≥s a primeira ocorr√™ncia de estagna√ß√£o na m√©trica de valida√ß√£o. 
            Isso pode ser √∫til em cen√°rios onde se deseja evitar qualquer risco de *overfitting*.
            """)
      
            st.write("##### Paci√™ncia ‚â• 1")
            st.write("""
            Uma paci√™ncia maior (como 1 ou 2) permite que o modelo continue sendo treinado mesmo ap√≥s pequenas flutua√ß√µes no desempenho, 
            o que pode ser ben√©fico em conjuntos de dados ruidosos (Sakizadeh et al., 2015).
            """)
      
            st.write("""
            #### Impacto do *Early Stopping* e da Paci√™ncia
            A configura√ß√£o do par√¢metro de paci√™ncia influencia diretamente a efici√™ncia do treinamento. Com uma paci√™ncia muito baixa, o treinamento pode ser interrompido de forma prematura, 
            mesmo que o modelo ainda tenha potencial de melhoria. Por outro lado, uma paci√™ncia muito alta pode permitir que o modelo se ajuste excessivamente aos dados de treinamento, 
            levando ao *overfitting* (Sakizadeh et al., 2015).
            """)
      
            st.write("""
            #### Exemplos de Aplica√ß√£o
            Um exemplo pr√°tico de uso da parada precoce √© em tarefas de **classifica√ß√£o de imagens**. Durante o treinamento de um modelo para detec√ß√£o de melanoma, se a acur√°cia no conjunto de valida√ß√£o 
            n√£o melhorar ap√≥s um determinado n√∫mero de √©pocas, o early stopping √© acionado.
            """)
      
            st.write("""
            #### Integra√ß√£o com Outras T√©cnicas de Regulariza√ß√£o
            A parada precoce pode ser usada em conjunto com outras t√©cnicas de regulariza√ß√£o, como a **inje√ß√£o de ru√≠do** e a regulariza√ß√£o **L1/L2**, 
            para melhorar a robustez do modelo e sua capacidade de generaliza√ß√£o (Friedrich et al., 2022). 
            A combina√ß√£o dessas t√©cnicas ajuda a evitar que o modelo se ajuste excessivamente aos dados de treinamento, principalmente em cen√°rios com volumes limitados de dados.
            """)
      
            st.write("""
            #### Conclus√£o
            A **parada precoce** √© uma t√©cnica eficaz para evitar o *overfitting* no treinamento de redes neurais profundas. O valor da paci√™ncia desempenha um papel cr√≠tico, 
            permitindo o equil√≠brio entre **efici√™ncia computacional** e **capacidade de aprendizado**. Al√©m disso, a combina√ß√£o da parada precoce com outras t√©cnicas de regulariza√ß√£o 
            pode melhorar ainda mais o desempenho do modelo.
            """)
      
            st.write("""
            #### Refer√™ncias
            - PIOTROWSKI, A.; NAPIORKOWSKI, J. A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling. *Journal of Hydrology*, v. 476, p. 97-111, 2013. https://doi.org/10.1016/j.jhydrol.2012.10.019.
            - AL‚ÄêRIMY, B. et al. An adaptive early stopping technique for densenet169-based knee osteoarthritis detection model. *Diagnostics*, v. 13, n. 11, p. 1903, 2023. https://doi.org/10.3390/diagnostics13111903.
            - SAKIZADEH, M.; MALIAN, A.; AHMADPOUR, E. Groundwater quality modeling with a small data set. *Ground Water*, v. 54, n. 1, p. 115-120, 2015. https://doi.org/10.1111/gwat.12317.
            - FRIEDRICH, S. et al. Regularization approaches in clinical biostatistics: a review of methods and their applications. *Statistical Methods in Medical Research*, v. 32, n. 2, p. 425-440, 2022. https://doi.org/10.1177/09622802221133557.
            """)


    #________________________________________________________________________________________
    patience = st.sidebar.number_input("Paci√™ncia para Early Stopping:", min_value=1, max_value=10, value=3, step=1)

    #____________________________________________________________________________________________
    with st.sidebar:
        with st.expander("Perda Ponderada para Classes Desbalanceadas:"):
            st.write("""
            ### Perda Ponderada para Classes Desbalanceadas
        
            A t√©cnica de **perda ponderada** para lidar com **classes desbalanceadas** √© amplamente utilizada em **aprendizado de m√°quina**, especialmente em redes neurais, para tratar problemas onde o n√∫mero de amostras entre as classes de um conjunto de dados n√£o √© equilibrado. O desbalanceamento ocorre em diversos dom√≠nios, como detec√ß√£o de fraudes, diagn√≥stico de doen√ßas e classifica√ß√£o de imagens. O principal objetivo da perda ponderada √© ajustar a fun√ß√£o de perda, atribuindo diferentes pesos √†s classes, de forma que o impacto das classes minorit√°rias (menos representadas) seja ampliado e o impacto das classes majorit√°rias seja reduzido. Isso ajuda o modelo a aprender de forma mais eficaz em cen√°rios onde o desequil√≠brio entre as classes pode levar ao **overfitting** nas classes majorit√°rias e √† **sub-representa√ß√£o** das classes minorit√°rias (Buda et al., 2018).
        
            ### Motiva√ß√£o e Justificativa Cient√≠fica
        
            Em um cen√°rio de classifica√ß√£o de imagens, se o modelo for treinado com uma quantidade muito maior de amostras de uma classe (classe majorit√°ria) em rela√ß√£o a outra (classe minorit√°ria), o modelo tende a ser enviesado para a classe majorit√°ria. Isso ocorre porque o objetivo padr√£o da maioria das fun√ß√µes de perda, como a **entropia cruzada**, √© minimizar a soma dos erros. Em um conjunto de dados desbalanceado, essa minimiza√ß√£o pode ser alcan√ßada simplesmente classificando todas as amostras como pertencentes √† classe majorit√°ria, resultando em alta acur√°cia geral, mas com desempenho ruim na classe minorit√°ria. Para resolver esse problema, atribui-se um peso maior √† classe minorit√°ria, for√ßando a fun√ß√£o de perda a penalizar mais fortemente os erros cometidos nessa classe (Buda et al., 2018).
        
            ### Implementa√ß√£o no C√≥digo
        
            No c√≥digo, a implementa√ß√£o da perda ponderada √© feita utilizando a fun√ß√£o de perda **CrossEntropyLoss** do PyTorch, que suporta a aplica√ß√£o de pesos √†s classes. Esses pesos s√£o calculados com base na **frequ√™ncia das classes** no conjunto de treinamento. Classes com menos amostras recebem pesos maiores, enquanto classes com mais amostras recebem pesos menores, balanceando o impacto de ambas durante o treinamento do modelo.
        
            """)
            
            st.write("**criterion = nn.CrossEntropyLoss(weight=class_weights)**")
            
            st.write("""
            No trecho de c√≥digo acima, o vetor `targets` coleta os r√≥tulos das amostras no conjunto de treino e a fun√ß√£o `np.bincount(targets)` conta quantas vezes cada classe aparece, resultando em um vetor `class_counts`, onde cada √≠ndice corresponde √† quantidade de amostras de uma classe espec√≠fica (Buda et al., 2018).
        
            ### Etapas do Processo
        
            1. **C√°lculo das Frequ√™ncias das Classes**: As frequ√™ncias de cada classe s√£o calculadas usando `np.bincount`. Classes menos representadas recebem pesos maiores.
            2. **Ajuste para Evitar Divis√£o por Zero**: Um pequeno valor (1e-6) √© adicionado para evitar divis√£o por zero quando uma classe n√£o tem nenhuma amostra.
            3. **C√°lculo dos Pesos Inversos**: A partir da frequ√™ncia, os pesos s√£o calculados tomando o inverso da frequ√™ncia de cada classe. Isso aumenta a penaliza√ß√£o dos erros nas classes minorit√°rias.
            4. **Fun√ß√£o de Perda Ponderada**: A fun√ß√£o de perda `nn.CrossEntropyLoss(weight=class_weights)` usa os pesos calculados, penalizando mais fortemente os erros das classes minorit√°rias.
        
            ### Impacto e Efic√°cia da Perda Ponderada
        
            A **perda ponderada** ajusta o aprendizado do modelo, incentivando a penaliza√ß√£o dos erros cometidos nas classes minorit√°rias. Estudos demonstram que essa t√©cnica √© eficaz em aumentar a **recall** das classes minorit√°rias, sem comprometer drasticamente a precis√£o das classes majorit√°rias (Buda et al., 2018). No entanto, a aplica√ß√£o da perda ponderada pode tornar o treinamento mais **sens√≠vel √† escolha dos hiperpar√¢metros**, como a **taxa de aprendizado**, pois o modelo passa a ser fortemente influenciado pelas amostras menos representativas.
        
            ### Conclus√£o
        
            A implementa√ß√£o da **perda ponderada** no c√≥digo √© uma abordagem robusta para lidar com **classes desbalanceadas**. Ao ajustar os pesos da fun√ß√£o de perda com base nas frequ√™ncias das classes, o modelo consegue equilibrar melhor o aprendizado entre as classes majorit√°rias e minorit√°rias, evitando vieses que favorecem a classe mais representada no conjunto de dados (Buda et al., 2018).
        
            ### Refer√™ncias
        
            - Buda, M., Maki, A., & Mazurowski, M. (2018). A systematic study of the class imbalance problem in convolutional neural networks. *Neural Networks*, 106, 249-259. https://doi.org/10.1016/j.neunet.2018.07.011
            """)

    use_weighted_loss = st.sidebar.checkbox("Usar Perda Ponderada para Classes Desbalanceadas", value=False)
    st.sidebar.image("eu.ico", width=80)
   
    st.sidebar.write("""
    Produzido pelo:
    
    Projeto Geomaker + IA 
    
    https://doi.org/10.5281/zenodo.13910277
    
    - Professor: Marcelo Claro.
    
    Contatos: marceloclaro@gmail.com
    
    Whatsapp: (88)981587145
    
    Instagram: [marceloclaro.geomaker](https://www.instagram.com/marceloclaro.geomaker/)
    
    """)
     # _____________________________________________
    # Controle de √Åudio
    st.sidebar.title("Controle de √Åudio")
    
    # Dicion√°rio de arquivos de √°udio, com nomes amig√°veis mapeando para o caminho do arquivo
    mp3_files = {
        "√Åudio explicativo para Leigos": "leigo.mp3",
        "√Åudio explicativo para treinamentos de poucos dados": "bucal.mp3",
    }
    
    # Lista de arquivos MP3 para sele√ß√£o
    selected_mp3 = st.sidebar.radio("Escolha um √°udio explicativo:", options=list(mp3_files.keys()))
    
    # Controle de op√ß√£o de repeti√ß√£o
    loop = st.sidebar.checkbox("Repetir √°udio")
    
    # Bot√£o de Play para iniciar o √°udio
    play_button = st.sidebar.button("Play")
    
    # Placeholder para o player de √°udio
    audio_placeholder = st.sidebar.empty()
    
    # Fun√ß√£o para verificar se o arquivo existe
    def check_file_exists(mp3_path):
        if not os.path.exists(mp3_path):
            st.sidebar.error(f"Arquivo {mp3_path} n√£o encontrado.")
            return False
        return True
    
    # Se o bot√£o Play for pressionado e um arquivo de √°udio estiver selecionado
    if play_button and selected_mp3:
        mp3_path = mp3_files[selected_mp3]
        
        # Verifica√ß√£o da exist√™ncia do arquivo
        if check_file_exists(mp3_path):
            try:
                # Abrindo o arquivo de √°udio no modo bin√°rio
                with open(mp3_path, "rb") as audio_file:
                    audio_bytes = audio_file.read()
                    
                    # Codificando o arquivo em base64 para embutir no HTML
                    audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                    
                    # Controle de loop (repeti√ß√£o)
                    loop_attr = "loop" if loop else ""
                    
                    # Gerando o player de √°udio em HTML
                    audio_html = f"""
                    <audio id="audio-player" controls autoplay {loop_attr}>
                      <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                      Seu navegador n√£o suporta o elemento de √°udio.
                    </audio>
                    """
                    
                    # Inserindo o player de √°udio na interface
                    audio_placeholder.markdown(audio_html, unsafe_allow_html=True)
            
            except FileNotFoundError:
                st.sidebar.error(f"Arquivo {mp3_path} n√£o encontrado.")
            except Exception as e:
                st.sidebar.error(f"Erro ao carregar o arquivo: {str(e)}")
    #______________________________________________________________________________________-


    # Verificar se a soma dos splits √© v√°lida
    if train_split + valid_split > 0.95:
        st.sidebar.error("A soma dos splits de treinamento e valida√ß√£o deve ser menor ou igual a 0.95.")

    # Upload do arquivo ZIP
    
    zip_file = st.file_uploader("Upload do arquivo ZIP com as imagens", type=["zip"])

    if zip_file is not None and train_split + valid_split <= 0.95:
        temp_dir = tempfile.mkdtemp()
        zip_path = os.path.join(temp_dir, "uploaded.zip")
        with open(zip_path, "wb") as f:
            f.write(zip_file.read())
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        data_dir = temp_dir

        # Detectar automaticamente o n√∫mero de classes do dataset
        try:
            temp_dataset = datasets.ImageFolder(root=data_dir)
            detected_num_classes = len(temp_dataset.classes)
            st.success(f"‚úÖ N√∫mero de classes detectado automaticamente: **{detected_num_classes}**")
            st.write(f"Classes encontradas: {', '.join(temp_dataset.classes)}")
            num_classes = detected_num_classes
        except Exception as e:
            st.error(f"Erro ao detectar classes: {e}")
            st.error("Certifique-se de que o ZIP cont√©m pastas com nomes de classes e imagens dentro delas.")
            shutil.rmtree(temp_dir)
            return

        st.write("Iniciando o treinamento supervisionado...")
        model_data = train_model(data_dir, num_classes, model_name, fine_tune, epochs, learning_rate, 
                                batch_size, train_split, valid_split, use_weighted_loss, l2_lambda, l1_lambda, 
                                patience, optimizer_name, scheduler_name, augmentation_type)

        if model_data is None:
            st.error("Erro no treinamento do modelo.")
            shutil.rmtree(temp_dir)
            return

        model, classes = model_data
        st.success("Treinamento conclu√≠do!")

        # Extrair caracter√≠sticas usando o modelo pr√©-treinado (sem a camada final)
        st.write("Extraindo caracter√≠sticas para clustering...")
        # Remover a √∫ltima camada do modelo para obter embeddings
        if model_name.startswith('ResNet'):
            feature_extractor = nn.Sequential(*list(model.children())[:-1])
        elif model_name.startswith('DenseNet'):
            feature_extractor = nn.Sequential(*list(model.features))
            feature_extractor.add_module('global_pool', nn.AdaptiveAvgPool2d((1,1)))
        else:
            st.error("Modelo n√£o suportado para extra√ß√£o de caracter√≠sticas.")
            return

        feature_extractor = feature_extractor.to(device)
        feature_extractor.eval()

        # Carregar o dataset completo para extra√ß√£o de caracter√≠sticas
        full_dataset = datasets.ImageFolder(root=data_dir, transform=test_transforms)
        features, labels = extract_features(full_dataset, feature_extractor, batch_size)

        # Aplicar algoritmos de clustering
        st.write("Aplicando algoritmos de clustering...")
        features_reshaped = features.reshape(len(features), -1)
        hierarchical_labels, kmeans_labels = perform_clustering(features_reshaped, num_classes)

        # Avaliar e exibir os resultados
        st.write("Avaliando os resultados do clustering...")
        evaluate_clustering(labels, hierarchical_labels, "Clustering Hier√°rquico")
        evaluate_clustering(labels, kmeans_labels, "K-Means Clustering")

        # Visualizar clusters
        visualize_clusters(features_reshaped, labels, hierarchical_labels, kmeans_labels, classes)
        
        # ========== OP√á√ÉO DE VISUALIZA√á√ÉO PCA ==========
        st.write("---")
        st.write("## üî¨ An√°lise PCA das Features")
        
        show_pca = st.checkbox("üìä Mostrar An√°lise PCA das Features Extra√≠das", value=True)
        
        if show_pca:
            # Op√ß√£o de escolher n√∫mero de componentes
            n_components = st.selectbox(
                "Escolha o n√∫mero de componentes principais para visualiza√ß√£o:",
                options=[2, 3],
                index=0,
                help="2 componentes: Visualiza√ß√£o 2D | 3 componentes: Visualiza√ß√£o 3D interativa com Plotly"
            )
            
            if n_components == 2:
                visualize_pca_features(features_reshaped, labels, classes, n_components=2)
            else:
                # 3D Visualization with Plotly
                st.write("### üìä Visualiza√ß√£o PCA 3D Interativa")
                try:
                    fig_3d = visualize_pca_3d(features_reshaped, labels, classes)
                    st.plotly_chart(fig_3d, use_container_width=True)
                except Exception as e:
                    st.error(f"Erro ao gerar visualiza√ß√£o 3D: {str(e)}")
                    st.info("Mostrando visualiza√ß√£o 2D como alternativa")
                    visualize_pca_features(features_reshaped, labels, classes, n_components=2)

        # Avalia√ß√£o de uma imagem individual
        evaluate = st.radio("Deseja avaliar uma imagem?", ("Sim", "N√£o"))
        if evaluate == "Sim":
            eval_image_file = st.file_uploader("Fa√ßa upload da imagem para avalia√ß√£o", type=["png", "jpg", "jpeg", "bmp", "gif"])
            if eval_image_file is not None:
                eval_image_file.seek(0)
                try:
                    eval_image = Image.open(eval_image_file).convert("RGB")
                except Exception as e:
                    st.error(f"Erro ao abrir a imagem: {e}")
                    return

                st.image(eval_image, caption='Imagem para avalia√ß√£o', width='stretch')
                class_name, confidence = evaluate_image(model, eval_image, classes)
                st.write(f"**Classe Predita:** {class_name}")
                st.write(f"**Confian√ßa:** {confidence:.4f}")

                # Visualizar ativa√ß√µes com o tipo de Grad-CAM selecionado
                activation_map = visualize_activations(model, eval_image, classes, gradcam_type)
                
                # ========== 3D GRAD-CAM VISUALIZATION ==========
                if activation_map is not None:
                    st.write("---")
                    st.write("### üåê Visualiza√ß√£o 3D do Grad-CAM")
                    show_3d_gradcam = st.checkbox("Mostrar Grad-CAM em 3D", value=False)
                    if show_3d_gradcam:
                        try:
                            fig_gradcam_3d = visualize_activation_heatmap_3d(activation_map)
                            st.plotly_chart(fig_gradcam_3d, use_container_width=True)
                        except Exception as e:
                            st.error(f"Erro ao gerar visualiza√ß√£o 3D do Grad-CAM: {str(e)}")
                
                # ========== AI CHAT DIAGNOSTIC ANALYSIS ==========
                st.write("---")
                st.write("## ü§ñ An√°lise Diagn√≥stica com IA")
                
                enable_ai_analysis = st.checkbox("Ativar An√°lise Diagn√≥stica Avan√ßada com IA", value=False)
                
                if enable_ai_analysis:
                    st.write("### Configura√ß√£o da API")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        api_provider = st.selectbox(
                            "Provedor de API:",
                            options=['gemini', 'groq'],
                            help="Escolha entre Google Gemini ou Groq"
                        )
                    
                    with col2:
                        if api_provider == 'gemini':
                            model_options = ['gemini-1.0-pro', 'gemini-1.5-pro', 'gemini-1.5-flash']
                        else:
                            model_options = ['mixtral-8x7b-32768', 'llama-3.1-70b-versatile', 'llama-3.1-8b-instant']
                        
                        ai_model = st.selectbox(
                            "Modelo:",
                            options=model_options
                        )
                    
                    api_key = st.text_input(
                        "API Key:",
                        type="password",
                        help="Insira sua chave API"
                    )
                    
                    if api_key:
                        if st.button("üî¨ Gerar An√°lise Diagn√≥stica Completa"):
                            with st.spinner("Gerando an√°lise diagn√≥stica aprofundada..."):
                                try:
                                    # Fetch academic references
                                    st.write("üìö Buscando refer√™ncias acad√™micas...")
                                    ref_fetcher = AcademicReferenceFetcher()
                                    references = ref_fetcher.get_references_for_classification(
                                        class_name=class_name,
                                        domain="image classification",
                                        max_per_source=3
                                    )
                                    
                                    if references:
                                        with st.expander("üìö Refer√™ncias Acad√™micas Encontradas"):
                                            st.markdown(format_references_for_display(references))
                                    
                                    # Generate Grad-CAM description
                                    gradcam_desc = ""
                                    if activation_map is not None:
                                        gradcam_desc = describe_gradcam_regions(activation_map)
                                    
                                    # Collect training statistics
                                    training_stats = {
                                        "√âpocas Treinadas": epochs,
                                        "Taxa de Aprendizagem": learning_rate,
                                        "Batch Size": batch_size,
                                        "Modelo": model_name,
                                        "Tipo de Augmenta√ß√£o": augmentation_type,
                                        "Otimizador": optimizer_name
                                    }
                                    
                                    # Collect statistical results
                                    # Note: These are placeholder values. In a production system,
                                    # these should be computed from actual test set evaluation.
                                    # TODO: Integrate with actual test metrics from trained model
                                    statistical_results = {
                                        "Informa√ß√£o": "M√©tricas baseadas no treinamento realizado",
                                        "Nota": "Para an√°lise completa, avalie em conjunto de teste separado"
                                    }
                                    
                                    # Initialize AI analyzer
                                    ai_analyzer = AIAnalyzer(
                                        api_provider=api_provider,
                                        api_key=api_key,
                                        model_name=ai_model
                                    )
                                    
                                    # Generate comprehensive analysis
                                    st.write("üß† Gerando interpreta√ß√£o diagn√≥stica...")
                                    analysis = ai_analyzer.generate_comprehensive_analysis(
                                        predicted_class=class_name,
                                        confidence=confidence,
                                        training_stats=training_stats,
                                        statistical_results=statistical_results,
                                        gradcam_description=gradcam_desc,
                                        academic_references=references
                                    )
                                    
                                    # Display analysis
                                    st.write("---")
                                    st.write("## üìã An√°lise Diagn√≥stica Completa")
                                    st.markdown(analysis)
                                    
                                    # ========== MULTI-AGENT SYSTEM ANALYSIS (15 AGENTS + MANAGER) ==========
                                    st.write("---")
                                    st.write("## ü§ñ Sistema Multi-Agente (15 Agentes + 1 Gerente)")
                                    
                                    use_multiagent = st.checkbox("Ativar An√°lise com Sistema Multi-Agente", value=True)
                                    
                                    if use_multiagent:
                                        with st.spinner("Coordenando an√°lise de 15 agentes especializados..."):
                                            try:
                                                manager = ManagerAgent()
                                                
                                                # Preparar contexto com informa√ß√µes dispon√≠veis
                                                agent_context = {
                                                    'training_stats': training_stats,
                                                    'statistical_results': statistical_results,
                                                    'gradcam_description': gradcam_desc,
                                                    'references': references
                                                }
                                                
                                                multi_agent_report = manager.coordinate_analysis(
                                                    predicted_class=class_name,
                                                    confidence=confidence,
                                                    context=agent_context
                                                )
                                                
                                                st.markdown(multi_agent_report)
                                                st.success("‚úÖ An√°lise Multi-Agente Conclu√≠da! 15 especialistas + 1 gerente coordenador")
                                                
                                            except Exception as e:
                                                st.error(f"Erro ao gerar an√°lise multi-agente: {str(e)}")
                                                import traceback
                                                st.code(traceback.format_exc())
                                    
                                    # ========== GENETIC ALGORITHM MULTI-ANGLE INTERPRETATION ==========
                                    st.write("---")
                                    st.write("## üß¨ Interpreta√ß√£o Multi-Angular com Algoritmos Gen√©ticos")
                                    
                                    use_genetic = st.checkbox("Gerar An√°lise Multi-Perspectiva", value=True)
                                    
                                    if use_genetic:
                                        with st.spinner("Aplicando algoritmos gen√©ticos para interpreta√ß√£o multi-angular..."):
                                            try:
                                                genetic_interpreter = GeneticDiagnosticInterpreter(
                                                    population_size=20,
                                                    generations=10
                                                )
                                                
                                                multi_angle_report = genetic_interpreter.generate_multi_angle_report(
                                                    predicted_class=class_name,
                                                    confidence=confidence
                                                )
                                                
                                                st.markdown(multi_angle_report)
                                                
                                            except Exception as e:
                                                st.error(f"Erro ao gerar an√°lise multi-angular: {str(e)}")
                                    
                                except Exception as e:
                                    st.error(f"Erro ao gerar an√°lise: {str(e)}")
                                    st.info("Verifique se a API key est√° correta e se voc√™ tem cr√©ditos dispon√≠veis.")
                    else:
                        st.warning("‚ö†Ô∏è Por favor, insira sua API key para gerar a an√°lise.")

        # Limpar o diret√≥rio tempor√°rio
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()
