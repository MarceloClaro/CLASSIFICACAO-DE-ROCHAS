import os
import zipfile
import shutil
import tempfile
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image, ImageEnhance
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms, models, datasets
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.metrics import (adjusted_rand_score, normalized_mutual_info_score,
                             confusion_matrix, classification_report,
                             roc_auc_score, roc_curve)
from sklearn.preprocessing import label_binarize
from sklearn.decomposition import PCA
import streamlit as st
import gc
import logging
import base64
import io
# Importa√ß√µes adicionais para Grad-CAM
from torchcam.methods import SmoothGradCAMpp, GradCAM, GradCAMpp, LayerCAM
from torchvision.transforms.functional import normalize, resize, to_pil_image
import cv2
# Importar otimizadores avan√ßados
try:
    import torch_optimizer as optim_advanced
    ADVANCED_OPTIMIZERS_AVAILABLE = True
except ImportError:
    ADVANCED_OPTIMIZERS_AVAILABLE = False

# Importar APIs com suporte de vis√£o
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

# Import multi-agent system
try:
    from multi_agent_system import ManagerAgent
    MULTI_AGENT_AVAILABLE = True
except ImportError:
    MULTI_AGENT_AVAILABLE = False

# Definir o dispositivo (CPU ou GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Configura√ß√µes para tornar os gr√°ficos mais bonitos
sns.set_style('whitegrid')

def set_seed(seed):
    """
    Define a seed para garantir a reprodutibilidade.
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)  # Definir a seed para reprodutibilidade

# ImageNet normalization values
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

def denormalize_image(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):
    """
    Denormaliza um tensor de imagem normalizado com valores ImageNet.
    
    Args:
        tensor: Tensor de imagem (C, H, W) ou array numpy (H, W, C)
        mean: M√©dia usada na normaliza√ß√£o
        std: Desvio padr√£o usado na normaliza√ß√£o
    
    Returns:
        Array numpy (H, W, C) com valores no intervalo [0, 1]
    """
    if isinstance(tensor, torch.Tensor):
        # Convert tensor to numpy
        image = tensor.permute(1, 2, 0).cpu().numpy()
    else:
        image = tensor
    
    # Denormalize
    mean = np.array(mean)
    std = np.array(std)
    image = std * image + mean
    
    # Clip to valid range
    image = np.clip(image, 0, 1)
    
    return image

# ==================== STATISTICAL ANALYSIS MODULE ====================

class StatisticalAnalyzer:
    """
    Classe para an√°lise estat√≠stica avan√ßada de predi√ß√µes do modelo.
    Inclui intervalos de confian√ßa, testes de signific√¢ncia, bootstrap, etc.
    """
    
    @staticmethod
    def calculate_confidence_interval(probabilities, confidence_level=0.95):
        """
        Calcula intervalos de confian√ßa para as probabilidades usando m√©todo normal.
        
        Args:
            probabilities: Array de probabilidades
            confidence_level: N√≠vel de confian√ßa (padr√£o 0.95 para 95%)
        
        Returns:
            Dict com intervalo inferior e superior
        """
        from scipy import stats
        
        mean = np.mean(probabilities)
        std_error = stats.sem(probabilities)
        margin_error = std_error * stats.t.ppf((1 + confidence_level) / 2, len(probabilities) - 1)
        
        return {
            'mean': mean,
            'lower': max(0, mean - margin_error),
            'upper': min(1, mean + margin_error),
            'margin_error': margin_error
        }
    
    @staticmethod
    def bootstrap_validation(model, image_tensor, n_iterations=100, dropout_rate=0.1):
        """
        Realiza valida√ß√£o bootstrap atrav√©s de m√∫ltiplas predi√ß√µes com dropout.
        
        Args:
            model: Modelo treinado
            image_tensor: Tensor da imagem
            n_iterations: N√∫mero de itera√ß√µes bootstrap
            dropout_rate: Taxa de dropout para varia√ß√£o
        
        Returns:
            Dict com estat√≠sticas bootstrap
        """
        model.train()  # Ativa dropout
        predictions = []
        probabilities_per_class = []
        
        with torch.no_grad():
            for _ in range(n_iterations):
                output = model(image_tensor)
                probs = torch.nn.functional.softmax(output, dim=1)
                predictions.append(probs.cpu().numpy()[0])
        
        predictions = np.array(predictions)
        mean_probs = np.mean(predictions, axis=0)
        std_probs = np.std(predictions, axis=0)
        
        model.eval()  # Volta para modo de avalia√ß√£o
        
        return {
            'mean_probabilities': mean_probs,
            'std_probabilities': std_probs,
            'predictions_distribution': predictions,
            'confidence_bootstrap': np.max(mean_probs),
            'uncertainty': np.max(std_probs)
        }
    
    @staticmethod
    def significance_test(prob1, prob2, predictions_dist):
        """
        Testa se h√° diferen√ßa significativa entre duas probabilidades.
        
        Args:
            prob1: Probabilidade da classe 1
            prob2: Probabilidade da classe 2
            predictions_dist: Distribui√ß√£o de predi√ß√µes do bootstrap
        
        Returns:
            Dict com resultado do teste
        """
        from scipy import stats
        
        # Teste t pareado
        diff = predictions_dist[:, 0] - predictions_dist[:, 1] if predictions_dist.shape[1] > 1 else None
        
        if diff is not None:
            t_stat, p_value = stats.ttest_1samp(diff, 0)
            significant = p_value < 0.05
        else:
            t_stat, p_value, significant = None, None, None
        
        return {
            'probability_diff': abs(prob1 - prob2),
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': significant,
            'interpretation': 'Diferen√ßa significativa' if significant else 'Diferen√ßa n√£o significativa'
        }

class DiagnosticAnalyzer:
    """
    Classe para an√°lise diagn√≥stica diferencial e crit√©rios de exclus√£o.
    """
    
    @staticmethod
    def differential_diagnosis(probabilities, classes, top_k=3, threshold=0.1):
        """
        Lista diagn√≥sticos diferenciais principais baseado nas probabilidades.
        
        Args:
            probabilities: Array de probabilidades para cada classe
            classes: Lista de nomes das classes
            top_k: N√∫mero de diagn√≥sticos principais a retornar
            threshold: Limiar m√≠nimo de probabilidade para considerar
        
        Returns:
            Lista de diagn√≥sticos diferenciais
        """
        # Ordenar por probabilidade
        sorted_indices = np.argsort(probabilities)[::-1]
        
        differentials = []
        for i in range(min(top_k, len(sorted_indices))):
            idx = sorted_indices[i]
            prob = probabilities[idx]
            
            if prob >= threshold:
                differentials.append({
                    'rank': i + 1,
                    'class': classes[idx],
                    'probability': prob,
                    'confidence_level': DiagnosticAnalyzer._interpret_confidence(prob)
                })
        
        return differentials
    
    @staticmethod
    def _interpret_confidence(prob):
        """Interpreta o n√≠vel de confian√ßa"""
        if prob >= 0.9:
            return 'Muito Alto'
        elif prob >= 0.75:
            return 'Alto'
        elif prob >= 0.5:
            return 'Moderado'
        elif prob >= 0.3:
            return 'Baixo'
        else:
            return 'Muito Baixo'
    
    @staticmethod
    def exclusion_criteria(probabilities, classes, exclusion_threshold=0.05):
        """
        Aplica crit√©rios de exclus√£o baseados em probabilidades muito baixas.
        
        Args:
            probabilities: Array de probabilidades
            classes: Lista de classes
            exclusion_threshold: Limiar abaixo do qual classes s√£o exclu√≠das
        
        Returns:
            Dict com classes exclu√≠das e raz√µes
        """
        excluded = []
        
        for i, (prob, class_name) in enumerate(zip(probabilities, classes)):
            if prob < exclusion_threshold:
                excluded.append({
                    'class': class_name,
                    'probability': prob,
                    'reason': f'Probabilidade muito baixa (< {exclusion_threshold:.1%})'
                })
        
        return {
            'excluded_count': len(excluded),
            'excluded_classes': excluded,
            'remaining_count': len(classes) - len(excluded)
        }
    
    @staticmethod
    def distinctive_features(activation_map, threshold_percentile=75):
        """
        Identifica caracter√≠sticas distintivas baseadas no mapa de ativa√ß√£o.
        
        Args:
            activation_map: Mapa de ativa√ß√£o do Grad-CAM
            threshold_percentile: Percentil para identificar regi√µes importantes
        
        Returns:
            Dict com informa√ß√µes sobre caracter√≠sticas distintivas
        """
        if activation_map is None:
            return None
        
        threshold = np.percentile(activation_map, threshold_percentile)
        important_regions = activation_map > threshold
        
        return {
            'high_activation_percentage': (np.sum(important_regions) / important_regions.size) * 100,
            'max_activation': np.max(activation_map),
            'mean_activation': np.mean(activation_map),
            'activation_concentration': np.std(activation_map),
            'interpretation': DiagnosticAnalyzer._interpret_activation_pattern(
                (np.sum(important_regions) / important_regions.size) * 100
            )
        }
    
    @staticmethod
    def _interpret_activation_pattern(percentage):
        """Interpreta o padr√£o de ativa√ß√£o"""
        if percentage > 30:
            return 'Caracter√≠sticas dispersas - m√∫ltiplas regi√µes importantes'
        elif percentage > 15:
            return 'Caracter√≠sticas moderadamente focadas'
        elif percentage > 5:
            return 'Caracter√≠sticas altamente focadas - regi√£o espec√≠fica'
        else:
            return 'Caracter√≠sticas muito concentradas - aten√ß√£o localizada'

class UncertaintyAnalyzer:
    """
    Classe para quantifica√ß√£o de incerteza e an√°lise de risco.
    """
    
    @staticmethod
    def quantify_uncertainty(bootstrap_results, entropy_weight=0.5):
        """
        Quantifica fontes de incerteza na predi√ß√£o.
        
        Args:
            bootstrap_results: Resultados do bootstrap
            entropy_weight: Peso para a entropia na incerteza total
        
        Returns:
            Dict com an√°lise de incerteza
        """
        mean_probs = bootstrap_results['mean_probabilities']
        std_probs = bootstrap_results['std_probabilities']
        
        # Incerteza aleat√≥ria (epist√™mica) - varia√ß√£o das predi√ß√µes
        aleatoric_uncertainty = np.mean(std_probs)
        
        # Incerteza do modelo (entropia)
        entropy = -np.sum(mean_probs * np.log(mean_probs + 1e-10))
        max_entropy = np.log(len(mean_probs))
        normalized_entropy = entropy / max_entropy
        
        # Incerteza total
        total_uncertainty = (1 - entropy_weight) * aleatoric_uncertainty + entropy_weight * normalized_entropy
        
        return {
            'aleatoric_uncertainty': aleatoric_uncertainty,
            'model_entropy': entropy,
            'normalized_entropy': normalized_entropy,
            'total_uncertainty': total_uncertainty,
            'uncertainty_level': UncertaintyAnalyzer._classify_uncertainty(total_uncertainty),
            'sources': {
                'model_variation': aleatoric_uncertainty,
                'prediction_ambiguity': normalized_entropy
            }
        }
    
    @staticmethod
    def _classify_uncertainty(uncertainty):
        """Classifica o n√≠vel de incerteza"""
        if uncertainty < 0.1:
            return 'Muito Baixa'
        elif uncertainty < 0.2:
            return 'Baixa'
        elif uncertainty < 0.4:
            return 'Moderada'
        elif uncertainty < 0.6:
            return 'Alta'
        else:
            return 'Muito Alta'
    
    @staticmethod
    def assess_error_impact(top_probabilities, classes, risk_categories=None):
        """
        Avalia o impacto de poss√≠veis erros de classifica√ß√£o.
        
        Args:
            top_probabilities: Probabilidades das top classes
            classes: Nomes das classes
            risk_categories: Dict mapeando classes para n√≠veis de risco (opcional)
        
        Returns:
            Dict com avalia√ß√£o de impacto
        """
        if risk_categories is None:
            # Risco padr√£o baseado apenas na confian√ßa
            risk_categories = {cls: 'medium' for cls in classes}
        
        # Calcular probabilidade de erro
        error_probability = 1 - np.max(top_probabilities)
        
        # Avaliar impacto
        predicted_class = classes[np.argmax(top_probabilities)]
        risk_level = risk_categories.get(predicted_class, 'medium')
        
        return {
            'error_probability': error_probability,
            'predicted_class_risk': risk_level,
            'impact_score': error_probability * UncertaintyAnalyzer._risk_weight(risk_level),
            'recommendation': UncertaintyAnalyzer._generate_recommendation(
                error_probability, risk_level
            )
        }
    
    @staticmethod
    def _risk_weight(risk_level):
        """Retorna peso do n√≠vel de risco"""
        weights = {'low': 1, 'medium': 2, 'high': 3, 'critical': 5}
        return weights.get(risk_level.lower(), 2)
    
    @staticmethod
    def _generate_recommendation(error_prob, risk_level):
        """Gera recomenda√ß√£o baseada em erro e risco"""
        if error_prob > 0.3 and risk_level in ['high', 'critical']:
            return '‚ö†Ô∏è ATEN√á√ÉO: Alta probabilidade de erro em categoria de alto risco. Recomenda-se valida√ß√£o adicional.'
        elif error_prob > 0.5:
            return '‚ö†Ô∏è Confian√ßa baixa. Considere an√°lise complementar ou consulta especializada.'
        elif error_prob > 0.3:
            return '‚ÑπÔ∏è Confian√ßa moderada. Monitoramento recomendado.'
        else:
            return '‚úÖ Confian√ßa adequada. Resultado confi√°vel.'
    
    @staticmethod
    def safety_margin(confidence, min_acceptable=0.7, target=0.9):
        """
        Estabelece margem de seguran√ßa para a predi√ß√£o.
        
        Args:
            confidence: Confian√ßa da predi√ß√£o
            min_acceptable: Confian√ßa m√≠nima aceit√°vel
            target: Confian√ßa alvo desejada
        
        Returns:
            Dict com an√°lise de margem de seguran√ßa
        """
        margin_to_minimum = confidence - min_acceptable
        margin_to_target = target - confidence
        
        status = 'safe' if confidence >= min_acceptable else 'unsafe'
        meets_target = confidence >= target
        
        return {
            'confidence': confidence,
            'min_acceptable': min_acceptable,
            'target': target,
            'margin_to_minimum': margin_to_minimum,
            'margin_to_target': margin_to_target,
            'status': status,
            'meets_target': meets_target,
            'safety_score': min(1.0, confidence / target),
            'interpretation': UncertaintyAnalyzer._interpret_safety(
                margin_to_minimum, meets_target
            )
        }
    
    @staticmethod
    def _interpret_safety(margin, meets_target):
        """Interpreta a margem de seguran√ßa"""
        if margin < 0:
            return 'üî¥ ABAIXO DO M√çNIMO ACEIT√ÅVEL - N√£o recomendado para uso'
        elif margin < 0.1:
            return 'üü° MARGEM CR√çTICA - Usar com extrema cautela'
        elif meets_target:
            return 'üü¢ MARGEM ADEQUADA - Confian√ßa alvo atingida'
        else:
            return 'üü¢ MARGEM ACEIT√ÅVEL - Dentro dos par√¢metros seguros'
    
    @staticmethod
    def clinical_impact_assessment(confidence, class_name, differential_diagnoses):
        """
        Avalia o impacto cl√≠nico/pr√°tico da predi√ß√£o.
        
        Args:
            confidence: Confian√ßa da predi√ß√£o principal
            class_name: Nome da classe predita
            differential_diagnoses: Lista de diagn√≥sticos diferenciais
        
        Returns:
            Dict com avalia√ß√£o de impacto cl√≠nico
        """
        # Calcular ambiguidade diagn√≥stica
        if len(differential_diagnoses) > 1:
            top2_diff = differential_diagnoses[0]['probability'] - differential_diagnoses[1]['probability']
        else:
            top2_diff = 1.0
        
        # Determinar a√ß√£o recomendada
        if confidence >= 0.9 and top2_diff > 0.3:
            action = 'Proceder com diagn√≥stico prim√°rio'
            priority = 'Normal'
        elif confidence >= 0.75 and top2_diff > 0.2:
            action = 'Considerar diagn√≥stico prim√°rio com monitoramento'
            priority = 'M√©dia'
        elif len(differential_diagnoses) > 1 and differential_diagnoses[1]['probability'] > 0.3:
            action = 'Investigar diagn√≥sticos diferenciais - m√∫ltiplas possibilidades'
            priority = 'Alta'
        else:
            action = 'An√°lise complementar necess√°ria'
            priority = 'Alta'
        
        return {
            'primary_diagnosis': class_name,
            'diagnostic_confidence': confidence,
            'differential_count': len(differential_diagnoses),
            'diagnostic_ambiguity': 1 - top2_diff if len(differential_diagnoses) > 1 else 0,
            'recommended_action': action,
            'priority_level': priority,
            'requires_specialist': confidence < 0.75 or (
                len(differential_diagnoses) > 1 and differential_diagnoses[1]['probability'] > 0.3
            )
        }

# Enhanced image preprocessing class
class EnhancedImagePreprocessor:
    """Classe para melhorar o tratamento de imagens antes do treinamento"""
    
    @staticmethod
    def enhance_image_quality(image):
        """Aplica melhorias de qualidade na imagem"""
        # Ajustar contraste
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.2)
        
        # Ajustar nitidez
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.1)
        
        # Ajustar brilho levemente
        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(1.05)
        
        return image

def get_augmentation_transforms(augmentation_type='standard'):
    """
    Retorna transforma√ß√µes de acordo com o tipo de aumento de dados
    
    Args:
        augmentation_type: 'none', 'standard', 'mixup', 'cutmix'
    """
    if augmentation_type == 'none':
        # Sem aumento de dados - apenas transforma√ß√µes b√°sicas
        train_transform = transforms.Compose([
            transforms.Lambda(EnhancedImagePreprocessor.enhance_image_quality),
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ])
    else:
        # Standard ou base para mixup/cutmix
        train_transform = transforms.Compose([
            transforms.Lambda(EnhancedImagePreprocessor.enhance_image_quality),
            transforms.RandomApply([
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(degrees=90),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
                transforms.RandomAffine(degrees=0, shear=10),
            ], p=0.5),
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ])
    
    return train_transform

# Transforma√ß√µes para valida√ß√£o e teste com normaliza√ß√£o ImageNet
test_transforms = transforms.Compose([
    transforms.Lambda(EnhancedImagePreprocessor.enhance_image_quality),
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])

# Implementa√ß√£o de Mixup
def mixup_data(x, y, alpha=1.0):
    """Aplica Mixup ao batch de dados"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Calcula a loss para Mixup"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# Implementa√ß√£o de CutMix
def cutmix_data(x, y, alpha=1.0):
    """Aplica CutMix ao batch de dados"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)

    # Gerar bbox
    W = x.size()[2]
    H = x.size()[3]
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)

    # Centro do box
    cx = np.random.randint(0, W)
    cy = np.random.randint(0, H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    x_cutmix = x.clone()
    x_cutmix[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]

    # Ajustar lambda com a √°rea real
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
    y_a, y_b = y, y[index]
    return x_cutmix, y_a, y_b, lam

# Definir as transforma√ß√µes padr√£o para compatibilidade com c√≥digo existente
train_transforms = get_augmentation_transforms('standard')

# Dataset personalizado
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

def seed_worker(worker_id):
    """
    Fun√ß√£o para definir a seed em cada worker do DataLoader.
    """
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

def visualize_data(dataset, classes):
    """
    Exibe algumas imagens do conjunto de dados com suas classes.
    """
    st.write("### üìä Visualiza√ß√£o de algumas imagens do conjunto de dados original:")
    fig, axes = plt.subplots(1, 5, figsize=(15, 3))
    for i in range(5):
        idx = np.random.randint(len(dataset))
        image, label = dataset[idx]
        image = np.array(image)  # Converter a imagem PIL em array NumPy
        axes[i].imshow(image)
        axes[i].set_title(classes[label])
        axes[i].axis('off')
    st.pyplot(fig)
    plt.close(fig)

def plot_class_distribution(dataset, classes, title="Distribui√ß√£o das Classes"):
    """
    Exibe a distribui√ß√£o das classes no conjunto de dados e mostra os valores quantitativos.
    """
    # Extrair os r√≥tulos das classes para todas as imagens no dataset
    labels = [label for _, label in dataset]
    
    # Contagem de cada classe
    class_counts = np.bincount(labels)
    
    # Plotar o gr√°fico com as contagens
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.countplot(x=labels, hue=labels, ax=ax, palette="Set2", legend=False)
    
    # Adicionar os nomes das classes no eixo X
    ax.set_xticks(range(len(classes)))
    ax.set_xticklabels(classes, rotation=45, ha='right')
    
    # Adicionar as contagens acima das barras
    for i, count in enumerate(class_counts):
        ax.text(i, count, str(count), ha='center', va='bottom', fontweight='bold')
    
    ax.set_title(title)
    ax.set_xlabel("Classes")
    ax.set_ylabel("N√∫mero de Imagens")
    
    st.pyplot(fig)
    plt.close(fig)
    
    return class_counts

def show_augmented_images(dataset, transform, classes, num_augmentations=5):
    """
    Mostra imagens originais e suas vers√µes aumentadas.
    """
    st.write("### üîÑ Exemplos de Imagens Aumentadas (Data Augmentation)")
    st.write("Cada linha mostra uma imagem original seguida de suas vers√µes aumentadas:")
    
    # Selecionar 3 imagens aleat√≥rias
    num_samples = 3
    for sample_idx in range(num_samples):
        idx = np.random.randint(len(dataset))
        original_image, label = dataset[idx]
        
        # Criar figura com 1 original + num_augmentations aumentadas
        fig, axes = plt.subplots(1, num_augmentations + 1, figsize=(15, 3))
        
        # Mostrar imagem original
        axes[0].imshow(np.array(original_image))
        axes[0].set_title(f'Original\n{classes[label]}')
        axes[0].axis('off')
        axes[0].set_facecolor('#e6f2ff')
        
        # Mostrar imagens aumentadas
        for i in range(1, num_augmentations + 1):
            augmented_image = transform(original_image)
            # Desnormalizar para visualiza√ß√£o usando a fun√ß√£o helper
            augmented_np = denormalize_image(augmented_image)
            
            axes[i].imshow(augmented_np)
            axes[i].set_title(f'Aumentada {i}')
            axes[i].axis('off')
        
        plt.tight_layout()
        st.pyplot(fig)
        plt.close(fig)

def calculate_dataset_statistics(dataset, classes):
    """
    Calcula estat√≠sticas do dataset incluindo m√©dia, desvio padr√£o, etc.
    """
    st.write("### üìà Estat√≠sticas do Dataset")
    
    # Contagem por classe
    labels = [label for _, label in dataset]
    class_counts = np.bincount(labels)
    
    # Criar dataframe com estat√≠sticas
    stats_data = {
        'Classe': classes,
        'Quantidade': class_counts,
        'Percentual (%)': [f"{(count/len(dataset)*100):.2f}" for count in class_counts]
    }
    
    df_stats = pd.DataFrame(stats_data)
    
    st.write("#### Distribui√ß√£o de Classes:")
    # Fixed: Removed width=None parameter as it's no longer supported in Streamlit
    st.dataframe(df_stats)
    
    # Estat√≠sticas gerais
    st.write("#### Estat√≠sticas Gerais:")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total de Imagens", len(dataset))
    
    with col2:
        st.metric("N√∫mero de Classes", len(classes))
    
    with col3:
        st.metric("Imagens por Classe (M√©dia)", f"{np.mean(class_counts):.1f}")
    
    with col4:
        st.metric("Desvio Padr√£o", f"{np.std(class_counts):.1f}")
    
    # Verificar balanceamento
    min_count = np.min(class_counts)
    max_count = np.max(class_counts)
    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
    
    if imbalance_ratio > 1.5:
        st.warning(f"‚ö†Ô∏è Dataset desbalanceado detectado! Raz√£o: {imbalance_ratio:.2f}x (Classe mais frequente / Classe menos frequente)")
        st.info("üí° Recomenda√ß√£o: Considere usar 'Perda Ponderada para Classes Desbalanceadas' nas configura√ß√µes.")
    else:
        st.success(f"‚úÖ Dataset relativamente balanceado. Raz√£o: {imbalance_ratio:.2f}x")
    
    return df_stats

def visualize_pca_features(features, labels, classes, n_components=2):
    """
    Visualiza features usando PCA.
    """
    st.write(f"### üî¨ An√°lise PCA ({n_components} Componentes)")
    
    # Aplicar PCA
    pca = PCA(n_components=n_components)
    features_pca = pca.fit_transform(features)
    
    # Mostrar vari√¢ncia explicada
    explained_var = pca.explained_variance_ratio_
    st.write(f"**Vari√¢ncia Explicada:** {explained_var[0]*100:.2f}% (PC1), {explained_var[1]*100:.2f}% (PC2)")
    st.write(f"**Vari√¢ncia Total Explicada:** {sum(explained_var)*100:.2f}%")
    
    # Criar visualiza√ß√£o
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Mapear labels para nomes de classes
    labels_named = [classes[label] for label in labels]
    
    # Criar scatter plot
    scatter = sns.scatterplot(
        x=features_pca[:, 0], 
        y=features_pca[:, 1], 
        hue=labels_named,
        palette="tab10",
        ax=ax,
        s=100,
        alpha=0.7,
        edgecolor='black',
        linewidth=0.5
    )
    
    ax.set_xlabel(f'Componente Principal 1 ({explained_var[0]*100:.1f}%)')
    ax.set_ylabel(f'Componente Principal 2 ({explained_var[1]*100:.1f}%)')
    ax.set_title('Visualiza√ß√£o PCA das Features Extra√≠das')
    ax.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close(fig)
    
    return features_pca, explained_var

def get_model(model_name, num_classes, dropout_p=0.5, fine_tune=False):
    """
    Retorna o modelo pr√©-treinado selecionado, incluindo CNNs e Vision Transformers.
    
    Args:
        model_name: Nome do modelo (ResNet18, ResNet50, DenseNet121, ViT-B/16, ViT-B/32, ViT-L/16)
        num_classes: N√∫mero de classes
        dropout_p: Taxa de dropout
        fine_tune: Se deve fazer fine-tuning completo
    
    Returns:
        model: Modelo PyTorch configurado
    """
    # CNNs Tradicionais
    if model_name == 'ResNet18':
        model = models.resnet18(weights='DEFAULT')
    elif model_name == 'ResNet50':
        model = models.resnet50(weights='DEFAULT')
    elif model_name == 'DenseNet121':
        model = models.densenet121(weights='DEFAULT')
    # Vision Transformers
    elif model_name == 'ViT-B/16':
        model = models.vit_b_16(weights='DEFAULT')
    elif model_name == 'ViT-B/32':
        model = models.vit_b_32(weights='DEFAULT')
    elif model_name == 'ViT-L/16':
        model = models.vit_l_16(weights='DEFAULT')
    else:
        st.error(f"Modelo '{model_name}' n√£o suportado.")
        return None

    if not fine_tune:
        for param in model.parameters():
            param.requires_grad = False

    # Configurar camada de sa√≠da baseada no tipo de modelo
    if model_name.startswith('ResNet'):
        num_ftrs = model.fc.in_features
        model.fc = nn.Sequential(
            nn.Dropout(p=dropout_p),
            nn.Linear(num_ftrs, num_classes)
        )
        # Ensure final layer requires grad
        for param in model.fc.parameters():
            param.requires_grad = True
    elif model_name.startswith('DenseNet'):
        num_ftrs = model.classifier.in_features
        model.classifier = nn.Sequential(
            nn.Dropout(p=dropout_p),
            nn.Linear(num_ftrs, num_classes)
        )
        # Ensure final layer requires grad
        for param in model.classifier.parameters():
            param.requires_grad = True
    elif model_name.startswith('ViT'):
        # Vision Transformers usam 'heads' ao inv√©s de 'fc' ou 'classifier'
        num_ftrs = model.heads.head.in_features
        model.heads.head = nn.Sequential(
            nn.Dropout(p=dropout_p),
            nn.Linear(num_ftrs, num_classes)
        )
        # Ensure final layer requires grad
        for param in model.heads.head.parameters():
            param.requires_grad = True
    else:
        st.error("Tipo de modelo n√£o suportado para configura√ß√£o.")
        return None

    model = model.to(device)
    return model

def train_model(data_dir, num_classes, model_name, fine_tune, epochs, learning_rate, batch_size, train_split, valid_split, use_weighted_loss, l2_lambda, l1_lambda, patience, optimizer_name='Adam', scheduler_name='None', augmentation_type='standard'):
    """
    Fun√ß√£o principal para treinamento do modelo.
    
    Args:
        data_dir: Diret√≥rio com os dados
        num_classes: N√∫mero de classes
        model_name: Nome do modelo
        fine_tune: Se deve fazer fine-tuning completo
        epochs: N√∫mero de √©pocas
        learning_rate: Taxa de aprendizagem
        batch_size: Tamanho do lote
        train_split: Propor√ß√£o de treino
        valid_split: Propor√ß√£o de valida√ß√£o
        use_weighted_loss: Se deve usar perda ponderada
        l2_lambda: Regulariza√ß√£o L2 (weight decay)
        l1_lambda: Regulariza√ß√£o L1
        patience: Paci√™ncia para early stopping
        optimizer_name: Nome do otimizador (Adam, AdamW, SGD, Ranger, Lion)
        scheduler_name: Nome do scheduler (None, CosineAnnealingLR, OneCycleLR)
        augmentation_type: Tipo de aumento de dados (none, standard, mixup, cutmix)
    
    Returns:
        tuple: (model, classes) ou None em caso de erro
    """
    set_seed(42)

    # Carregar o dataset original sem transforma√ß√µes
    full_dataset = datasets.ImageFolder(root=data_dir)
    
    # ========== CONTAGEM INICIAL DOS DADOS ==========
    st.write("## üìä AN√ÅLISE INICIAL DO DATASET")
    st.write(f"### üî¢ **Contagem Inicial: {len(full_dataset)} imagens**")
    
    # Exibir estat√≠sticas detalhadas
    stats_df = calculate_dataset_statistics(full_dataset, full_dataset.classes)
    
    # Exibir algumas imagens do dataset original
    visualize_data(full_dataset, full_dataset.classes)
    
    # Plotar distribui√ß√£o inicial
    st.write("### üìä Distribui√ß√£o Inicial das Classes")
    initial_class_counts = plot_class_distribution(full_dataset, full_dataset.classes, 
                                                    title="Distribui√ß√£o INICIAL das Classes (Sem Aumento de Dados)")

    # ========== T√âCNICA DE AUMENTO DE DADOS ==========
    st.write("---")
    st.write("## üîÑ APLICA√á√ÉO DA T√âCNICA DE AUMENTO DE DADOS")
    st.write(f"**T√©cnica Selecionada:** `{augmentation_type}`")
    
    if augmentation_type == 'none':
        st.info("‚ÑπÔ∏è Nenhuma t√©cnica de aumento de dados foi selecionada. As imagens ser√£o usadas como est√£o.")
    elif augmentation_type == 'standard':
        st.info("‚ÑπÔ∏è T√©cnica Standard: Aplica√ß√£o de transforma√ß√µes aleat√≥rias (rota√ß√£o, flip, crop, jitter, etc.)")
    elif augmentation_type == 'mixup':
        st.info("‚ÑπÔ∏è T√©cnica Mixup: Mistura linear de pares de imagens e seus r√≥tulos")
    elif augmentation_type == 'cutmix':
        st.info("‚ÑπÔ∏è T√©cnica CutMix: Recorte e colagem de regi√µes entre imagens diferentes")
    
    # Obter transforma√ß√µes baseadas no tipo de augmenta√ß√£o
    train_transform = get_augmentation_transforms(augmentation_type)
    
    # Mostrar exemplos de imagens aumentadas
    if augmentation_type != 'none':
        show_augmented_images(full_dataset, train_transform, full_dataset.classes, num_augmentations=4)
    
    # ========== ESTIMATIVA AP√ìS AUMENTO ==========
    st.write("---")
    st.write("## üìà ESTIMATIVA AP√ìS AUMENTO DE DADOS")
    
    # Calcular estimativa de imagens ap√≥s aumento
    # Durante o treinamento, cada √©poca gera vers√µes aumentadas
    if augmentation_type == 'none':
        augmentation_multiplier = 1
        st.write(f"### üî¢ **Total Estimado: {len(full_dataset)} imagens** (sem aumento)")
    else:
        # Com augmentation, cada √©poca gera vers√µes diferentes
        # Estimativa conservadora: cada imagem pode gerar de 3-5 varia√ß√µes por √©poca
        augmentation_multiplier = 4  # M√©dia estimada
        total_estimated = len(full_dataset) * augmentation_multiplier * epochs
        st.write(f"### üî¢ **Total de Imagens Original: {len(full_dataset)}**")
        st.write(f"### üî¢ **Multiplicador Estimado por √âpoca: ~{augmentation_multiplier}x**")
        st.write(f"### üî¢ **Total Estimado Durante {epochs} √âpocas: ~{total_estimated:,} imagens aumentadas**")
        st.info(f"üí° **Explica√ß√£o:** Durante o treinamento, cada uma das {len(full_dataset)} imagens originais ser√° " +
                f"transformada aleatoriamente a cada √©poca, gerando aproximadamente {augmentation_multiplier}x varia√ß√µes √∫nicas " +
                f"ao longo de {epochs} √©pocas, totalizando cerca de {total_estimated:,} imagens processadas.")
    
    st.write("---")
    
    # Criar o dataset personalizado com aumento de dados
    train_dataset = CustomDataset(full_dataset, transform=train_transform)
    valid_dataset = CustomDataset(full_dataset, transform=test_transforms)
    test_dataset = CustomDataset(full_dataset, transform=test_transforms)

    # Dividir os √≠ndices para treino, valida√ß√£o e teste
    dataset_size = len(full_dataset)
    indices = list(range(dataset_size))
    np.random.shuffle(indices)

    train_end = int(train_split * dataset_size)
    valid_end = int((train_split + valid_split) * dataset_size)

    train_indices = indices[:train_end]
    valid_indices = indices[train_end:valid_end]
    test_indices = indices[valid_end:]

    train_dataset = torch.utils.data.Subset(train_dataset, train_indices)
    valid_dataset = torch.utils.data.Subset(valid_dataset, valid_indices)
    test_dataset = torch.utils.data.Subset(test_dataset, test_indices)

    # Dataloaders
    g = torch.Generator()
    g.manual_seed(42)

    if use_weighted_loss:
        targets = [full_dataset.targets[i] for i in train_indices]
        class_counts = np.bincount(targets)
        class_counts = class_counts + 1e-6  # Para evitar divis√£o por zero
        class_weights = 1.0 / class_counts
        class_weights = torch.FloatTensor(class_weights).to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
    else:
        criterion = nn.CrossEntropyLoss()

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)

    # Carregar o modelo
    model = get_model(model_name, num_classes, dropout_p=0.5, fine_tune=fine_tune)
    if model is None:
        return None

    # Definir o otimizador com L2 regularization (weight_decay)
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'AdamW':
        optimizer = optim.AdamW(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(trainable_params, lr=learning_rate, weight_decay=l2_lambda, momentum=0.9, nesterov=True)
    elif optimizer_name == 'Ranger' and ADVANCED_OPTIMIZERS_AVAILABLE:
        optimizer = optim_advanced.Ranger(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'Lion' and ADVANCED_OPTIMIZERS_AVAILABLE:
        optimizer = optim_advanced.Lion(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    else:
        st.warning(f"Otimizador {optimizer_name} n√£o dispon√≠vel. Usando Adam.")
        optimizer = optim.Adam(trainable_params, lr=learning_rate, weight_decay=l2_lambda)
    
    # Configurar Learning Rate Scheduler
    scheduler = None
    if scheduler_name == 'CosineAnnealingLR':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=learning_rate/100)
    elif scheduler_name == 'OneCycleLR':
        steps_per_epoch = len(train_loader)
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer, 
            max_lr=learning_rate*10, 
            epochs=epochs,
            steps_per_epoch=steps_per_epoch,
            pct_start=0.3
        )

    # Listas para armazenar as perdas e acur√°cias
    train_losses = []
    valid_losses = []
    train_accuracies = []
    valid_accuracies = []

    # Early Stopping
    best_valid_loss = float('inf')
    epochs_no_improve = 0
    
    # Par√¢metros para Mixup e CutMix
    use_mixup = (augmentation_type == 'mixup')
    use_cutmix = (augmentation_type == 'cutmix')
    mixup_alpha = 1.0
    cutmix_alpha = 1.0
    
    # Cache de par√¢metros para regulariza√ß√£o L1 (otimiza√ß√£o)
    trainable_params_list = list(filter(lambda p: p.requires_grad, model.parameters())) if l1_lambda > 0 else []

    # Treinamento
    for epoch in range(epochs):
        set_seed(42 + epoch)
        running_loss = 0.0
        running_corrects = 0
        model.train()

        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            
            # Aplicar Mixup ou CutMix se selecionado
            if use_mixup:
                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, mixup_alpha)
            elif use_cutmix:
                inputs, labels_a, labels_b, lam = cutmix_data(inputs, labels, cutmix_alpha)
            
            try:
                outputs = model(inputs)
            except Exception as e:
                st.error(f"Erro durante o treinamento: {e}")
                return None

            # Calcular loss
            if use_mixup or use_cutmix:
                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
                _, preds = torch.max(outputs, 1)
            else:
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)
            
            # Adicionar regulariza√ß√£o L1 se configurado
            if l1_lambda > 0:
                l1_reg = torch.tensor(0., device=device)
                for param in trainable_params_list:
                    l1_reg += torch.norm(param, 1)
                loss = loss + l1_lambda * l1_reg
            
            loss.backward()
            optimizer.step()
            
            # Atualizar scheduler OneCycleLR a cada batch
            if scheduler_name == 'OneCycleLR' and scheduler is not None:
                scheduler.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data if not (use_mixup or use_cutmix) else preds == labels_a.data)

        epoch_loss = running_loss / len(train_dataset)
        epoch_acc = running_corrects.double() / len(train_dataset)
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc.item())

        # Valida√ß√£o
        model.eval()
        valid_running_loss = 0.0
        valid_running_corrects = 0

        with torch.no_grad():
            for inputs, labels in valid_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                valid_running_loss += loss.item() * inputs.size(0)
                valid_running_corrects += torch.sum(preds == labels.data)

        valid_epoch_loss = valid_running_loss / len(valid_dataset)
        valid_epoch_acc = valid_running_corrects.double() / len(valid_dataset)
        valid_losses.append(valid_epoch_loss)
        valid_accuracies.append(valid_epoch_acc.item())

        st.write(f'**√âpoca {epoch+1}/{epochs}**')
        st.write(f'Perda de Treino: {epoch_loss:.4f} | Acur√°cia de Treino: {epoch_acc:.4f}')
        st.write(f'Perda de Valida√ß√£o: {valid_epoch_loss:.4f} | Acur√°cia de Valida√ß√£o: {valid_epoch_acc:.4f}')

        # Atualizar scheduler CosineAnnealingLR ap√≥s cada √©poca
        if scheduler_name == 'CosineAnnealingLR' and scheduler is not None:
            scheduler.step()

        # Early Stopping
        if valid_epoch_loss < best_valid_loss:
            best_valid_loss = valid_epoch_loss
            epochs_no_improve = 0
            best_model_wts = model.state_dict()
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                st.write('Early stopping!')
                model.load_state_dict(best_model_wts)
                break

    # Carregar os melhores pesos do modelo
    model.load_state_dict(best_model_wts)

    # Gr√°ficos de Perda e Acur√°cia
    plot_metrics(epochs, train_losses, valid_losses, train_accuracies, valid_accuracies)

    # Avalia√ß√£o Final no Conjunto de Teste
    st.write("**Avalia√ß√£o no Conjunto de Teste**")
    compute_metrics(model, test_loader, full_dataset.classes)

    # An√°lise de Erros
    st.write("**An√°lise de Erros**")
    error_analysis(model, test_loader, full_dataset.classes)

    # Liberar mem√≥ria
    del train_loader, valid_loader
    gc.collect()
    
    # Preparar hist√≥rico de treinamento para exporta√ß√£o
    training_history = {
        'epoch': list(range(1, len(train_losses) + 1)),
        'train_loss': train_losses,
        'valid_loss': valid_losses,
        'train_accuracy': train_accuracies,
        'valid_accuracy': valid_accuracies
    }

    return model, full_dataset.classes, training_history

def plot_metrics(epochs, train_losses, valid_losses, train_accuracies, valid_accuracies):
    """
    Plota os gr√°ficos de perda e acur√°cia.
    """
    epochs_range = range(1, len(train_losses)+1)
    fig, ax = plt.subplots(1, 2, figsize=(14, 5))

    # Gr√°fico de Perda
    ax[0].plot(epochs_range, train_losses, label='Treino')
    ax[0].plot(epochs_range, valid_losses, label='Valida√ß√£o')
    ax[0].set_title('Perda por √âpoca')
    ax[0].set_xlabel('√âpocas')
    ax[0].set_ylabel('Perda')
    ax[0].legend()

    # Gr√°fico de Acur√°cia
    ax[1].plot(epochs_range, train_accuracies, label='Treino')
    ax[1].plot(epochs_range, valid_accuracies, label='Valida√ß√£o')
    ax[1].set_title('Acur√°cia por √âpoca')
    ax[1].set_xlabel('√âpocas')
    ax[1].set_ylabel('Acur√°cia')
    ax[1].legend()

    st.pyplot(fig)
    plt.close(fig)

def compute_metrics(model, dataloader, classes):
    """
    Calcula m√©tricas detalhadas e exibe matriz de confus√£o e relat√≥rio de classifica√ß√£o.
    """
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probabilities.cpu().numpy())

    # Relat√≥rio de Classifica√ß√£o
    report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True, zero_division=0)
    st.text("Relat√≥rio de Classifica√ß√£o:")
    st.write(pd.DataFrame(report).transpose())

    # Matriz de Confus√£o Normalizada
    cm = confusion_matrix(all_labels, all_preds, normalize='true')
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=ax)
    ax.set_xlabel('Predito')
    ax.set_ylabel('Verdadeiro')
    ax.set_title('Matriz de Confus√£o Normalizada')
    st.pyplot(fig)
    plt.close(fig)

    # Curva ROC
    if len(classes) == 2:
        fpr, tpr, thresholds = roc_curve(all_labels, [p[1] for p in all_probs])
        roc_auc = roc_auc_score(all_labels, [p[1] for p in all_probs])
        fig, ax = plt.subplots()
        ax.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)
        ax.plot([0, 1], [0, 1], 'k--')
        ax.set_xlabel('Taxa de Falsos Positivos')
        ax.set_ylabel('Taxa de Verdadeiros Positivos')
        ax.set_title('Curva ROC')
        ax.legend(loc='lower right')
        st.pyplot(fig)
        plt.close(fig)
    else:
        # Multiclasse
        binarized_labels = label_binarize(all_labels, classes=range(len(classes)))
        roc_auc = roc_auc_score(binarized_labels, np.array(all_probs), average='weighted', multi_class='ovr')
        st.write(f"AUC-ROC M√©dia Ponderada: {roc_auc:.4f}")

def error_analysis(model, dataloader, classes):
    """
    Realiza an√°lise de erros mostrando algumas imagens mal classificadas.
    """
    model.eval()
    misclassified_images = []
    misclassified_labels = []
    misclassified_preds = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            incorrect = preds != labels
            if incorrect.any():
                misclassified_images.extend(inputs[incorrect].cpu())
                misclassified_labels.extend(labels[incorrect].cpu())
                misclassified_preds.extend(preds[incorrect].cpu())
                if len(misclassified_images) >= 5:
                    break

    if misclassified_images:
        st.write("Algumas imagens mal classificadas:")
        num_images = min(5, len(misclassified_images))
        fig, axes = plt.subplots(1, num_images, figsize=(15, 3))
        
        # Handle case when only one image (axes is not an array)
        if num_images == 1:
            axes = [axes]
            
        for i in range(num_images):
            image = misclassified_images[i]
            # Denormalize the image for proper display
            image = denormalize_image(image)
            axes[i].imshow(image)
            axes[i].set_title(f"V: {classes[misclassified_labels[i]]}\nP: {classes[misclassified_preds[i]]}")
            axes[i].axis('off')
        st.pyplot(fig)
        plt.close(fig)
    else:
        st.write("Nenhuma imagem mal classificada encontrada.")

def create_export_csv(training_history, classification_results=None, clustering_results=None):
    """
    Cria um DataFrame consolidado com todos os resultados para exporta√ß√£o CSV.
    
    Args:
        training_history: Dict com hist√≥rico de treinamento (epoch, losses, accuracies)
        classification_results: Dict opcional com resultados de classifica√ß√£o de imagens
        clustering_results: Dict opcional com resultados de clustering
    
    Returns:
        pd.DataFrame: DataFrame consolidado para exporta√ß√£o
    """
    # Criar DataFrame do hist√≥rico de treinamento
    df_training = pd.DataFrame(training_history)
    
    # Se houver resultados de classifica√ß√£o, adicionar
    if classification_results:
        df_classification = pd.DataFrame([classification_results])
        # Adicionar colunas vazias de treinamento para manter consist√™ncia
        for col in df_training.columns:
            if col not in df_classification.columns:
                df_classification[col] = None
        df_combined = pd.concat([df_training, df_classification], ignore_index=True)
    else:
        df_combined = df_training
    
    # Se houver resultados de clustering, adicionar
    if clustering_results:
        df_clustering = pd.DataFrame([clustering_results])
        for col in df_combined.columns:
            if col not in df_clustering.columns:
                df_clustering[col] = None
        df_combined = pd.concat([df_combined, df_clustering], ignore_index=True)
    
    return df_combined

def export_to_csv(df, filename="resultados_treinamento.csv"):
    """
    Converte DataFrame para CSV e retorna para download.
    
    Args:
        df: DataFrame para exportar
        filename: Nome do arquivo CSV
    
    Returns:
        str: CSV em formato de string
    """
    return df.to_csv(index=False).encode('utf-8')

def encode_image_to_base64(image):
    """
    Codifica uma imagem PIL para base64.
    
    Args:
        image: PIL Image
    
    Returns:
        str: Imagem codificada em base64
    """
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

def analyze_image_with_gemini(image, api_key, model_name, class_name, confidence, gradcam_description=""):
    """
    Analisa uma imagem usando Google Gemini com vis√£o computacional.
    
    Args:
        image: PIL Image
        api_key: Chave API do Gemini
        model_name: Nome do modelo Gemini
        class_name: Classe predita pelo modelo
        confidence: Confian√ßa da predi√ß√£o
        gradcam_description: Descri√ß√£o do Grad-CAM
    
    Returns:
        str: An√°lise t√©cnica e forense da imagem
    """
    if not GEMINI_AVAILABLE:
        return "Google Generative AI n√£o est√° dispon√≠vel. Instale com: pip install google-generativeai"
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        
        prompt = f"""
        Voc√™ √© um especialista em an√°lise de imagens e interpreta√ß√£o t√©cnica e forense.
        
        **Contexto da Classifica√ß√£o:**
        - Classe Predita: {class_name}
        - Confian√ßa: {confidence:.4f} ({confidence*100:.2f}%)
        - An√°lise Grad-CAM: {gradcam_description if gradcam_description else 'N√£o dispon√≠vel'}
        
        Por favor, realize uma an√°lise COMPLETA e DETALHADA da imagem fornecida, incluindo:
        
        1. **Descri√ß√£o Visual Detalhada:**
           - Descreva todos os elementos visuais presentes na imagem
           - Identifique padr√µes, texturas, cores e formas relevantes
           - Analise a qualidade e caracter√≠sticas da imagem
        
        2. **Interpreta√ß√£o T√©cnica:**
           - Avalie se a classifica√ß√£o como "{class_name}" √© compat√≠vel com o que voc√™ observa
           - Identifique caracter√≠sticas espec√≠ficas que suportam ou contradizem a classifica√ß√£o
           - Analise a confian√ßa de {confidence*100:.2f}% em rela√ß√£o aos padr√µes visuais
        
        3. **An√°lise Forense:**
           - Identifique poss√≠veis artefatos ou anomalias na imagem
           - Avalie a integridade e autenticidade da imagem
           - Destaque √°reas de interesse ou preocupa√ß√£o
        
        4. **Recomenda√ß√µes:**
           - Sugira se a classifica√ß√£o deve ser aceita ou revista
           - Recomende an√°lises adicionais se necess√°rio
           - Forne√ßa orienta√ß√µes para melhorar a confian√ßa na classifica√ß√£o
        
        Seja detalhado, t√©cnico e preciso na sua an√°lise.
        """
        
        response = model.generate_content([prompt, image])
        return response.text
    
    except Exception as e:
        return f"Erro ao analisar com Gemini: {str(e)}"

def analyze_image_with_groq_vision(image, api_key, model_name, class_name, confidence, gradcam_description=""):
    """
    Analisa uma imagem usando Groq com vis√£o computacional.
    Nota: Groq pode ter limita√ß√µes de vis√£o dependendo do modelo.
    
    Args:
        image: PIL Image
        api_key: Chave API do Groq
        model_name: Nome do modelo Groq
        class_name: Classe predita pelo modelo
        confidence: Confian√ßa da predi√ß√£o
        gradcam_description: Descri√ß√£o do Grad-CAM
    
    Returns:
        str: An√°lise t√©cnica e forense da imagem
    """
    if not GROQ_AVAILABLE:
        return "Groq n√£o est√° dispon√≠vel. Instale com: pip install groq"
    
    try:
        # Codificar imagem para base64
        image_base64 = encode_image_to_base64(image)
        
        client = Groq(api_key=api_key)
        
        prompt = f"""
        Voc√™ √© um especialista em an√°lise de imagens e interpreta√ß√£o t√©cnica e forense.
        
        **Contexto da Classifica√ß√£o:**
        - Classe Predita: {class_name}
        - Confian√ßa: {confidence:.4f} ({confidence*100:.2f}%)
        - An√°lise Grad-CAM: {gradcam_description if gradcam_description else 'N√£o dispon√≠vel'}
        
        IMPORTANTE: Com base nas informa√ß√µes fornecidas e na descri√ß√£o visual que voc√™ pode inferir,
        realize uma an√°lise COMPLETA e DETALHADA, incluindo:
        
        1. **Interpreta√ß√£o T√©cnica:**
           - Avalie se a classifica√ß√£o como "{class_name}" parece apropriada
           - Identifique caracter√≠sticas que voc√™ esperaria ver nesta classe
           - Analise a confian√ßa de {confidence*100:.2f}%
        
        2. **An√°lise Forense:**
           - Discuta poss√≠veis pontos de aten√ß√£o na classifica√ß√£o
           - Sugira √°reas que podem precisar de verifica√ß√£o adicional
        
        3. **Recomenda√ß√µes:**
           - Sugira se a classifica√ß√£o deve ser aceita ou revista
           - Recomende an√°lises adicionais se necess√°rio
           - Forne√ßa orienta√ß√µes para melhorar a confian√ßa
        
        Nota: Se o modelo n√£o suporta vis√£o direta, forne√ßa an√°lise baseada no contexto fornecido.
        """
        
        # Tentar com suporte de imagem (alguns modelos Groq podem n√£o suportar)
        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_base64}",
                                },
                            },
                        ],
                    }
                ],
                model=model_name,
            )
        except:
            # Fallback: an√°lise apenas com texto se vis√£o n√£o √© suportada
            chat_completion = client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompt + "\n\n[NOTA: An√°lise baseada apenas em contexto textual, pois o modelo pode n√£o suportar vis√£o direta]"
                    }
                ],
                model=model_name,
            )
        
        return chat_completion.choices[0].message.content
    
    except Exception as e:
        return f"Erro ao analisar com Groq: {str(e)}"

def generate_gradcam_description(activation_map):
    """
    Gera uma descri√ß√£o textual do mapa de ativa√ß√£o Grad-CAM.
    
    Args:
        activation_map: Mapa de ativa√ß√£o numpy array
    
    Returns:
        str: Descri√ß√£o das regi√µes ativadas
    """
    if activation_map is None:
        return "Grad-CAM n√£o dispon√≠vel"
    
    # Calcular estat√≠sticas do mapa de ativa√ß√£o
    mean_activation = np.mean(activation_map)
    max_activation = np.max(activation_map)
    
    # Encontrar regi√µes de alta ativa√ß√£o (acima de 70% do m√°ximo)
    threshold = 0.7 * max_activation
    high_activation_regions = activation_map > threshold
    num_high_regions = np.sum(high_activation_regions)
    total_pixels = activation_map.size
    percentage_high = (num_high_regions / total_pixels) * 100
    
    description = f"""
    O mapa Grad-CAM mostra:
    - Ativa√ß√£o m√©dia: {mean_activation:.3f}
    - Ativa√ß√£o m√°xima: {max_activation:.3f}
    - Regi√µes de alta ativa√ß√£o: {percentage_high:.1f}% da imagem
    - O modelo focou em {num_high_regions} pixels espec√≠ficos para tomar sua decis√£o
    """
    
    # Analisar distribui√ß√£o espacial
    height, width = activation_map.shape
    center_activation = activation_map[height//4:3*height//4, width//4:3*width//4].mean()
    border_activation = (activation_map[:height//4, :].mean() + 
                        activation_map[3*height//4:, :].mean() + 
                        activation_map[:, :width//4].mean() + 
                        activation_map[:, 3*width//4:].mean()) / 4
    
    if center_activation > border_activation * 1.5:
        description += "\n    - O modelo focou principalmente no CENTRO da imagem"
    elif border_activation > center_activation * 1.5:
        description += "\n    - O modelo focou principalmente nas BORDAS da imagem"
    else:
        description += "\n    - O modelo analisou a imagem de forma DISTRIBU√çDA"
    
    return description

def extract_features(dataset, model, batch_size):
    """
    Extrai caracter√≠sticas de um conjunto de dados usando um modelo pr√©-treinado.
    """
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker)

    features = []
    labels = []

    model.eval()
    with torch.no_grad():
        for inputs, lbls in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            outputs = outputs.view(outputs.size(0), -1)  # Flatten
            features.append(outputs.cpu().numpy())
            labels.extend(lbls.numpy())

    features = np.concatenate(features, axis=0)
    labels = np.array(labels)
    return features, labels

def perform_clustering(features, num_clusters):
    """
    Aplica algoritmos de clustering √†s caracter√≠sticas.
    """
    # Clustering Hier√°rquico
    hierarchical = AgglomerativeClustering(n_clusters=num_clusters)
    hierarchical_labels = hierarchical.fit_predict(features)

    # K-Means
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    kmeans_labels = kmeans.fit_predict(features)

    return hierarchical_labels, kmeans_labels

def evaluate_clustering(true_labels, cluster_labels, method_name):
    """
    Avalia os resultados do clustering comparando com as classes reais.
    """
    ari = adjusted_rand_score(true_labels, cluster_labels)
    nmi = normalized_mutual_info_score(true_labels, cluster_labels)
    st.write(f"**M√©tricas para {method_name}:**")
    st.write(f"Adjusted Rand Index: {ari:.4f}")
    st.write(f"Normalized Mutual Information Score: {nmi:.4f}")

def visualize_clusters(features, true_labels, hierarchical_labels, kmeans_labels, classes):
    """
    Visualiza os clusters usando redu√ß√£o de dimensionalidade e inclui as classes verdadeiras com nomes de r√≥tulos.
    """
    # Redu√ß√£o de dimensionalidade com PCA para visualizar os clusters em 2D
    pca = PCA(n_components=2)
    reduced_features = pca.fit_transform(features)

    # Mapear os r√≥tulos verdadeiros para os nomes das classes
    true_labels_named = [classes[label] for label in true_labels]
    
    # Usar as cores distintas e vis√≠veis para garantir que os clusters sejam claramente separados
    color_palette = sns.color_palette("tab10", len(set(true_labels)))

    fig, axes = plt.subplots(1, 3, figsize=(21, 6))  # Agora temos 3 gr√°ficos: Hierarchical, K-Means e classes verdadeiras

    # Clustering Hier√°rquico
    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=hierarchical_labels, palette="deep", ax=axes[0], legend='full')
    axes[0].set_title('Clustering Hier√°rquico')
    ari_hierarchical = adjusted_rand_score(true_labels, hierarchical_labels)
    nmi_hierarchical = normalized_mutual_info_score(true_labels, hierarchical_labels)
    axes[0].text(0.1, 0.9, f"ARI: {ari_hierarchical:.2f}\nNMI: {nmi_hierarchical:.2f}", horizontalalignment='center', verticalalignment='center', transform=axes[0].transAxes, bbox=dict(facecolor='white', alpha=0.5))

    # K-Means Clustering
    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=kmeans_labels, palette="deep", ax=axes[1], legend='full')
    axes[1].set_title('K-Means Clustering')
    ari_kmeans = adjusted_rand_score(true_labels, kmeans_labels)
    nmi_kmeans = normalized_mutual_info_score(true_labels, kmeans_labels)
    axes[1].text(0.1, 0.9, f"ARI: {ari_kmeans:.2f}\nNMI: {nmi_kmeans:.2f}", horizontalalignment='center', verticalalignment='center', transform=axes[1].transAxes, bbox=dict(facecolor='white', alpha=0.5))

    # Classes verdadeiras
    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=true_labels_named, palette=color_palette, ax=axes[2], legend='full')
    axes[2].set_title('Classes Verdadeiras')

    # Exibir os gr√°ficos
    st.pyplot(fig)
    plt.close(fig)

def evaluate_image(model, image, classes):
    """
    Avalia uma √∫nica imagem e retorna a classe predita e a confian√ßa.
    """
    model.eval()
    image_tensor = test_transforms(image).unsqueeze(0).to(device)
    with torch.no_grad():
        output = model(image_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        confidence, predicted = torch.max(probabilities, 1)
        class_idx = predicted.item()
        class_name = classes[class_idx]
        return class_name, confidence.item()

def evaluate_image_with_statistics(model, image, classes, activation_map=None, n_bootstrap=100):
    """
    Avalia uma imagem com an√°lise estat√≠stica completa.
    
    Args:
        model: Modelo treinado
        image: Imagem PIL
        classes: Lista de nomes das classes
        activation_map: Mapa de ativa√ß√£o do Grad-CAM (opcional)
        n_bootstrap: N√∫mero de itera√ß√µes para bootstrap
    
    Returns:
        Dict com an√°lise completa incluindo estat√≠sticas e diagn√≥sticos
    """
    model.eval()
    
    # Ensure model parameters don't require gradients (in case Grad-CAM left them enabled)
    for param in model.parameters():
        param.requires_grad = False
    
    image_tensor = test_transforms(image).unsqueeze(0).to(device)
    
    # 1. Predi√ß√£o b√°sica
    with torch.no_grad():
        output = model(image_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        probs_array = probabilities.cpu().numpy()[0]
        confidence, predicted = torch.max(probabilities, 1)
        class_idx = predicted.item()
        class_name = classes[class_idx]
    
    # 2. Bootstrap validation
    stat_analyzer = StatisticalAnalyzer()
    bootstrap_results = stat_analyzer.bootstrap_validation(model, image_tensor, n_bootstrap)
    
    # 3. Confidence intervals
    confidence_interval = stat_analyzer.calculate_confidence_interval(
        bootstrap_results['predictions_distribution'][:, class_idx]
    )
    
    # 4. Differential diagnostics
    diag_analyzer = DiagnosticAnalyzer()
    differential_diagnoses = diag_analyzer.differential_diagnosis(
        bootstrap_results['mean_probabilities'], 
        classes, 
        top_k=5
    )
    
    # 5. Exclusion criteria
    exclusion_analysis = diag_analyzer.exclusion_criteria(
        bootstrap_results['mean_probabilities'], 
        classes
    )
    
    # 6. Distinctive features (se houver activation map)
    distinctive_features = None
    if activation_map is not None:
        distinctive_features = diag_analyzer.distinctive_features(activation_map)
    
    # 7. Uncertainty quantification
    uncertainty_analyzer = UncertaintyAnalyzer()
    uncertainty_analysis = uncertainty_analyzer.quantify_uncertainty(bootstrap_results)
    
    # 8. Error impact assessment
    error_impact = uncertainty_analyzer.assess_error_impact(
        bootstrap_results['mean_probabilities'],
        classes
    )
    
    # 9. Safety margin
    safety_analysis = uncertainty_analyzer.safety_margin(
        confidence.item(),
        min_acceptable=0.7,
        target=0.9
    )
    
    # 10. Clinical/practical impact
    clinical_impact = uncertainty_analyzer.clinical_impact_assessment(
        confidence.item(),
        class_name,
        differential_diagnoses
    )
    
    # 11. Significance test (se houver diagn√≥sticos diferenciais)
    significance_test = None
    if len(differential_diagnoses) >= 2:
        significance_test = stat_analyzer.significance_test(
            differential_diagnoses[0]['probability'],
            differential_diagnoses[1]['probability'],
            bootstrap_results['predictions_distribution']
        )
    
    return {
        # B√°sico
        'predicted_class': class_name,
        'predicted_index': class_idx,
        'confidence': confidence.item(),
        'all_probabilities': probs_array,
        
        # Estat√≠sticas
        'confidence_interval': confidence_interval,
        'bootstrap_results': bootstrap_results,
        'significance_test': significance_test,
        
        # Diagn√≥stico
        'differential_diagnoses': differential_diagnoses,
        'exclusion_analysis': exclusion_analysis,
        'distinctive_features': distinctive_features,
        
        # Incerteza e Risco
        'uncertainty_analysis': uncertainty_analysis,
        'error_impact': error_impact,
        'safety_analysis': safety_analysis,
        'clinical_impact': clinical_impact
    }

#________________________________________________

#________________________________________________

def display_statistical_analysis(analysis_results):
    """
    Exibe an√°lise estat√≠stica completa em formato organizado no Streamlit.
    
    Args:
        analysis_results: Resultados da fun√ß√£o evaluate_image_with_statistics
    """
    st.write("---")
    st.write("## üìä AN√ÅLISE ESTAT√çSTICA COMPLETA")
    
    # ========== PREDI√á√ÉO PRINCIPAL ==========
    st.write("### üéØ Predi√ß√£o Principal")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Classe Predita", analysis_results['predicted_class'])
    with col2:
        st.metric("Confian√ßa", f"{analysis_results['confidence']:.2%}")
    with col3:
        safety_emoji = {
            'safe': 'üü¢',
            'unsafe': 'üî¥'
        }[analysis_results['safety_analysis']['status']]
        st.metric("Status de Seguran√ßa", 
                 f"{safety_emoji} {analysis_results['safety_analysis']['status'].upper()}")
    
    # ========== INTERVALOS DE CONFIAN√áA ==========
    st.write("### üìà Intervalo de Confian√ßa (95%)")
    ci = analysis_results['confidence_interval']
    st.write(f"**Confian√ßa M√©dia (Bootstrap):** {ci['mean']:.2%}")
    st.write(f"**Intervalo:** [{ci['lower']:.2%}, {ci['upper']:.2%}]")
    st.write(f"**Margem de Erro:** ¬±{ci['margin_error']:.2%}")
    
    # Progress bar visual (convert to Python float for Streamlit compatibility)
    st.progress(float(ci['mean']))
    
    # ========== DIAGN√ìSTICOS DIFERENCIAIS ==========
    st.write("### üîç Diagn√≥sticos Diferenciais")
    
    diff_data = []
    for diff in analysis_results['differential_diagnoses']:
        diff_data.append({
            'Rank': diff['rank'],
            'Classe': diff['class'],
            'Probabilidade': f"{diff['probability']:.2%}",
            'N√≠vel de Confian√ßa': diff['confidence_level']
        })
    
    if diff_data:
        st.dataframe(pd.DataFrame(diff_data), use_container_width=True)
    
    # Teste de signific√¢ncia
    if analysis_results['significance_test'] and analysis_results['significance_test']['p_value']:
        st.write("#### üìä Teste de Signific√¢ncia (Top 2)")
        sig_test = analysis_results['significance_test']
        st.write(f"**Diferen√ßa de Probabilidade:** {sig_test['probability_diff']:.2%}")
        st.write(f"**Valor-p:** {sig_test['p_value']:.4f}")
        
        if sig_test['significant']:
            st.success(f"‚úÖ {sig_test['interpretation']} (p < 0.05)")
        else:
            st.warning(f"‚ö†Ô∏è {sig_test['interpretation']} (p ‚â• 0.05)")
    
    # ========== CRIT√âRIOS DE EXCLUS√ÉO ==========
    st.write("### ‚ùå Crit√©rios de Exclus√£o")
    excl = analysis_results['exclusion_analysis']
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Classes Exclu√≠das", excl['excluded_count'])
    with col2:
        st.metric("Classes Restantes", excl['remaining_count'])
    
    if excl['excluded_classes']:
        with st.expander("Ver classes exclu√≠das"):
            for exc in excl['excluded_classes'][:5]:  # Mostrar at√© 5
                st.write(f"- **{exc['class']}**: {exc['reason']}")
    
    # ========== CARACTER√çSTICAS DISTINTIVAS ==========
    if analysis_results['distinctive_features']:
        st.write("### üé® Caracter√≠sticas Distintivas")
        feat = analysis_results['distinctive_features']
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Ativa√ß√£o M√°xima", f"{feat['max_activation']:.3f}")
        with col2:
            st.metric("Ativa√ß√£o M√©dia", f"{feat['mean_activation']:.3f}")
        with col3:
            st.metric("√Årea de Alta Ativa√ß√£o", f"{feat['high_activation_percentage']:.1f}%")
        
        st.info(f"**Interpreta√ß√£o:** {feat['interpretation']}")
    
    # ========== AN√ÅLISE DE INCERTEZA ==========
    st.write("### üé≤ Quantifica√ß√£o de Incerteza")
    uncert = analysis_results['uncertainty_analysis']
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("N√≠vel de Incerteza", uncert['uncertainty_level'])
    with col2:
        st.metric("Incerteza Total", f"{uncert['total_uncertainty']:.3f}")
    with col3:
        st.metric("Entropia Normalizada", f"{uncert['normalized_entropy']:.3f}")
    
    st.write("**Fontes de Incerteza:**")
    st.write(f"- Varia√ß√£o do Modelo: {uncert['sources']['model_variation']:.3f}")
    st.write(f"- Ambiguidade da Predi√ß√£o: {uncert['sources']['prediction_ambiguity']:.3f}")
    
    # ========== IMPACTO DE ERROS ==========
    st.write("### ‚ö†Ô∏è Avalia√ß√£o de Impacto de Erros")
    error_imp = analysis_results['error_impact']
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Probabilidade de Erro", f"{error_imp['error_probability']:.2%}")
    with col2:
        st.metric("Escore de Impacto", f"{error_imp['impact_score']:.3f}")
    
    # Mostrar recomenda√ß√£o com cor apropriada
    if '‚ö†Ô∏è ATEN√á√ÉO' in error_imp['recommendation']:
        st.error(error_imp['recommendation'])
    elif '‚ö†Ô∏è' in error_imp['recommendation']:
        st.warning(error_imp['recommendation'])
    else:
        st.success(error_imp['recommendation'])
    
    # ========== MARGEM DE SEGURAN√áA ==========
    st.write("### üõ°Ô∏è Margem de Seguran√ßa")
    safety = analysis_results['safety_analysis']
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Confian√ßa Atual", f"{safety['confidence']:.2%}")
    with col2:
        st.metric("M√≠nimo Aceit√°vel", f"{safety['min_acceptable']:.2%}")
    with col3:
        st.metric("Alvo Desejado", f"{safety['target']:.2%}")
    with col4:
        st.metric("Escore de Seguran√ßa", f"{safety['safety_score']:.2%}")
    
    st.write(f"**Margem at√© M√≠nimo:** {safety['margin_to_minimum']:.2%}")
    st.write(f"**Margem at√© Alvo:** {safety['margin_to_target']:.2%}")
    
    # Interpreta√ß√£o com emoji
    st.info(safety['interpretation'])
    
    # ========== IMPACTO CL√çNICO/PR√ÅTICO ==========
    st.write("### üè• Avalia√ß√£o de Impacto Cl√≠nico/Pr√°tico")
    clinical = analysis_results['clinical_impact']
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Diagn√≥stico Prim√°rio", clinical['primary_diagnosis'])
    with col2:
        priority_color = {
            'Normal': 'üü¢',
            'M√©dia': 'üü°',
            'Alta': 'üî¥'
        }[clinical['priority_level']]
        st.metric("Prioridade", f"{priority_color} {clinical['priority_level']}")
    with col3:
        st.metric("Ambiguidade Diagn√≥stica", f"{clinical['diagnostic_ambiguity']:.2%}")
    
    st.write(f"**A√ß√£o Recomendada:** {clinical['recommended_action']}")
    st.write(f"**N√∫mero de Diagn√≥sticos Diferenciais:** {clinical['differential_count']}")
    
    if clinical['requires_specialist']:
        st.warning("‚öïÔ∏è Consulta com especialista recomendada devido √† complexidade do caso")
    else:
        st.success("‚úÖ Caso pode ser gerenciado com protocolos padr√£o")
    
    # ========== VALIDA√á√ÉO BOOTSTRAP ==========
    with st.expander("üìä Detalhes da Valida√ß√£o Bootstrap"):
        boot = analysis_results['bootstrap_results']
        st.write(f"**Confian√ßa Bootstrap:** {boot['confidence_bootstrap']:.2%}")
        st.write(f"**Incerteza (std):** {boot['uncertainty']:.4f}")
        
        st.write("**Probabilidades M√©dias por Classe:**")
        # Create proper dataframe for all classes
        all_classes = list(range(len(boot['mean_probabilities'])))
        prob_df = pd.DataFrame({
            '√çndice': all_classes,
            'Probabilidade M√©dia': [f"{p:.2%}" for p in boot['mean_probabilities']],
            'Desvio Padr√£o': [f"{s:.4f}" for s in boot['std_probabilities']]
        })
        st.dataframe(prob_df.head(10), use_container_width=True)  # Mostrar top 10

def visualize_activations(model, image, class_names, gradcam_type='SmoothGradCAMpp'):
    """
    Visualiza as ativa√ß√µes na imagem usando diferentes variantes de Grad-CAM.
    
    Args:
        model: Modelo treinado
        image: Imagem PIL
        class_names: Lista de nomes das classes
        gradcam_type: Tipo de Grad-CAM ('GradCAM', 'GradCAMpp', 'SmoothGradCAMpp', 'LayerCAM')
    
    Returns:
        activation_map_resized: Mapa de ativa√ß√£o normalizado ou None em caso de erro
    """
    cam_extractor = None
    try:
        # Ensure model is in eval mode
        model.eval()
        
        # Prepare input tensor
        # Note: torchcam handles gradient requirements internally
        input_tensor = test_transforms(image).unsqueeze(0).to(device)
        
        # Verificar se o modelo √© suportado
        model_type = type(model).__name__
        if 'ResNet' in model_type:
            target_layer = model.layer4[-1]
        elif 'DenseNet' in model_type:
            target_layer = model.features.denseblock4.denselayer16
        elif 'VisionTransformer' in model_type:
            # Para ViT, usar a √∫ltima camada do encoder
            target_layer = model.encoder.layers[-1].ln_1
        else:
            st.warning(f"Modelo {model_type} pode n√£o ter suporte completo para Grad-CAM. Tentando com camada padr√£o...")
            # Tentar usar a √∫ltima camada dispon√≠vel
            try:
                if hasattr(model, 'encoder'):
                    target_layer = model.encoder.layers[-1]
                else:
                    st.error("N√£o foi poss√≠vel determinar camada para Grad-CAM.")
                    return None
            except:
                st.error("Modelo n√£o suportado para Grad-CAM.")
                return None
        
        # Criar o objeto CAM usando torchcam
        if gradcam_type == 'GradCAM':
            cam_extractor = GradCAM(model, target_layer=target_layer)
        elif gradcam_type == 'GradCAMpp':
            cam_extractor = GradCAMpp(model, target_layer=target_layer)
        elif gradcam_type == 'SmoothGradCAMpp':
            cam_extractor = SmoothGradCAMpp(model, target_layer=target_layer)
        elif gradcam_type == 'LayerCAM':
            cam_extractor = LayerCAM(model, target_layer=target_layer)
        else:
            st.error(f"Tipo de Grad-CAM n√£o suportado: {gradcam_type}")
            return None
        
        # Habilitar gradientes explicitamente
        with torch.set_grad_enabled(True):
            out = model(input_tensor)  # Faz a previs√£o
            _, pred = torch.max(out, 1)  # Obt√©m a classe predita
            pred_class = pred.item()
        
        # Gerar o mapa de ativa√ß√£o
        activation_map = cam_extractor(pred_class, out)
        
        # Obter o mapa de ativa√ß√£o da primeira imagem no lote
        activation_map = activation_map[0].cpu().detach().numpy()
        
        # Redimensionar o mapa de ativa√ß√£o para coincidir com o tamanho da imagem original
        activation_map_resized = cv2.resize(activation_map, (image.size[0], image.size[1]))
        
        # Normalizar o mapa de ativa√ß√£o para o intervalo [0, 1]
        activation_map_resized = (activation_map_resized - activation_map_resized.min()) / (activation_map_resized.max() - activation_map_resized.min() + 1e-8)
        
        # Converter a imagem para array NumPy
        image_np = np.array(image)
        
        # Converter o mapa de ativa√ß√£o em uma imagem RGB
        heatmap = cv2.applyColorMap(np.uint8(255 * activation_map_resized), cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        # Sobrepor o mapa de ativa√ß√£o na imagem original
        superimposed_img = heatmap * 0.4 + image_np * 0.6
        superimposed_img = np.uint8(superimposed_img)
        
        # Exibir a imagem original e o mapa de ativa√ß√£o sobreposto
        fig, ax = plt.subplots(1, 2, figsize=(10, 5))
        
        # Imagem original
        ax[0].imshow(image_np)
        ax[0].set_title('Imagem Original')
        ax[0].axis('off')
        
        # Imagem com Grad-CAM
        ax[1].imshow(superimposed_img)
        ax[1].set_title(f'{gradcam_type}')
        ax[1].axis('off')
        
        # Exibir as imagens com o Streamlit
        st.pyplot(fig)
        plt.close(fig)
        
        return activation_map_resized
        
    except Exception as e:
        st.error(f"Erro ao gerar Grad-CAM: {str(e)}")
        st.info("Visualiza√ß√£o Grad-CAM n√£o dispon√≠vel para este modelo/configura√ß√£o.")
        return None
    finally:
        # CRITICAL: Remove hooks and reset model state to prevent interference with subsequent calls
        if cam_extractor is not None:
            try:
                # Try multiple cleanup methods for compatibility with different torchcam versions
                if hasattr(cam_extractor, 'remove_hooks'):
                    cam_extractor.remove_hooks()
                elif hasattr(cam_extractor, 'clear_hooks'):
                    cam_extractor.clear_hooks()
                elif hasattr(cam_extractor, 'reset_hooks'):
                    cam_extractor.reset_hooks()
            except Exception as e:
                # If hook removal fails, log it but continue
                st.warning(f"Aviso: N√£o foi poss√≠vel remover hooks: {e}")




def main():

    # Definir o caminho do √≠cone
    icon_path = "logo.png"  # Verifique se o arquivo logo.png est√° no diret√≥rio correto
    
    # Verificar se o arquivo de √≠cone existe antes de configur√°-lo
    if os.path.exists(icon_path):
        st.set_page_config(page_title="Geomaker", page_icon=icon_path, layout="wide")
        logging.info(f"√çcone {icon_path} carregado com sucesso.")
    else:
        # Se o √≠cone n√£o for encontrado, carrega sem favicon
        st.set_page_config(page_title="Geomaker", layout="wide")
        logging.warning(f"√çcone {icon_path} n√£o encontrado, carregando sem favicon.")
    
    # Layout da p√°gina
    if os.path.exists('capa.png'):
        st.image('capa.png', caption='Laborat√≥rio de Educa√ß√£o e Intelig√™ncia Artificial - Geomaker. "A melhor forma de prever o futuro √© invent√°-lo." - Alan Kay', width='stretch')
    else:
        st.warning("Imagem 'capa.png' n√£o encontrada.")
    
    if os.path.exists("logo.png"):
        st.sidebar.image("logo.png", width=200)
    else:
        st.sidebar.text("Imagem do logotipo n√£o encontrada.")
    
    
  #___________________________________________________________
    st.title("Classifica√ß√£o por Imagens com Aprendizado Profundo")
    st.write("Este aplicativo permite treinar um modelo de classifica√ß√£o de imagens e aplicar algoritmos de clustering para an√°lise comparativa.")
    with st.expander("Transforma√ß√µes de Dados e Aumento de Dados no Treinamento de Redes Neurais"):
        st.write("""
        As **transforma√ß√µes de dados** e o **aumento de dados** s√£o t√©cnicas essenciais no treinamento de redes neurais profundas, principalmente em tarefas de vis√£o computacional. 
        Essas abordagens buscam melhorar a capacidade de generaliza√ß√£o dos modelos, gerando **imagens sint√©ticas** a partir dos dados de treinamento. Tais t√©cnicas s√£o particularmente 
        valiosas quando o conjunto de dados dispon√≠vel √© pequeno ou apresenta pouca diversidade. A normaliza√ß√£o, por sua vez, assegura que os valores dos pixels estejam em uma escala adequada, 
        resultando em um treinamento mais est√°vel e eficiente. Diversos estudos apontam que essas pr√°ticas s√£o eficazes para evitar **overfitting** e aumentar a robustez do modelo 
        (Shorten & Khoshgoftaar, 2019).
        """)
    
        st.write("### Aumento de Dados no Treinamento")
    
        st.write("""
        O **aumento de dados** ou *data augmentation* consiste na aplica√ß√£o de transforma√ß√µes aleat√≥rias √†s imagens do conjunto de treinamento para gerar novas amostras sint√©ticas. 
        No c√≥digo implementado, essa t√©cnica √© realizada com a classe `transforms.Compose` da biblioteca **torchvision**, que aplica uma sequ√™ncia de transforma√ß√µes.
        """)
    
        st.write("#### Transforma√ß√µes Aplicadas no Treinamento")
        
        st.write("""
        1. **RandomApply**: Aplica aleatoriamente um conjunto de transforma√ß√µes com 50% de probabilidade. Esse procedimento aumenta a variabilidade dos dados, gerando imagens diferentes a partir de uma √∫nica imagem de entrada.
       
        2. **RandomHorizontalFlip**: Realiza a invers√£o horizontal da imagem com 50% de probabilidade. Isso √© √∫til em cen√°rios onde a orienta√ß√£o horizontal da imagem n√£o altera seu significado, como em imagens de rochas ou melanomas.
    
        3. **RandomRotation(degrees=90)**: Rotaciona a imagem em at√© 90 graus, criando varia√ß√µes angulares, o que ajuda o modelo a reconhecer objetos independentemente da orienta√ß√£o.
    
        4. **ColorJitter**: Introduz varia√ß√µes de brilho, contraste, satura√ß√£o e matiz, simulando diferentes condi√ß√µes de ilumina√ß√£o e tornando o modelo mais robusto a mudan√ßas de ilumina√ß√£o.
    
        5. **RandomResizedCrop(224, scale=(0.8, 1.0))**: Realiza cortes aleat√≥rios na imagem e os redimensiona para 224x224 pixels, permitindo que diferentes partes da imagem sejam enfatizadas.
    
        6. **RandomAffine(degrees=0, shear=10)**: Aplica transforma√ß√µes afins, como cisalhamento, simulando distor√ß√µes que podem ocorrer no mundo real, como mudan√ßas de perspectiva.
    
        7. **Resize(256)**: Redimensiona a imagem para 256x256 pixels, assegurando que todas as imagens possuam a mesma dimens√£o.
    
        8. **CenterCrop(224)**: Recorta o centro da imagem, garantindo que o tamanho final seja 224x224 pixels.
    
        9. **ToTensor**: Converte a imagem para um tensor PyTorch, normalizando os valores dos pixels para o intervalo de [0,1], facilitando o processamento pelo modelo.
        """)
    
        st.write("### Gera√ß√£o de Imagens Sint√©ticas")
    
        st.write("""
        Essas transforma√ß√µes permitem que cada imagem original gere at√© **5 a 10 imagens sint√©ticas**. Por exemplo, em um conjunto de dados de 1000 imagens, 
        o processo pode expandir o conjunto para **5000 a 10000 imagens** ao longo do treinamento. Essa amplia√ß√£o artificial do conjunto de dados reduz o risco de **overfitting**, 
        permitindo que o modelo treine em um conjunto "maior" e mais diverso, o que √© crucial para melhorar a generaliza√ß√£o do modelo em dados novos.
        """)
    
        st.write("### Normaliza√ß√£o nas Imagens de Teste e Valida√ß√£o")
    
        st.write("""
        Nas imagens de **teste** e **valida√ß√£o**, o aumento de dados n√£o √© aplicado. O objetivo nesses conjuntos √© avaliar o modelo de maneira consistente, 
        utilizando imagens que representem o mais fielmente poss√≠vel os dados reais. No entanto, a normaliza√ß√£o dessas imagens √© fundamental para assegurar que seus valores de pixel 
        estejam adequados para as opera√ß√µes de aprendizado. Isso tamb√©m garante um desempenho est√°vel durante o treinamento.
        """)
    
        st.write("#### Transforma√ß√µes Aplicadas no Teste e Valida√ß√£o")
        
        st.write("""
        1. **Resize(256)**: Redimensiona a imagem para 256x256 pixels, garantindo que todas as imagens tenham o mesmo tamanho inicial.
    
        2. **CenterCrop(224)**: Realiza o corte central para que as dimens√µes da imagem sejam 224x224 pixels, correspondendo ao tamanho esperado pelo modelo.
    
        3. **ToTensor**: Converte a imagem para tensor e normaliza os valores dos pixels para o intervalo de [0,1], o que melhora a estabilidade num√©rica e a taxa de converg√™ncia do treinamento.
        """)
    
        st.write("### Import√¢ncia da Normaliza√ß√£o")
    
        st.write("""
        A **normaliza√ß√£o** garante que os valores dos pixels estejam em uma escala apropriada para as opera√ß√µes aritm√©ticas realizadas no modelo, melhorando a estabilidade e o desempenho do processo de treinamento. 
        Ela tamb√©m contribui para a estabilidade num√©rica durante o c√°lculo do gradiente e para uma converg√™ncia mais eficiente do modelo (Nguy·ªÖn et al., 2021).
        """)
    
        st.write("### Conclus√£o")
    
        st.write("""
        O c√≥digo exemplifica a implementa√ß√£o eficaz de transforma√ß√µes de dados e aumento de dados como parte da pipeline de treinamento de redes neurais profundas. 
        As transforma√ß√µes aplicadas aumentam a diversidade do conjunto de treinamento, ajudando a mitigar o **overfitting** e melhorar a generaliza√ß√£o do modelo. 
        Al√©m disso, a normaliza√ß√£o aplicada aos dados de teste e valida√ß√£o garante que o desempenho do modelo seja avaliado de forma precisa e consistente, 
        alinhada √†s melhores pr√°ticas de aprendizado profundo.
        """)
    
        st.write("### Refer√™ncias")
        
        st.write("""
        - Huang, G., Liu, Z., Maaten, L., & Weinberger, K. (2017). Densely connected convolutional networks. https://doi.org/10.1109/cvpr.2017.243
        - Li, S. (2023). Clouddensenet: lightweight ground-based cloud classification method for large-scale datasets based on reconstructed densenet. *Sensors*, 23(18), 7957. https://doi.org/10.3390/s23187957
        - Nguy·ªÖn, H., Yu, G., Shin, N., Kwon, G., Kwak, W., & Kim, J. (2021). Defective product classification system for smart factory based on deep learning. *Electronics*, 10(7), 826. https://doi.org/10.3390/electronics10070826
        - Shorten, C. & Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. *Journal of Big Data*, 6(1). https://doi.org/10.1186/s40537-019-0197-0
        """)

    # Barra Lateral de Configura√ß√µes
    st.sidebar.title("Configura√ß√µes do Treinamento")
      # Imagem e Contatos___________________________
    #_______________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmulas LaTeX
    with st.sidebar:
        with st.expander("Discuss√£o sobre o N√∫mero de Classes em Modelos de Aprendizado Profundo"):
            st.write("""
            ### Introdu√ß√£o
    
            A discuss√£o sobre o n√∫mero de classes em modelos de aprendizado profundo √© fundamental para a compreens√£o da arquitetura e do desempenho de redes neurais em tarefas de classifica√ß√£o. O n√∫mero de classes refere-se ao total de categorias ou r√≥tulos que um modelo deve prever, e a configura√ß√£o correta desse par√¢metro impacta diretamente o desempenho do modelo, pois afeta a dimens√£o da sa√≠da da rede neural e a complexidade da tarefa. O n√∫mero de classes pode variar de tarefas bin√°rias, que envolvem apenas duas classes, at√© problemas com centenas ou milhares de classes, como nas classifica√ß√µes de imagens do **ImageNet** (Cheng, 2023).
            """)
    
            st.write("### Impacto do N√∫mero de Classes")
            st.write("""
            O n√∫mero de classes define a estrutura da √∫ltima camada da rede neural, que √© respons√°vel por realizar as predi√ß√µes. Para um problema de **classifica√ß√£o bin√°ria**, o modelo ter√° uma √∫nica sa√≠da que prev√™ a probabilidade de uma classe ou outra. Em contrapartida, em um problema de **classifica√ß√£o multiclasse**, o n√∫mero de sa√≠das ser√° igual ao n√∫mero de categorias poss√≠veis (Cheng, 2023). A fun√ß√£o de ativa√ß√£o utilizada na √∫ltima camada √© crucial para a interpreta√ß√£o dos resultados. A equa√ß√£o que representa essa rela√ß√£o pode ser expressa como:
            """)
            st.latex(r'''
            \mathbf{y} = \text{Softmax}(Wx + b)
            ''')
    
            st.write("""
            onde **W** e **b** s√£o os pesos e o bias, respectivamente, que conectam a camada anterior √†s classes de sa√≠da. O resultado √© passado pela fun√ß√£o **softmax**, que converte os valores em probabilidades associadas a cada classe (Petrovska et al., 2020).
            """)
    
                       
            st.write("""
            Em tarefas de classifica√ß√£o bin√°ria, o modelo tem apenas duas classes poss√≠veis, como **detec√ß√£o de fraude** ou **diagn√≥stico de doen√ßas** (positivo ou negativo). Nesse caso, a fun√ß√£o de ativa√ß√£o final √© geralmente a **sigmoide**, que retorna uma probabilidade entre 0 e 1 para cada entrada. Um limiar √© ent√£o aplicado para decidir a classe final predita pelo modelo (Cheng, 2023).
            """)
    
            st.write("### Classifica√ß√£o Multiclasse")
            st.write("""
            Em problemas de classifica√ß√£o multiclasse, o n√∫mero de classes pode variar consideravelmente. Por exemplo, em tarefas de **classifica√ß√£o de imagens geol√≥gicas**, o n√∫mero de classes pode ser pequeno, mas em aplica√ß√µes como a **classifica√ß√£o de imagens m√©dicas** ou **reconhecimento facial**, o n√∫mero de classes pode ser muito maior. A arquitetura da rede deve ser ajustada para garantir que a √∫ltima camada tenha o n√∫mero correto de sa√≠das correspondente ao n√∫mero de categorias (Cheng, 2023; Sardeshmukh, 2023).
            """)
    
            st.write("### Classifica√ß√£o Multirr√≥tulo")
            st.write("""
            Em problemas de **classifica√ß√£o multirr√≥tulo**, uma entrada pode pertencer a mais de uma classe ao mesmo tempo. Nesse cen√°rio, o n√∫mero de sa√≠das da rede neural √© igual ao n√∫mero de classes poss√≠veis, mas cada sa√≠da √© independente das demais. A fun√ß√£o de ativa√ß√£o usada √© a **sigmoide**, pois ela calcula a probabilidade de cada classe independentemente das outras (Petrovska et al., 2020).
            """)
    
            st.write("### Efeitos do N√∫mero de Classes no Desempenho")
            st.write("""
            O n√∫mero de classes influencia diretamente a complexidade do modelo e o tempo de treinamento. Conforme o n√∫mero de classes aumenta, a tarefa de classifica√ß√£o se torna mais dif√≠cil, exigindo mais par√¢metros e tempo de computa√ß√£o. Al√©m disso, um maior n√∫mero de classes aumenta o risco de **sobreajuste** (overfitting), especialmente em conjuntos de dados pequenos (Cheng, 2023; Suhana, 2022).
            """)
    
            st.write("### Conclus√£o")
            st.write("""
            O n√∫mero de classes √© um fator determinante na defini√ß√£o da arquitetura de redes neurais para tarefas de classifica√ß√£o. Seja em problemas bin√°rios, multiclasse ou multirr√≥tulo, a escolha adequada desse par√¢metro garante que a rede neural seja capaz de aprender as caracter√≠sticas relevantes de cada categoria. Em problemas com muitas classes, estrat√©gias como a **regulariza√ß√£o** e o **data augmentation** podem ser utilizadas para melhorar o desempenho do modelo, evitando o sobreajuste (Cheng, 2023; Sardeshmukh, 2023).
            """)
    
            st.write("### Refer√™ncias")
          
            st.write("""
            1. Cheng, R. (2023). Expansion of the CT-scans image set based on the pretrained DCGAN for improving the performance of the CNN. *Journal of Physics Conference Series*, 2646(1), 012015. https://doi.org/10.1088/1742-6596/2646/1/012015
            2. Petrovska, B., Atanasova-Pacemska, T., Corizzo, R., Mignone, P., Lameski, P., & Zdravevski, E. (2020). Aerial Scene Classification through Fine-Tuning with Adaptive Learning Rates and Label Smoothing. *Applied Sciences*, 10(17), 5792. https://doi.org/10.3390/app10175792
            3. Sardeshmukh, M. (2023). Crop image classification using convolutional neural network. *Multidisciplinary Science Journal*, 5(4), 2023039. https://doi.org/10.31893/multiscience.2023039
            4. Suhana, R. (2022). Fish Image Classification Using Adaptive Learning Rate In Transfer Learning Method. *Knowledge Engineering and Data Science*, 5(1), 67-77. https://doi.org/10.17977/um018v5i12022p67-77
            """)

  
    # Nota: O n√∫mero de classes ser√° detectado automaticamente do dataset
    num_classes = st.sidebar.number_input("N√∫mero de Classes (ser√° detectado automaticamente):", min_value=1, step=1, value=2, disabled=True, help="Este valor ser√° automaticamente detectado do dataset ap√≥s o upload")
    #_______________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmula LaTeX
    with st.sidebar:
        with st.expander("Modelos Pr√©-Treinados: ResNet18, ResNet50 e DenseNet121:"):
            st.write("""
            ### Introdu√ß√£o
        
            As redes neurais convolucionais (CNNs) t√™m se tornado uma ferramenta essencial no campo do aprendizado profundo, especialmente em tarefas de vis√£o computacional, como a classifica√ß√£o de imagens. 
            Modelos como **ResNet18**, **ResNet50** e **DenseNet121** s√£o amplamente reconhecidos por seu desempenho superior em competi√ß√µes de classifica√ß√£o de imagens, como o **ImageNet**. Esses modelos s√£o considerados 
            **pr√©-treinados**, pois foram inicialmente treinados em grandes conjuntos de dados, permitindo que sejam reutilizados e ajustados para novas tarefas espec√≠ficas, uma pr√°tica conhecida como **transfer√™ncia de aprendizado** 
            (Cheng, 2023; Petrovska et al., 2020; Alaoui, 2023).
            """)
        
            st.write("### ResNet18 e ResNet50")
            st.write("""
            A arquitetura **ResNet** (Rede Residual) foi desenvolvida para mitigar o problema de **degrada√ß√£o** que ocorre em redes neurais muito profundas, onde o aumento do n√∫mero de camadas pode levar a uma diminui√ß√£o no desempenho.
            A inova√ß√£o dos **blocos residuais** permite que algumas camadas "saltem" conex√µes, aprendendo uma **fun√ß√£o de identidade** em vez de novas representa√ß√µes para cada camada. Essa abordagem facilita o treinamento de redes mais profundas, pois a fun√ß√£o residual pode ser aprendida de forma mais eficiente (Zhang et al., 2018; Sandotra et al., 2023; Petrovska et al., 2020).
            """)
            
            st.latex(r'''
            \mathbf{y} = \mathcal{F}(x, \{W_i\}) + x
            ''')
            
            st.write("""
            onde 
            """)
            st.latex(r'''
            \mathcal{F}(x, \{W_i\}) + x
            ''')
          
            st.write("""
            representa a fun√ß√£o aprendida e x √© a entrada. O termo x √© adicionado √† sa√≠da, o que simplifica o processo de treinamento e permite que redes mais profundas sejam treinadas com maior efic√°cia 
            ("A Framework for Flood Extent Mapping using CNN Transfer Learning", 2022; Petrovska et al., 2020).
            """)
        
            st.write("""
            O modelo **ResNet18** possui 18 camadas trein√°veis e √© uma vers√£o mais leve, adequada para aplica√ß√µes com restri√ß√µes de recursos computacionais, enquanto o **ResNet50**, com 50 camadas, √© capaz de capturar padr√µes mais complexos em imagens, sendo ideal para tarefas que exigem maior profundidade de an√°lise (Sandotra et al., 2023; Qin et al., 2019; Petrovska et al., 2020).
            """)
        
            st.write("""
            Ambos os modelos foram pr√©-treinados no conjunto de dados **ImageNet**, o que facilita a **transfer√™ncia de aprendizado** em novos dom√≠nios. As camadas iniciais desses modelos j√° s√£o capazes de identificar caracter√≠sticas gerais, acelerando o processo de treinamento em conjuntos de dados menores e espec√≠ficos, como em aplica√ß√µes m√©dicas ou de classifica√ß√£o de imagens geol√≥gicas (Cheng, 2023; Petrovska et al., 2020; Alaoui, 2023).
            """)
        
            st.write("### DenseNet121")
            st.write("""
            A arquitetura **DenseNet** (Rede Convolucional Densamente Conectada) oferece uma abordagem alternativa, onde todas as camadas est√£o interconectadas, promovendo a preserva√ß√£o do fluxo de gradiente e da informa√ß√£o original. Isso facilita a reutiliza√ß√£o das representa√ß√µes intermedi√°rias e otimiza a efici√™ncia do modelo. A equa√ß√£o que expressa essa estrutura √©:
            """)
        
            st.latex(r'''
            \mathbf{x}_l = H_l(\mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_{l-1})
            ''')
        
            st.write("""
            onde
            """)
          
            st.latex(r'''
            \mathbf{x}_l 
            ''')
          
            st.write("""
            √© a sa√≠da da l-√©sima camada e 
            """)
          
            st.latex(r'''
             \mathbf{H}_l
            ''')
          
            st.write("""
            √© a fun√ß√£o aplicada. Essa configura√ß√£o otimiza o uso de gradientes e representa√ß√µes, resultando em um desempenho superior em tarefas de classifica√ß√£o 
            (Benegui & Ionescu, 2020; Varshni et al., 2019; Hamdaoui et al., 2021).
            """)
        
            st.write("""
            O modelo **DenseNet121**, que possui 121 camadas trein√°veis, √© particularmente eficaz em contextos onde a efici√™ncia √© crucial, maximizando o uso de recursos computacionais e facilitando a extra√ß√£o de caracter√≠sticas relevantes de imagens (Sardeshmukh, 2023; Hamdaoui et al., 2021).
            """)
        
            st.write("### Transfer√™ncia de Aprendizado e Ajuste Fino")
            st.write("""
            A utiliza√ß√£o de modelos pr√©-treinados, como ResNet18, ResNet50 e DenseNet121, √© uma t√©cnica de **transfer√™ncia de aprendizado** que permite que o conhecimento adquirido em tarefas anteriores seja aplicado a novos problemas. 
            Em vez de treinar um modelo do zero, o ajuste fino √© realizado nas camadas do modelo para se adaptar a um novo conjunto de dados, permitindo que caracter√≠sticas espec√≠ficas sejam aprendidas de forma mais eficiente. Por exemplo, em aplica√ß√µes de **classifica√ß√£o de melanomas** ou **an√°lise de rochas vulc√¢nicas**, as camadas mais profundas dos modelos s√£o ajustadas para entender caracter√≠sticas espec√≠ficas de imagens m√©dicas ou geol√≥gicas (Suhana, 2022; Petrovska et al., 2020).
            """)
        
            st.write("""
            Estudos demonstram que a transfer√™ncia de aprendizado √© especialmente eficaz ao se trabalhar com conjuntos de dados pequenos. O uso de modelos pr√©-treinados pode proporcionar resultados semelhantes ou at√© superiores aos de modelos treinados a partir do zero, reduzindo o tempo de treinamento e melhorando a precis√£o (Raghava et al., 2019; Alaoui, 2023; Ahmed, 2021).
            """)
        
            st.write("### Conclus√£o")
            st.write("""
            As arquiteturas **ResNet18**, **ResNet50** e **DenseNet121** s√£o ferramentas poderosas no campo do aprendizado profundo, especialmente em tarefas de classifica√ß√£o de imagens. Seu pr√©-treinamento em grandes conjuntos de dados, como o **ImageNet**, e a capacidade de serem ajustados para novas tarefas atrav√©s da transfer√™ncia de aprendizado, tornam esses modelos ideais para uma ampla gama de aplica√ß√µes, incluindo a classifica√ß√£o de imagens m√©dicas e geol√≥gicas. O uso dessas arquiteturas n√£o apenas reduz o tempo de treinamento, mas tamb√©m melhora a precis√£o e a efic√°cia em diversas √°reas de pesquisa e aplica√ß√£o pr√°tica (Zeimarani et al., 2020; "Dog Breed Identification with Fine Tuning of Pre-trained Models", 2019; Awais et al., 2020).
            """)
        
            st.write("### Refer√™ncias")
        
            st.write("""
            - (2019). Dog breed identification with fine tuning of pre-trained models. *International Journal of Recent Technology and Engineering*, 8(2S11), 3677-3680. https://doi.org/10.35940/ijrte.b1464.0982s1119
            - (2022). A framework for flood extent mapping using cnn transfer learning. https://doi.org/10.17762/ijisae.v10i3s.2426
            - Ahmed, A. (2021). Pre-trained cnns models for content based image retrieval. *International Journal of Advanced Computer Science and Applications*, 12(7). https://doi.org/10.14569/ijacsa.2021.0120723
            - Alaoui, A. (2023). Pre-trained cnns: evaluating emergency vehicle image classification. *Data & Metadata*, 2, 153. https://doi.org/10.56294/dm2023153
            - Benegui, C. and Ionescu, R. (2020). Convolutional neural networks for user identification based on motion sensors represented as images. *IEEE Access*, 8, 61255-61266. https://doi.org/10.1109/access.2020.2984214
            - Cheng, R. (2023). Expansion of the ct-scans image set based on the pretrained dcgan for improving the performance of the cnn. *Journal of Physics Conference Series*, 2646(1), 012015. https://doi.org/10.1088/1742-6596/2646/1/012015
            - Hamdaoui, H., Ben-fares, A., Boujraf, S., Chaoui, N., Alami, B., Ma√¢roufi, M., ‚Ä¶ & Qjidaa, H. (2021). High precision brain tumor classification model based on deep transfer learning and stacking concepts. *Indonesian Journal of Electrical Engineering and Computer Science*, 24(1), 167. https://doi.org/10.11591/ijeecs.v24.i1.pp167-177
            - Petrovska, B., Atanasova-Pacemska, T., Corizzo, R., Mignone, P., Lameski, P., & Zdravevski, E. (2020). Aerial scene classification through fine-tuning with adaptive learning rates and label smoothing. *Applied Sciences*, 10(17), 5792. https://doi.org/10.3390/app10175792
            - Raghava, Y., Kuthadi, V., & Rajalakshmi, S. (2019). Enhanced deep learning with featured transfer learning in identifying disguised faces. *International Journal of Innovative Technology and Exploring Engineering*, 8(10), 1257-1260. https://doi.org/10.35940/ijitee.h7286.0881019
            - Sandotra, N., Mahajan, P., Abrol, P., & Lehana, P. (2023). Analyzing performance of deep learning models under the presence of distortions in identifying plant leaf disease. *International Journal of Informatics and Communication Technology (IJ-ICT)*, 12(2), 115. https://doi.org/10.11591/ijict.v12i2.pp115-126
            - Sardeshmukh, M. (2023). Crop image classification using convolutional neural network. *Multidisciplinary Science Journal*, 5(4), 2023039. https://doi.org/10.31893/multiscience.2023039
            - Suhana, R. (2022). Fish image classification using adaptive learning rate in transfer learning method. *Knowledge Engineering and Data Science*, 5(1), 67. https://doi.org/10.17977/um018v5i12022p67-77
            - Varshni, D., Thakral, K., Agarwal, L., Nijhawan, R., & Mittal, A. (2019). Pneumonia detection using cnn based feature extraction. https://doi.org/10.1109/icecct.2019.8869364
            - Zeimarani, B., Costa, M., Nurani, N., Bianco, S., Pereira, W., & Filho, C. (2020). Breast lesion classification in ultrasound images using deep convolutional neural network. *IEEE Access*, 8, 133349-133359. https://doi.org/10.1109/access.2020.3010863
            - Zhang, B., Wang, C., Shen, Y., & Liu, Y. (2018). Fully connected conditional random fields for high-resolution remote sensing land use/land cover classification with convolutional neural networks. *Remote Sensing*, 10(12), 1889. https://doi.org/10.3390/rs10121889
            """)

    # Sele√ß√£o de Tipo de Arquitetura
    st.sidebar.write("---")
    st.sidebar.subheader("üèóÔ∏è Arquitetura do Modelo")
    
    architecture_type = st.sidebar.radio(
        "Tipo de Arquitetura:",
        options=["CNN (Convolucional)", "Transformer (ViT)"],
        help="CNN: Redes Neurais Convolucionais tradicionais | Transformer: Vision Transformers modernos"
    )
    
    if architecture_type == "CNN (Convolucional)":
        model_options = ['ResNet18', 'ResNet50', 'DenseNet121']
        st.sidebar.info("üî∑ **CNNs** s√£o excelentes para capturar padr√µes locais e hier√°rquicos em imagens atrav√©s de convolu√ß√µes.")
    else:
        model_options = ['ViT-B/16', 'ViT-B/32', 'ViT-L/16']
        st.sidebar.info("üî∂ **Vision Transformers** usam mecanismos de aten√ß√£o para capturar rela√ß√µes globais na imagem. Requerem mais dados mas podem ter melhor desempenho.")
        st.sidebar.warning("‚ö†Ô∏è ViT requer mais mem√≥ria GPU. Use batch size menor se necess√°rio.")
    
    model_name = st.sidebar.selectbox("Modelo Pr√©-treinado:", options=model_options)
    
    # Explica√ß√£o sobre o modelo selecionado
    with st.sidebar.expander(f"‚ÑπÔ∏è Sobre {model_name}"):
        if model_name == 'ResNet18':
            st.write("**ResNet18:** 18 camadas, ~11M par√¢metros. R√°pido e eficiente para datasets menores.")
        elif model_name == 'ResNet50':
            st.write("**ResNet50:** 50 camadas, ~25M par√¢metros. Melhor precis√£o, requer mais recursos.")
        elif model_name == 'DenseNet121':
            st.write("**DenseNet121:** Conex√µes densas entre camadas, ~8M par√¢metros. Eficiente e preciso.")
        elif model_name == 'ViT-B/16':
            st.write("**ViT-B/16:** Base model, patches 16x16, ~86M par√¢metros. Melhor performance geral.")
        elif model_name == 'ViT-B/32':
            st.write("**ViT-B/32:** Base model, patches 32x32, ~88M par√¢metros. Mais r√°pido, menos preciso.")
        elif model_name == 'ViT-L/16':
            st.write("**ViT-L/16:** Large model, patches 16x16, ~307M par√¢metros. M√°xima precis√£o, requer muitos recursos.")

    #________________________________________________________________________________________
    # Fine-Tuning Completo em Redes Neurais Profundas
    with st.sidebar:
        with st.expander("Fine-Tuning Completo em Redes Neurais Profundas:"):
            st.write("""
            ### Introdu√ß√£o
        
            O **fine-tuning** (ajuste fino) √© uma t√©cnica poderosa utilizada para ajustar redes neurais pr√©-treinadas em novos conjuntos de dados. No contexto de redes como a **ResNet18**, **ResNet50** ou **DenseNet121**, que foram inicialmente treinadas em grandes bases de dados (como o **ImageNet**), o fine-tuning permite que essas redes sejam adaptadas a novos problemas, como a **classifica√ß√£o de melanomas** ou de **rochas vulc√¢nicas e plut√¥nicas**. Ao realizar o fine-tuning, todas as camadas do modelo s√£o atualizadas para refletir as caracter√≠sticas do novo conjunto de dados, ao inv√©s de congelar as camadas iniciais, o que permite uma adapta√ß√£o mais profunda e precisa ao novo problema (Piotrowski & Napiorkowski, 2013; Friedrich et al., 2022).
            """)
        
            st.write("""
            ### Fundamenta√ß√£o Te√≥rica
        
            O conceito de fine-tuning √© baseado no princ√≠pio de **transfer√™ncia de aprendizado**, no qual um modelo pr√©-treinado em um grande conjunto de dados gen√©ricos √© reaproveitado para um novo problema espec√≠fico. Essa abordagem √© particularmente √∫til quando o novo conjunto de dados √© relativamente pequeno, pois o modelo j√° foi treinado para capturar padr√µes gerais em dados visuais (como bordas, texturas e formas), o que pode acelerar o treinamento e melhorar a precis√£o final (Al‚Äêrimy et al., 2023; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            Ao utilizar o fine-tuning completo, todas as camadas do modelo s√£o ajustadas com base nos novos dados. Isso significa que os pesos das camadas profundas do modelo, que foram aprendidos durante o treinamento inicial, s√£o atualizados para se adequar √†s caracter√≠sticas espec√≠ficas do novo conjunto de dados. Matematicamente, essa abordagem pode ser descrita como a otimiza√ß√£o da seguinte fun√ß√£o de perda:
            """)
        
            st.latex(r'''
            L_{\text{fine-tuning}} = L_{\text{original}} + \lambda \sum_{i} w_i^2
            ''')
        
            st.write("""
            Onde:
            """)
          
            st.latex(r'''
            L_{\text{fine-tuning}}
            ''')
          
            st.write("""
            √© a fun√ß√£o de perda durante o fine-tuning;
            """)
          
            st.latex(r'''
            L_{\text{original}}
            ''')
          
            st.write("""
            representa a fun√ß√£o de perda original do modelo pr√©-treinado;
            """)
          
            st.latex(r'''
            \lambda
            ''')
          
            st.write("""
            √© o coeficiente de regulariza√ß√£o (no caso de utilizar a regulariza√ß√£o L2);
            """)
          
            st.latex(r'''
            w_i
            ''')
            st.write("""
            s√£o os pesos individuais que ser√£o atualizados durante o processo de fine-tuning (Friedrich et al., 2022; Al‚Äêrimy et al., 2023).
            """)
        
            st.write("""
            ### Benef√≠cios do Fine-Tuning Completo
        
            O fine-tuning completo oferece v√°rios benef√≠cios, especialmente quando o novo conjunto de dados difere substancialmente do conjunto no qual o modelo foi originalmente treinado. No caso da **classifica√ß√£o de melanomas** ou **rochas**, por exemplo, as caracter√≠sticas visuais dos dados podem ser muito diferentes das imagens do **ImageNet**, que incluem uma ampla variedade de objetos, animais e cen√°rios (Piotrowski & Napiorkowski, 2013; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            Os principais benef√≠cios incluem:
            1. **Adapta√ß√£o Profunda**: Ao ajustar todas as camadas, o modelo consegue adaptar n√£o apenas as caracter√≠sticas gen√©ricas (como bordas e texturas), mas tamb√©m padr√µes mais complexos e espec√≠ficos do novo problema.
            2. **Melhoria da Precis√£o**: O fine-tuning completo geralmente resulta em melhorias significativas na precis√£o, especialmente quando os dados de treinamento s√£o limitados ou possuem caracter√≠sticas visuais √∫nicas (Friedrich et al., 2022; Al‚Äêrimy et al., 2023).
            3. **Generaliza√ß√£o Melhorada**: O processo de fine-tuning permite que o modelo generalize melhor para novos dados, uma vez que ele √© treinado para capturar padr√µes mais espec√≠ficos do novo dom√≠nio (Piotrowski & Napiorkowski, 2013; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            ### Compara√ß√£o com o Fine-Tuning Parcial
        
            Em contraste com o fine-tuning completo, no qual todas as camadas s√£o atualizadas, o **fine-tuning parcial** mant√©m algumas das camadas iniciais congeladas, atualizando apenas as camadas finais. Essa abordagem pode ser √∫til quando o novo conjunto de dados √© semelhante ao conjunto de dados original no qual o modelo foi treinado. No entanto, quando os dados diferem substancialmente, o fine-tuning completo tende a ser mais eficaz, pois permite uma adapta√ß√£o mais profunda e personalizada (Al‚Äêrimy et al., 2023; Sakizadeh et al., 2015).
            """)
        
            st.write("""
            ### Efeitos do Fine-Tuning em Problemas Espec√≠ficos
        
            #### Classifica√ß√£o de Melanomas
        
            No caso da **classifica√ß√£o de melanomas**, o fine-tuning completo permite que o modelo identifique padr√µes visuais sutis na pele que podem ser indicativos de c√¢ncer. Essas caracter√≠sticas visuais podem incluir varia√ß√µes de textura, cor e bordas, que s√£o espec√≠ficas de imagens m√©dicas e diferem dos objetos comuns presentes em bases de dados gen√©ricas, como o **ImageNet** (Piotrowski & Napiorkowski, 2013; Friedrich et al., 2022).
            """)
        
            st.write("""
            #### Classifica√ß√£o de Rochas
        
            Para a **classifica√ß√£o de rochas vulc√¢nicas e plut√¥nicas**, o fine-tuning completo permite que o modelo capture padr√µes geol√≥gicos e estruturais espec√≠ficos, como varia√ß√µes de granula√ß√£o e texturas minerais. Novamente, esses padr√µes s√£o significativamente diferentes dos dados de objetos comuns, tornando o fine-tuning completo uma abordagem valiosa para melhorar a precis√£o da classifica√ß√£o (Friedrich et al., 2022; Al‚Äêrimy et al., 2023).
            """)
        
            st.write("""
            ### Considera√ß√µes Pr√°ticas
        
            Durante o processo de fine-tuning, √© importante monitorar o desempenho do modelo em um conjunto de valida√ß√£o para evitar o **overfitting**. Uma t√©cnica comum √© utilizar a **regulariza√ß√£o L2** ou o **dropout** para garantir que o modelo n√£o se ajuste excessivamente aos dados de treinamento (Piotrowski & Napiorkowski, 2013; Sakizadeh et al., 2015). Al√©m disso, a taxa de aprendizado deve ser cuidadosamente ajustada. Em muitos casos, utiliza-se uma taxa de aprendizado menor durante o fine-tuning para garantir que as atualiza√ß√µes dos pesos n√£o sejam muito dr√°sticas, preservando parte das informa√ß√µes aprendidas anteriormente.
            """)
        
            st.write("""
            ### Conclus√£o
        
            O fine-tuning completo √© uma t√©cnica eficaz para ajustar modelos pr√©-treinados, como a **ResNet18**, **ResNet50** ou **DenseNet121**, a novos conjuntos de dados. Ao permitir que todas as camadas do modelo sejam atualizadas, o fine-tuning completo oferece maior flexibilidade e precis√£o em problemas que diferem substancialmente dos dados originais. Quando combinado com outras t√©cnicas de regulariza√ß√£o, como a L2, o fine-tuning pode levar a modelos robustos e capazes de generalizar para novos dados, sendo uma ferramenta essencial no arsenal de t√©cnicas de aprendizado profundo.
            """)
        
            st.write("""
            ### Refer√™ncias
        
            - Al‚ÄêRIMY, B.; SAEED, F.; AL-SAREM, M.; ALBARRAK, A.; QASEM, S. An adaptive early stopping technique for densenet169-based knee osteoarthritis detection model. *Diagnostics*, 13(11), 1903, 2023. https://doi.org/10.3390/diagnostics13111903
            - FRIEDRICH, S. et al. Regularization approaches in clinical biostatistics: a review of methods and their applications. *Statistical Methods in Medical Research*, 32(2), 425-440, 2022. https://doi.org/10.1177/09622802221133557
            - PIOTROWSKI, A.; NAPIORKOWSKI, J. A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling. *Journal of Hydrology*, 476, 97-111, 2013. https://doi.org/10.1016/j.jhydrol.2012.10.019
            - REZAEEZADE, A.; BATINA, L. Regularizers to the rescue: fighting overfitting in deeplearning-based side-channel analysis. 2022. https://doi.org/10.21203/rs.3.rs-2386625/v1
            - SAKIZADEH, M.; MALIAN, A.; AHMADPOUR, E. Groundwater quality modeling with a small data set. *Ground Water*, 54(1), 115-120, 2015. https://doi.org/10.1111/gwat.12317
            """)

    fine_tune = st.sidebar.checkbox("Fine-Tuning Completo", value=False)
    epochs = st.sidebar.slider("N√∫mero de √âpocas:", min_value=1, max_value=500, value=200, step=1)
    learning_rate = st.sidebar.select_slider("Taxa de Aprendizagem:", options=[0.1, 0.01, 0.001, 0.0001], value=0.0001)
    batch_size = st.sidebar.selectbox("Tamanho de Lote:", options=[4, 8, 16, 32, 64], index=2)
    train_split = st.sidebar.slider("Percentual de Treinamento:", min_value=0.5, max_value=0.9, value=0.7, step=0.05)
    valid_split = st.sidebar.slider("Percentual de Valida√ß√£o:", min_value=0.05, max_value=0.4, value=0.15, step=0.05)
    #________________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmula LaTeX
    with st.sidebar:
        with st.expander("Implementa√ß√£o da T√©cnica de Regulariza√ß√£o L2 (Weight Decay):"):
            st.write("""
            ### Introdu√ß√£o
            A regulariza√ß√£o L2, frequentemente referida como *weight decay*, √© uma t√©cnica amplamente utilizada para mitigar o **overfitting** 
            em modelos de aprendizado de m√°quina, especialmente em redes neurais profundas. O *overfitting* ocorre quando o modelo se ajusta n√£o apenas 
            aos padr√µes dos dados de treinamento, mas tamb√©m ao ru√≠do presente, o que compromete sua capacidade de generaliza√ß√£o para novos dados 
            (Piotrowski & Napiorkowski, 2013). A regulariza√ß√£o L2 adiciona um termo de penaliza√ß√£o √† fun√ß√£o de perda do modelo, o que resulta em uma 
            redu√ß√£o dos valores absolutos dos pesos, promovendo, assim, modelos mais simples e generaliz√°veis (Friedrich et al., 2022).
            Esta revis√£o visa fornecer uma vis√£o clara e t√©cnica da aplica√ß√£o da regulariza√ß√£o L2, discutindo seus efeitos, a interpreta√ß√£o do coeficiente de regulariza√ß√£o 
            """)
          
            st.latex(r'''
            \lambda
            ''')
          
            st.write("""
            e as implica√ß√µes da escolha desse par√¢metro.
            """)
          
            st.latex(r'''
            L_{\text{total}} = L_{\text{original}} + \lambda \sum_{i} w_i^2
            ''')
          
            st.write("""
            Onde:
            """) 
            
            st.latex(r'''
            L_{\text{total}}
            ''')
          
            st.write("""
            √© a perda total que o modelo busca minimizar;
            """)
            
            st.latex(r'''
            L_{\text{original}}
            ''')
          
            st.write("""
            √© a fun√ß√£o de perda original (como a perda de entropia cruzada); Œª √© o coeficiente de regulariza√ß√£o, que controla a penalidade aplicada aos pesos;
            """)
          
            st.latex(r'''
            w_i
            ''')
          
            st.write(""" 
            s√£o os pesos individuais do modelo (Al‚ÄêRimy et al., 2023).
            """)
          
            st.write("""
            Este termo adicional penaliza pesos grandes, for√ßando o modelo a priorizar solu√ß√µes que utilizam pesos menores, o que √© crucial para evitar 
            que o modelo memorize os dados de treinamento, promovendo maior capacidade de generaliza√ß√£o (Sakizadeh et al., 2015).
            """)
          
            st.write("""
            ### Fundamenta√ß√£o Te√≥rica
            A regulariza√ß√£o L2 tem uma base te√≥rica s√≥lida, sendo amplamente aplicada para controlar a complexidade do modelo. Ao adicionar o termo de penaliza√ß√£o, 
            a regulariza√ß√£o L2 ajuda a evitar o overfitting e melhora a estabilidade num√©rica do modelo (Friedrich et al., 2022). Isso √© particularmente importante 
            em redes neurais profundas, onde o n√∫mero de par√¢metros pode ser grande e a complexidade do modelo alta.
            """)
          
            st.write("""
            ### Efeitos da Regulariza√ß√£o L2
            A regulariza√ß√£o L2 controla a complexidade do modelo ao penalizar pesos grandes, o que √© particularmente √∫til em cen√°rios com muitos par√¢metros 
            ou dados ruidosos (Piotrowski & Napiorkowski, 2013). Al√©m de reduzir o overfitting, a L2 promove a estabilidade no treinamento, melhorando a consist√™ncia do desempenho 
            em dados de teste (Friedrich et al., 2022).
            """)
    
            st.write("""
            ### Interpreta√ß√£o e Efeitos Pr√°ticos de Œª
            """)
          
            st.write("""        
            A escolha do valor de Œª
            """)
      
            st.write("""
            influencia diretamente o comportamento do modelo:
            """)
    
            st.write("""
            #### Œª = 0
            """)
            st.write("""
            Quando Œª = 0, a regulariza√ß√£o L2 est√° desativada. Isso permite que o modelo ajuste-se livremente aos dados de treinamento, 
            aumentando o risco de overfitting, especialmente em conjuntos de dados pequenos ou ruidosos (Friedrich et al., 2022).
            """)
    
            st.write("""
            #### Œª = 0,01
            """)
            st.write("""
            Este √© um valor moderado, que penaliza de forma equilibrada os pesos do modelo. Essa configura√ß√£o ajuda a evitar o overfitting sem comprometer a capacidade do modelo de 
            aprender padr√µes relevantes (Al‚ÄêRimy et al., 2023).
            """)
    
            st.write("""
            #### Œª = 0,02 ou Œª = 0,03
            Esses valores aumentam a intensidade da penaliza√ß√£o, sendo √∫teis em cen√°rios com dados ruidosos ou em que o n√∫mero de par√¢metros √© alto em rela√ß√£o √† quantidade de dados 
            dispon√≠veis (Piotrowski & Napiorkowski, 2013). Contudo, deve-se monitorar o desempenho do modelo, pois valores elevados de Œª podem resultar em **underfitting**, 
            comprometendo a capacidade do modelo de capturar padr√µes complexos (Friedrich et al., 2022).
            """)
    
            st.write("""
            ### Conclus√£o
            A regulariza√ß√£o L2 √© uma t√©cnica poderosa no treinamento de redes neurais profundas, ajudando a mitigar o overfitting e a melhorar a capacidade de generaliza√ß√£o do modelo. 
            Ao penalizar pesos grandes, a L2 incentiva solu√ß√µes mais simples e robustas. No entanto, a escolha do valor de Œª √© crucial para garantir que o modelo consiga capturar 
            padr√µes complexos sem se ajustar excessivamente aos dados de treinamento.
            """)
    
            st.write("""
            ### Refer√™ncias
            - AL‚ÄêRIMY, B.; SAEED, F.; AL-SAREM, M.; ALBARRAK, A.; QASEM, S. An adaptive early stopping technique for densenet169-based knee osteoarthritis detection model. *Diagnostics*, 13(11), 1903, 2023. https://doi.org/10.3390/diagnostics13111903
            - FRIEDRICH, S. et al. Regularization approaches in clinical biostatistics: a review of methods and their applications. *Statistical Methods in Medical Research*, 32(2), 425-440, 2022. https://doi.org/10.1177/09622802221133557
            - PIOTROWSKI, A.; NAPIORKOWSKI, J. A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling. *Journal of Hydrology*, 476, 97-111, 2013. https://doi.org/10.1016/j.jhydrol.2012.10.019
            - SAKIZADEH, M.; MALIAN, A.; AHMADPOUR, E. Groundwater quality modeling with a small data set. *Ground Water*, 54(1), 115-120, 2015. https://doi.org/10.1111/gwat.12317
            """)
    

  
    l2_lambda = st.sidebar.number_input("L2 Regularization (Weight Decay):", min_value=0.0, max_value=0.1, value=0.01, step=0.01)
    l1_lambda = st.sidebar.number_input("L1 Regularization:", min_value=0.0, max_value=0.01, value=0.0, step=0.001, 
                                        help="Adiciona regulariza√ß√£o L1 (Lasso) ao treinamento. Promove esparsidade nos pesos.")
    
    #________________________________________________________________________________________
    # Novos par√¢metros de treinamento
    st.sidebar.write("---")
    st.sidebar.subheader("‚öôÔ∏è Configura√ß√µes Avan√ßadas")
    
    # Tipo de Aumento de Dados
    augmentation_type = st.sidebar.selectbox(
        "T√©cnica de Aumento de Dados:",
        options=['none', 'standard', 'mixup', 'cutmix'],
        index=1,
        help="None: Sem aumento | Standard: Transforma√ß√µes b√°sicas | Mixup: Mistura imagens | Cutmix: Recorta e cola regi√µes"
    )
    
    # Otimizador
    optimizer_options = ['Adam', 'AdamW', 'SGD']
    if ADVANCED_OPTIMIZERS_AVAILABLE:
        optimizer_options.extend(['Ranger', 'Lion'])
    
    optimizer_name = st.sidebar.selectbox(
        "Otimizador:",
        options=optimizer_options,
        index=0,
        help="Adam: Adaptativo padr√£o | AdamW: Adam com weight decay melhorado | SGD: Gradiente descendente com momento | Ranger: Lookahead + RAdam | Lion: Otimizador eficiente recente"
    )
    
    # Learning Rate Scheduler
    scheduler_name = st.sidebar.selectbox(
        "Agendador de Learning Rate:",
        options=['None', 'CosineAnnealingLR', 'OneCycleLR'],
        index=0,
        help="None: LR constante | CosineAnnealingLR: Reduz LR com coseno | OneCycleLR: Aumenta e depois reduz LR"
    )
    
    # Tipo de Grad-CAM
    gradcam_type = st.sidebar.selectbox(
        "Tipo de Grad-CAM:",
        options=['GradCAM', 'GradCAMpp', 'SmoothGradCAMpp', 'LayerCAM'],
        index=2,
        help="GradCAM: B√°sico | GradCAMpp: Melhorado | SmoothGradCAMpp: Suavizado | LayerCAM: Por camada"
    )
    
    st.sidebar.write("---")
    
    #________________________________________________________________________________________
    # Sidebar com o conte√∫do explicativo e f√≥rmula LaTeX
    with st.sidebar:
        with st.expander("Implementa√ß√£o da T√©cnica de Parada Precoce - Early Stopping:"):
            st.write("""
            #### Introdu√ß√£o
            A t√©cnica de **parada precoce** (ou *early stopping*) √© amplamente utilizada para mitigar o **overfitting** no treinamento de redes neurais profundas. 
            O overfitting ocorre quando o modelo se ajusta t√£o bem aos dados de treinamento que sua capacidade de generaliza√ß√£o para novos dados √© prejudicada. 
            O princ√≠pio da parada precoce √© interromper o treinamento quando o desempenho do modelo em um conjunto de valida√ß√£o n√£o apresenta melhorias significativas 
            ap√≥s um n√∫mero predefinido de √©pocas. Essa abordagem baseia-se na observa√ß√£o de que, ap√≥s certo ponto, melhorias no desempenho do modelo em dados de treinamento 
            n√£o resultam em melhorias em dados que o modelo ainda n√£o viu (Piotrowski & Napiorkowski, 2013; Al‚ÄêRimy et al., 2023).
            """)
      
            st.write("Matematicamente, a parada precoce pode ser descrita pela seguinte condi√ß√£o de interrup√ß√£o:")
            # F√≥rmulas matem√°ticas
            st.latex(r'''
            \text{Se } L_{\text{val}}(t) \geq L_{\text{val}}(t-1)
            ''')
            st.write("""
            por (p) √©pocas consecutivas, ent√£o interrompa o treinamento. Aqui,
            """)
            st.latex(r'''
            L_{\text{val}}(t)
            ''')
    
            st.write("""
            representa o valor da **fun√ß√£o de perda** no conjunto de valida√ß√£o na √©poca (t), e (p) √© o **par√¢metro de paci√™ncia**. 
            A paci√™ncia (p) define quanto tempo o treinamento deve continuar mesmo que n√£o haja melhorias imediatas. Se a perda n√£o melhorar por (p) √©pocas consecutivas, 
            o treinamento √© interrompido.
            """)
      
            st.write("""
            #### A Import√¢ncia da Paci√™ncia
            O par√¢metro de **paci√™ncia** define o n√∫mero de √©pocas consecutivas sem melhoria na m√©trica de valida√ß√£o que o modelo pode suportar antes de o treinamento ser interrompido. 
            A escolha do valor de paci√™ncia tem impacto direto no equil√≠brio entre **evitar o overfitting** e **permitir que o modelo continue aprendendo**. 
            """)
      
            st.write("##### Paci√™ncia = 0")
            st.write("""
            Um valor de paci√™ncia igual a zero implica que o treinamento ser√° interrompido imediatamente ap√≥s a primeira ocorr√™ncia de estagna√ß√£o na m√©trica de valida√ß√£o. 
            Isso pode ser √∫til em cen√°rios onde se deseja evitar qualquer risco de *overfitting*.
            """)
      
            st.write("##### Paci√™ncia ‚â• 1")
            st.write("""
            Uma paci√™ncia maior (como 1 ou 2) permite que o modelo continue sendo treinado mesmo ap√≥s pequenas flutua√ß√µes no desempenho, 
            o que pode ser ben√©fico em conjuntos de dados ruidosos (Sakizadeh et al., 2015).
            """)
      
            st.write("""
            #### Impacto do *Early Stopping* e da Paci√™ncia
            A configura√ß√£o do par√¢metro de paci√™ncia influencia diretamente a efici√™ncia do treinamento. Com uma paci√™ncia muito baixa, o treinamento pode ser interrompido de forma prematura, 
            mesmo que o modelo ainda tenha potencial de melhoria. Por outro lado, uma paci√™ncia muito alta pode permitir que o modelo se ajuste excessivamente aos dados de treinamento, 
            levando ao *overfitting* (Sakizadeh et al., 2015).
            """)
      
            st.write("""
            #### Exemplos de Aplica√ß√£o
            Um exemplo pr√°tico de uso da parada precoce √© em tarefas de **classifica√ß√£o de imagens**. Durante o treinamento de um modelo para detec√ß√£o de melanoma, se a acur√°cia no conjunto de valida√ß√£o 
            n√£o melhorar ap√≥s um determinado n√∫mero de √©pocas, o early stopping √© acionado.
            """)
      
            st.write("""
            #### Integra√ß√£o com Outras T√©cnicas de Regulariza√ß√£o
            A parada precoce pode ser usada em conjunto com outras t√©cnicas de regulariza√ß√£o, como a **inje√ß√£o de ru√≠do** e a regulariza√ß√£o **L1/L2**, 
            para melhorar a robustez do modelo e sua capacidade de generaliza√ß√£o (Friedrich et al., 2022). 
            A combina√ß√£o dessas t√©cnicas ajuda a evitar que o modelo se ajuste excessivamente aos dados de treinamento, principalmente em cen√°rios com volumes limitados de dados.
            """)
      
            st.write("""
            #### Conclus√£o
            A **parada precoce** √© uma t√©cnica eficaz para evitar o *overfitting* no treinamento de redes neurais profundas. O valor da paci√™ncia desempenha um papel cr√≠tico, 
            permitindo o equil√≠brio entre **efici√™ncia computacional** e **capacidade de aprendizado**. Al√©m disso, a combina√ß√£o da parada precoce com outras t√©cnicas de regulariza√ß√£o 
            pode melhorar ainda mais o desempenho do modelo.
            """)
      
            st.write("""
            #### Refer√™ncias
            - PIOTROWSKI, A.; NAPIORKOWSKI, J. A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling. *Journal of Hydrology*, v. 476, p. 97-111, 2013. https://doi.org/10.1016/j.jhydrol.2012.10.019.
            - AL‚ÄêRIMY, B. et al. An adaptive early stopping technique for densenet169-based knee osteoarthritis detection model. *Diagnostics*, v. 13, n. 11, p. 1903, 2023. https://doi.org/10.3390/diagnostics13111903.
            - SAKIZADEH, M.; MALIAN, A.; AHMADPOUR, E. Groundwater quality modeling with a small data set. *Ground Water*, v. 54, n. 1, p. 115-120, 2015. https://doi.org/10.1111/gwat.12317.
            - FRIEDRICH, S. et al. Regularization approaches in clinical biostatistics: a review of methods and their applications. *Statistical Methods in Medical Research*, v. 32, n. 2, p. 425-440, 2022. https://doi.org/10.1177/09622802221133557.
            """)


    #________________________________________________________________________________________
    patience = st.sidebar.number_input("Paci√™ncia para Early Stopping:", min_value=1, max_value=10, value=3, step=1)

    #____________________________________________________________________________________________
    with st.sidebar:
        with st.expander("Perda Ponderada para Classes Desbalanceadas:"):
            st.write("""
            ### Perda Ponderada para Classes Desbalanceadas
        
            A t√©cnica de **perda ponderada** para lidar com **classes desbalanceadas** √© amplamente utilizada em **aprendizado de m√°quina**, especialmente em redes neurais, para tratar problemas onde o n√∫mero de amostras entre as classes de um conjunto de dados n√£o √© equilibrado. O desbalanceamento ocorre em diversos dom√≠nios, como detec√ß√£o de fraudes, diagn√≥stico de doen√ßas e classifica√ß√£o de imagens. O principal objetivo da perda ponderada √© ajustar a fun√ß√£o de perda, atribuindo diferentes pesos √†s classes, de forma que o impacto das classes minorit√°rias (menos representadas) seja ampliado e o impacto das classes majorit√°rias seja reduzido. Isso ajuda o modelo a aprender de forma mais eficaz em cen√°rios onde o desequil√≠brio entre as classes pode levar ao **overfitting** nas classes majorit√°rias e √† **sub-representa√ß√£o** das classes minorit√°rias (Buda et al., 2018).
        
            ### Motiva√ß√£o e Justificativa Cient√≠fica
        
            Em um cen√°rio de classifica√ß√£o de imagens, se o modelo for treinado com uma quantidade muito maior de amostras de uma classe (classe majorit√°ria) em rela√ß√£o a outra (classe minorit√°ria), o modelo tende a ser enviesado para a classe majorit√°ria. Isso ocorre porque o objetivo padr√£o da maioria das fun√ß√µes de perda, como a **entropia cruzada**, √© minimizar a soma dos erros. Em um conjunto de dados desbalanceado, essa minimiza√ß√£o pode ser alcan√ßada simplesmente classificando todas as amostras como pertencentes √† classe majorit√°ria, resultando em alta acur√°cia geral, mas com desempenho ruim na classe minorit√°ria. Para resolver esse problema, atribui-se um peso maior √† classe minorit√°ria, for√ßando a fun√ß√£o de perda a penalizar mais fortemente os erros cometidos nessa classe (Buda et al., 2018).
        
            ### Implementa√ß√£o no C√≥digo
        
            No c√≥digo, a implementa√ß√£o da perda ponderada √© feita utilizando a fun√ß√£o de perda **CrossEntropyLoss** do PyTorch, que suporta a aplica√ß√£o de pesos √†s classes. Esses pesos s√£o calculados com base na **frequ√™ncia das classes** no conjunto de treinamento. Classes com menos amostras recebem pesos maiores, enquanto classes com mais amostras recebem pesos menores, balanceando o impacto de ambas durante o treinamento do modelo.
        
            """)
            
            st.write("**criterion = nn.CrossEntropyLoss(weight=class_weights)**")
            
            st.write("""
            No trecho de c√≥digo acima, o vetor `targets` coleta os r√≥tulos das amostras no conjunto de treino e a fun√ß√£o `np.bincount(targets)` conta quantas vezes cada classe aparece, resultando em um vetor `class_counts`, onde cada √≠ndice corresponde √† quantidade de amostras de uma classe espec√≠fica (Buda et al., 2018).
        
            ### Etapas do Processo
        
            1. **C√°lculo das Frequ√™ncias das Classes**: As frequ√™ncias de cada classe s√£o calculadas usando `np.bincount`. Classes menos representadas recebem pesos maiores.
            2. **Ajuste para Evitar Divis√£o por Zero**: Um pequeno valor (1e-6) √© adicionado para evitar divis√£o por zero quando uma classe n√£o tem nenhuma amostra.
            3. **C√°lculo dos Pesos Inversos**: A partir da frequ√™ncia, os pesos s√£o calculados tomando o inverso da frequ√™ncia de cada classe. Isso aumenta a penaliza√ß√£o dos erros nas classes minorit√°rias.
            4. **Fun√ß√£o de Perda Ponderada**: A fun√ß√£o de perda `nn.CrossEntropyLoss(weight=class_weights)` usa os pesos calculados, penalizando mais fortemente os erros das classes minorit√°rias.
        
            ### Impacto e Efic√°cia da Perda Ponderada
        
            A **perda ponderada** ajusta o aprendizado do modelo, incentivando a penaliza√ß√£o dos erros cometidos nas classes minorit√°rias. Estudos demonstram que essa t√©cnica √© eficaz em aumentar a **recall** das classes minorit√°rias, sem comprometer drasticamente a precis√£o das classes majorit√°rias (Buda et al., 2018). No entanto, a aplica√ß√£o da perda ponderada pode tornar o treinamento mais **sens√≠vel √† escolha dos hiperpar√¢metros**, como a **taxa de aprendizado**, pois o modelo passa a ser fortemente influenciado pelas amostras menos representativas.
        
            ### Conclus√£o
        
            A implementa√ß√£o da **perda ponderada** no c√≥digo √© uma abordagem robusta para lidar com **classes desbalanceadas**. Ao ajustar os pesos da fun√ß√£o de perda com base nas frequ√™ncias das classes, o modelo consegue equilibrar melhor o aprendizado entre as classes majorit√°rias e minorit√°rias, evitando vieses que favorecem a classe mais representada no conjunto de dados (Buda et al., 2018).
        
            ### Refer√™ncias
        
            - Buda, M., Maki, A., & Mazurowski, M. (2018). A systematic study of the class imbalance problem in convolutional neural networks. *Neural Networks*, 106, 249-259. https://doi.org/10.1016/j.neunet.2018.07.011
            """)

    use_weighted_loss = st.sidebar.checkbox("Usar Perda Ponderada para Classes Desbalanceadas", value=False)
    
    #________________________________________________________________________________________
    # API Configuration Section for AI Analysis
    st.sidebar.write("---")
    st.sidebar.subheader("üîë Configura√ß√£o de API para An√°lise IA")
    
    with st.sidebar.expander("Configurar API (Gemini/Groq)", expanded=False):
        st.write("Configure sua API para an√°lise diagn√≥stica com IA")
        
        api_provider_sidebar = st.selectbox(
            "Provedor de API:",
            options=['Nenhum', 'Gemini', 'Groq'],
            key='api_provider_sidebar',
            help="Escolha entre Google Gemini ou Groq para an√°lise com IA"
        )
        
        if api_provider_sidebar != 'Nenhum':
            if api_provider_sidebar == 'Gemini':
                model_options_sidebar = ['gemini-1.0-pro', 'gemini-1.5-pro', 'gemini-1.5-flash']
            else:  # Groq
                model_options_sidebar = ['mixtral-8x7b-32768', 'llama-3.1-70b-versatile', 'llama-3.1-8b-instant']
            
            ai_model_sidebar = st.selectbox(
                "Modelo:",
                options=model_options_sidebar,
                key='ai_model_sidebar',
                help="Escolha o modelo de IA para an√°lise"
            )
            
            api_key_sidebar = st.text_input(
                "API Key:",
                type="password",
                key='api_key_sidebar',
                help="Insira sua chave API (ser√° usada durante a avalia√ß√£o de imagens)"
            )
            
            if api_key_sidebar:
                st.success("‚úÖ API Key configurada!")
                st.session_state['api_configured'] = True
                st.session_state['api_provider'] = api_provider_sidebar
                st.session_state['api_model'] = ai_model_sidebar
                st.session_state['api_key'] = api_key_sidebar
            else:
                st.session_state['api_configured'] = False
        else:
            st.session_state['api_configured'] = False
            api_key_sidebar = None
            ai_model_sidebar = None
    
    st.sidebar.image("eu.ico", width=80)
   
    st.sidebar.write("""
    Produzido pelo:
    
    Projeto Geomaker + IA 
    
    https://doi.org/10.5281/zenodo.13910277
    
    - Professor: Marcelo Claro.
    
    Contatos: marceloclaro@gmail.com
    
    Whatsapp: (88)981587145
    
    Instagram: [marceloclaro.geomaker](https://www.instagram.com/marceloclaro.geomaker/)
    
    """)
     # _____________________________________________
    # Controle de √Åudio
    st.sidebar.title("Controle de √Åudio")
    
    # Dicion√°rio de arquivos de √°udio, com nomes amig√°veis mapeando para o caminho do arquivo
    mp3_files = {
        "√Åudio explicativo para Leigos": "leigo.mp3",
        "√Åudio explicativo para treinamentos de poucos dados": "bucal.mp3",
    }
    
    # Lista de arquivos MP3 para sele√ß√£o
    selected_mp3 = st.sidebar.radio("Escolha um √°udio explicativo:", options=list(mp3_files.keys()))
    
    # Controle de op√ß√£o de repeti√ß√£o
    loop = st.sidebar.checkbox("Repetir √°udio")
    
    # Bot√£o de Play para iniciar o √°udio
    play_button = st.sidebar.button("Play")
    
    # Placeholder para o player de √°udio
    audio_placeholder = st.sidebar.empty()
    
    # Fun√ß√£o para verificar se o arquivo existe
    def check_file_exists(mp3_path):
        if not os.path.exists(mp3_path):
            st.sidebar.error(f"Arquivo {mp3_path} n√£o encontrado.")
            return False
        return True
    
    # Se o bot√£o Play for pressionado e um arquivo de √°udio estiver selecionado
    if play_button and selected_mp3:
        mp3_path = mp3_files[selected_mp3]
        
        # Verifica√ß√£o da exist√™ncia do arquivo
        if check_file_exists(mp3_path):
            try:
                # Abrindo o arquivo de √°udio no modo bin√°rio
                with open(mp3_path, "rb") as audio_file:
                    audio_bytes = audio_file.read()
                    
                    # Codificando o arquivo em base64 para embutir no HTML
                    audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                    
                    # Controle de loop (repeti√ß√£o)
                    loop_attr = "loop" if loop else ""
                    
                    # Gerando o player de √°udio em HTML
                    audio_html = f"""
                    <audio id="audio-player" controls autoplay {loop_attr}>
                      <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                      Seu navegador n√£o suporta o elemento de √°udio.
                    </audio>
                    """
                    
                    # Inserindo o player de √°udio na interface
                    audio_placeholder.markdown(audio_html, unsafe_allow_html=True)
            
            except FileNotFoundError:
                st.sidebar.error(f"Arquivo {mp3_path} n√£o encontrado.")
            except Exception as e:
                st.sidebar.error(f"Erro ao carregar o arquivo: {str(e)}")
    #______________________________________________________________________________________-


    # Verificar se a soma dos splits √© v√°lida
    if train_split + valid_split > 0.95:
        st.sidebar.error("A soma dos splits de treinamento e valida√ß√£o deve ser menor ou igual a 0.95.")

    # Upload do arquivo ZIP
    
    zip_file = st.file_uploader("Upload do arquivo ZIP com as imagens", type=["zip"])

    if zip_file is not None and train_split + valid_split <= 0.95:
        temp_dir = tempfile.mkdtemp()
        zip_path = os.path.join(temp_dir, "uploaded.zip")
        with open(zip_path, "wb") as f:
            f.write(zip_file.read())
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        data_dir = temp_dir

        # Detectar automaticamente o n√∫mero de classes do dataset
        try:
            temp_dataset = datasets.ImageFolder(root=data_dir)
            detected_num_classes = len(temp_dataset.classes)
            st.success(f"‚úÖ N√∫mero de classes detectado automaticamente: **{detected_num_classes}**")
            st.write(f"Classes encontradas: {', '.join(temp_dataset.classes)}")
            num_classes = detected_num_classes
        except Exception as e:
            st.error(f"Erro ao detectar classes: {e}")
            st.error("Certifique-se de que o ZIP cont√©m pastas com nomes de classes e imagens dentro delas.")
            shutil.rmtree(temp_dir)
            return

        st.write("Iniciando o treinamento supervisionado...")
        model_data = train_model(data_dir, num_classes, model_name, fine_tune, epochs, learning_rate, 
                                batch_size, train_split, valid_split, use_weighted_loss, l2_lambda, l1_lambda, 
                                patience, optimizer_name, scheduler_name, augmentation_type)

        if model_data is None:
            st.error("Erro no treinamento do modelo.")
            shutil.rmtree(temp_dir)
            return

        model, classes, training_history = model_data
        st.success("Treinamento conclu√≠do!")
        
        # Adicionar bot√£o de download do CSV com hist√≥rico de treinamento
        st.write("---")
        st.write("## üìä Exportar Resultados de Treinamento")
        df_training_export = pd.DataFrame(training_history)
        csv_training = export_to_csv(df_training_export, "historico_treinamento.csv")
        st.download_button(
            label="üì• Baixar CSV - Hist√≥rico de Treinamento",
            data=csv_training,
            file_name=f"historico_treinamento_{model_name}.csv",
            mime="text/csv",
            help="Download do hist√≥rico completo de treinamento (loss e accuracy por √©poca)"
        )

        # Extrair caracter√≠sticas usando o modelo pr√©-treinado (sem a camada final)
        st.write("Extraindo caracter√≠sticas para clustering...")
        # Remover a √∫ltima camada do modelo para obter embeddings
        if model_name.startswith('ResNet'):
            feature_extractor = nn.Sequential(*list(model.children())[:-1])
        elif model_name.startswith('DenseNet'):
            feature_extractor = nn.Sequential(*list(model.features))
            feature_extractor.add_module('global_pool', nn.AdaptiveAvgPool2d((1,1)))
        elif model_name.startswith('ViT'):
            # Para Vision Transformers, remover apenas a camada head
            # Mantemos o encoder completo
            class ViTFeatureExtractor(nn.Module):
                def __init__(self, vit_model):
                    super().__init__()
                    self.conv_proj = vit_model.conv_proj
                    self.encoder = vit_model.encoder
                    self.class_token = vit_model.class_token
                    
                def forward(self, x):
                    # Reshape and permute the input tensor
                    x = self.conv_proj(x)
                    x = x.flatten(2).transpose(1, 2)
                    
                    # Add class token
                    batch_size = x.shape[0]
                    class_tokens = self.class_token.expand(batch_size, -1, -1)
                    x = torch.cat([class_tokens, x], dim=1)
                    
                    # Pass through encoder
                    x = self.encoder(x)
                    
                    # Return the class token output
                    return x[:, 0]
            
            feature_extractor = ViTFeatureExtractor(model)
        else:
            st.error("Modelo n√£o suportado para extra√ß√£o de caracter√≠sticas.")
            return

        feature_extractor = feature_extractor.to(device)
        feature_extractor.eval()

        # Carregar o dataset completo para extra√ß√£o de caracter√≠sticas
        full_dataset = datasets.ImageFolder(root=data_dir, transform=test_transforms)
        features, labels = extract_features(full_dataset, feature_extractor, batch_size)

        # Aplicar algoritmos de clustering
        st.write("Aplicando algoritmos de clustering...")
        features_reshaped = features.reshape(len(features), -1)
        hierarchical_labels, kmeans_labels = perform_clustering(features_reshaped, num_classes)

        # Avaliar e exibir os resultados
        st.write("Avaliando os resultados do clustering...")
        evaluate_clustering(labels, hierarchical_labels, "Clustering Hier√°rquico")
        evaluate_clustering(labels, kmeans_labels, "K-Means Clustering")

        # Visualizar clusters
        visualize_clusters(features_reshaped, labels, hierarchical_labels, kmeans_labels, classes)
        
        # Exportar resultados de clustering para CSV
        st.write("---")
        st.write("## üìä Exportar Resultados de Clustering")
        ari_hierarchical = adjusted_rand_score(labels, hierarchical_labels)
        nmi_hierarchical = normalized_mutual_info_score(labels, hierarchical_labels)
        ari_kmeans = adjusted_rand_score(labels, kmeans_labels)
        nmi_kmeans = normalized_mutual_info_score(labels, kmeans_labels)
        
        clustering_results = {
            'sample_id': list(range(len(labels))),
            'true_label': labels,
            'true_class_name': [classes[label] for label in labels],
            'hierarchical_cluster': hierarchical_labels,
            'kmeans_cluster': kmeans_labels
        }
        df_clustering = pd.DataFrame(clustering_results)
        
        # Adicionar m√©tricas de avalia√ß√£o como linhas de resumo
        summary_data = {
            'sample_id': ['M√âTRICAS', 'M√âTRICAS'],
            'true_label': ['Hierarchical ARI', 'K-Means ARI'],
            'true_class_name': [f'{ari_hierarchical:.4f}', f'{ari_kmeans:.4f}'],
            'hierarchical_cluster': [f'NMI: {nmi_hierarchical:.4f}', ''],
            'kmeans_cluster': ['', f'NMI: {nmi_kmeans:.4f}']
        }
        df_summary = pd.DataFrame(summary_data)
        df_clustering_export = pd.concat([df_clustering, df_summary], ignore_index=True)
        
        csv_clustering = export_to_csv(df_clustering_export, "resultados_clustering.csv")
        st.download_button(
            label="üì• Baixar CSV - Resultados de Clustering",
            data=csv_clustering,
            file_name=f"clustering_{model_name}.csv",
            mime="text/csv",
            help="Download dos resultados completos de clustering"
        )
        
        # ========== OP√á√ÉO DE VISUALIZA√á√ÉO PCA ==========
        st.write("---")
        st.write("## üî¨ An√°lise PCA das Features")
        
        show_pca = st.checkbox("üìä Mostrar An√°lise PCA das Features Extra√≠das", value=True)
        
        if show_pca:
            # Op√ß√£o de escolher n√∫mero de componentes
            n_components = st.selectbox(
                "Escolha o n√∫mero de componentes principais para visualiza√ß√£o:",
                options=[2, 3],
                index=0,
                help="2 componentes: Visualiza√ß√£o 2D | 3 componentes: Visualiza√ß√£o 3D (n√£o implementado ainda)"
            )
            
            if n_components == 2:
                visualize_pca_features(features_reshaped, labels, classes, n_components=2)
            else:
                st.info("üìå Visualiza√ß√£o 3D ser√° implementada em vers√£o futura.")
                # Mostrar 2D por padr√£o
                visualize_pca_features(features_reshaped, labels, classes, n_components=2)

        # Avalia√ß√£o de uma imagem individual
        evaluate = st.radio("Deseja avaliar uma imagem?", ("Sim", "N√£o"))
        if evaluate == "Sim":
            eval_image_file = st.file_uploader("Fa√ßa upload da imagem para avalia√ß√£o", type=["png", "jpg", "jpeg", "bmp", "gif"])
            if eval_image_file is not None:
                eval_image_file.seek(0)
                try:
                    eval_image = Image.open(eval_image_file).convert("RGB")
                except Exception as e:
                    st.error(f"Erro ao abrir a imagem: {e}")
                    return

                st.image(eval_image, caption='Imagem para avalia√ß√£o', width='stretch')
                class_name, confidence = evaluate_image(model, eval_image, classes)
                st.write(f"**Classe Predita:** {class_name}")
                st.write(f"**Confian√ßa:** {confidence:.4f}")

                # Visualizar ativa√ß√µes com o tipo de Grad-CAM selecionado
                activation_map = visualize_activations(model, eval_image, classes, gradcam_type)
                
                # ========== AN√ÅLISE ESTAT√çSTICA COMPLETA ==========
                st.write("---")
                with st.spinner("üî¨ Realizando an√°lise estat√≠stica completa..."):
                    # An√°lise estat√≠stica com bootstrap (ajuste n_bootstrap conforme necess√°rio)
                    n_bootstrap = st.slider(
                        "N√∫mero de itera√ß√µes Bootstrap", 
                        min_value=50, 
                        max_value=500, 
                        value=100, 
                        step=50,
                        help="Mais itera√ß√µes = an√°lise mais precisa mas mais lenta"
                    )
                    
                    statistical_analysis = evaluate_image_with_statistics(
                        model, 
                        eval_image, 
                        classes, 
                        activation_map=activation_map,
                        n_bootstrap=n_bootstrap
                    )
                    
                    # Exibir an√°lise estat√≠stica
                    display_statistical_analysis(statistical_analysis)
                
                # Preparar dados para exporta√ß√£o CSV
                classification_result = {
                    'imagem': eval_image_file.name,
                    'classe_predita': class_name,
                    'confianca': confidence,
                    'modelo': model_name,
                    'tipo_gradcam': gradcam_type,
                    'epocas_treinamento': epochs,
                    'taxa_aprendizagem': learning_rate,
                    'batch_size': batch_size,
                    'augmentation_type': augmentation_type,
                    'optimizer': optimizer_name
                }
                
                # Criar DataFrame de classifica√ß√£o
                df_classification = pd.DataFrame([classification_result])
                
                # Bot√£o para exportar resultado da classifica√ß√£o
                st.write("---")
                st.write("## üìä Exportar Resultado da Classifica√ß√£o")
                csv_classification = export_to_csv(df_classification, "resultado_classificacao.csv")
                st.download_button(
                    label="üì• Baixar CSV - Resultado da Classifica√ß√£o",
                    data=csv_classification,
                    file_name=f"classificacao_{eval_image_file.name.split('.')[0]}.csv",
                    mime="text/csv",
                    help="Download do resultado da classifica√ß√£o desta imagem"
                )
                
                # Op√ß√£o para an√°lise com IA se API configurada
                if 'api_configured' in st.session_state and st.session_state['api_configured']:
                    st.write("---")
                    st.write("## ü§ñ An√°lise Diagn√≥stica com IA (Vis√£o Computacional)")
                    st.write(f"**API Configurada:** {st.session_state['api_provider']} - {st.session_state['api_model']}")
                    
                    if st.button("üî¨ Gerar An√°lise Completa com IA + Vis√£o"):
                        with st.spinner("üîç Analisando imagem com IA (vis√£o computacional)..."):
                            # Gerar descri√ß√£o do Grad-CAM
                            gradcam_desc = generate_gradcam_description(activation_map) if activation_map is not None else ""
                            
                            # Executar an√°lise com IA apropriada
                            if st.session_state['api_provider'] == 'Gemini':
                                if not GEMINI_AVAILABLE:
                                    st.error("‚ùå Google Generative AI n√£o est√° instalado. Execute: pip install google-generativeai")
                                    ai_analysis_text = "Erro: Biblioteca n√£o dispon√≠vel"
                                else:
                                    ai_analysis_text = analyze_image_with_gemini(
                                        eval_image,
                                        st.session_state['api_key'],
                                        st.session_state['api_model'],
                                        class_name,
                                        confidence,
                                        gradcam_desc
                                    )
                            else:  # Groq
                                if not GROQ_AVAILABLE:
                                    st.error("‚ùå Groq n√£o est√° instalado. Execute: pip install groq")
                                    ai_analysis_text = "Erro: Biblioteca n√£o dispon√≠vel"
                                else:
                                    ai_analysis_text = analyze_image_with_groq_vision(
                                        eval_image,
                                        st.session_state['api_key'],
                                        st.session_state['api_model'],
                                        class_name,
                                        confidence,
                                        gradcam_desc
                                    )
                            
                            # Exibir an√°lise
                            st.success("‚úÖ An√°lise Completa Gerada!")
                            st.write("### üìã Relat√≥rio de An√°lise com IA")
                            st.markdown(ai_analysis_text)
                            
                            # ========== MULTI-AGENT SYSTEM ANALYSIS (15 AGENTS + MANAGER) ==========
                            if MULTI_AGENT_AVAILABLE:
                                st.write("---")
                                st.write("## ü§ñ Sistema Multi-Agente (15 Agentes + 1 Gerente)")
                                
                                use_multiagent = st.checkbox("Ativar An√°lise com Sistema Multi-Agente (15 Especialistas)", value=True)
                                
                                if use_multiagent:
                                    with st.spinner("Coordenando an√°lise de 15 agentes especializados + 1 gerente..."):
                                        try:
                                            manager = ManagerAgent()
                                            
                                            # Preparar contexto
                                            agent_context = {
                                                'gradcam_description': gradcam_desc,
                                                'ai_analysis': ai_analysis_text
                                            }
                                            
                                            multi_agent_report = manager.coordinate_analysis(
                                                predicted_class=class_name,
                                                confidence=confidence,
                                                context=agent_context
                                            )
                                            
                                            st.markdown(multi_agent_report)
                                            st.success("‚úÖ An√°lise Multi-Agente Conclu√≠da! 15 especialistas + 1 gerente coordenador")
                                            
                                        except Exception as e:
                                            st.error(f"Erro ao gerar an√°lise multi-agente: {str(e)}")
                            
                            # Preparar dados para exporta√ß√£o
                            ai_analysis_result = {
                                'imagem': eval_image_file.name,
                                'classe_predita': class_name,
                                'confianca': confidence,
                                'api_provider': st.session_state['api_provider'],
                                'api_model': st.session_state['api_model'],
                                'gradcam_description': gradcam_desc,
                                'analise_completa': ai_analysis_text,
                                'modelo_classificacao': model_name,
                                'epocas': epochs,
                                'batch_size': batch_size,
                                'learning_rate': learning_rate
                            }
                            
                            # Exportar an√°lise IA para CSV
                            df_ai_analysis = pd.DataFrame([ai_analysis_result])
                            csv_ai = export_to_csv(df_ai_analysis, "analise_ia_visao.csv")
                            
                            st.write("---")
                            st.download_button(
                                label="üì• Baixar CSV - An√°lise Completa com IA",
                                data=csv_ai,
                                file_name=f"analise_ia_visao_{eval_image_file.name.split('.')[0]}.csv",
                                mime="text/csv",
                                help="Download da an√°lise completa com IA incluindo vis√£o computacional"
                            )
                else:
                    st.info("""
                    üí° **An√°lise com IA Dispon√≠vel**
                    
                    Configure uma API (Gemini ou Groq) na barra lateral para ativar a an√°lise 
                    diagn√≥stica com IA que inclui:
                    - ‚úÖ Vis√£o Computacional (a IA pode "ver" e analisar a imagem)
                    - ‚úÖ Interpreta√ß√£o t√©cnica detalhada
                    - ‚úÖ An√°lise forense da imagem
                    - ‚úÖ Recomenda√ß√µes baseadas em an√°lise visual
                    - ‚úÖ Exporta√ß√£o completa para CSV
                    
                    **Modelos com Suporte de Vis√£o:**
                    - Gemini: gemini-1.0-pro, gemini-1.5-pro, gemini-1.5-flash
                    - Groq: Suporte limitado dependendo do modelo
                    """)

        # Limpar o diret√≥rio tempor√°rio
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()
